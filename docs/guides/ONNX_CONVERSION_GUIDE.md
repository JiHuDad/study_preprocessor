# ONNX ë³€í™˜ ê°€ì´ë“œ

## ğŸ¯ ê°œìš”

PyTorchë¡œ í•™ìŠµí•œ DeepLog/MS-CRED ëª¨ë¸ì„ ONNX í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ C inference engineì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1ë‹¨ê³„: ëª¨ë¸ í•™ìŠµ

```bash
# ë¡œê·¸ íŒŒì‹±
alog-detect parse --input data/raw/system.log --out-dir data/processed

# DeepLog ì‹œí€€ìŠ¤ ìƒì„± ë° í•™ìŠµ
alog-detect train-deeplog \
    --parsed data/processed/parsed.parquet \
    --out-dir data/processed \
    --epochs 50

# ëª¨ë¸ ì €ì¥ ìœ„ì¹˜: .cache/deeplog.pth
```

### 2ë‹¨ê³„: ONNX ë³€í™˜ (ìë™ vocab ë³€í™˜ í¬í•¨!)

```bash
python hybrid_system/training/model_converter.py \
    --deeplog-model .cache/deeplog.pth \
    --vocab data/processed/vocab.json \
    --output-dir hybrid_system/inference/models
```

**ì¶œë ¥ íŒŒì¼**:
- âœ… `deeplog.onnx` - ONNX ëª¨ë¸
- âœ… `deeplog_optimized.onnx` - ìµœì í™”ëœ ONNX ëª¨ë¸
- âœ… `vocab.json` - **ìë™ìœ¼ë¡œ C ì—”ì§„ìš© í˜•ì‹ìœ¼ë¡œ ë³€í™˜ë¨!**
- âœ… `deeplog.onnx.meta.json` - ëª¨ë¸ ë©”íƒ€ë°ì´í„°
- âœ… `conversion_summary.json` - ë³€í™˜ ìš”ì•½

### 3ë‹¨ê³„: C Inference Engine ì‹¤í–‰

```bash
# ONNX Runtime ì„¤ì¹˜ (ì²˜ìŒ í•œ ë²ˆë§Œ)
./scripts/install_onnxruntime.sh

# Inference Engine ë¹Œë“œ
cd hybrid_system/inference
make clean && make

# í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰
./bin/inference_engine \
    -d models/deeplog.onnx \
    -v models/vocab.json \
    -t

# ì‹¤ì œ ë¡œê·¸ íŒŒì¼ ì²˜ë¦¬
./bin/inference_engine \
    -d models/deeplog.onnx \
    -v models/vocab.json \
    -i /var/log/syslog \
    -o results.json
```

## ğŸ“‹ ìë™ Vocab ë³€í™˜ ë™ì‘ ë°©ì‹

### ê¸°ì¡´ ë¬¸ì œ

Python í•™ìŠµ ì‹œ ìƒì„±ë˜ëŠ” `vocab.json`:
```json
{
  "1": 0,   // template_id â†’ index
  "2": 1,
  "3": 2
}
```

C ì—”ì§„ì´ í•„ìš”í•œ `vocab.json`:
```json
{
  "1": "[<NUM>] usb device connected",  // template_id â†’ template_string
  "2": "[<NUM>] CPU temperature high",
  "3": "[<NUM>] Network link up"
}
```

### âœ… ìë™ ë³€í™˜ ë¡œì§

`model_converter.py`ëŠ” ì´ì œ **ìë™ìœ¼ë¡œ** ì˜¬ë°”ë¥¸ í˜•ì‹ì˜ vocab.jsonì„ ìƒì„±í•©ë‹ˆë‹¤:

1. **parsed.parquetì—ì„œ ì¶”ì¶œ** (ìš°ì„ ìˆœìœ„ 1)
   - `data/processed/parsed.parquet` íŒŒì¼ í™•ì¸
   - `template_id`ì™€ `template` ì»¬ëŸ¼ì—ì„œ ë§¤í•‘ ì¶”ì¶œ

2. **preview.jsonì—ì„œ ì¶”ì¶œ** (ìš°ì„ ìˆœìœ„ 2)
   - `data/processed/preview.json` íŒŒì¼ í™•ì¸
   - ê° í•­ëª©ì˜ `template_id`ì™€ `template` í•„ë“œ ì¶”ì¶œ

3. **ê²½ê³  í‘œì‹œ** (ë³€í™˜ ë¶ˆê°€ëŠ¥í•œ ê²½ìš°)
   - í•„ìš”í•œ íŒŒì¼ì´ ì—†ìœ¼ë©´ ê²½ê³  ë©”ì‹œì§€ ì¶œë ¥
   - ìˆ˜ë™ ë³€í™˜ ëª…ë ¹ì–´ ì•ˆë‚´

## ğŸ”§ ê³ ê¸‰ ì‚¬ìš©ë²•

### Option 1: ê²€ì¦ í¬í•¨ ë³€í™˜

```bash
python hybrid_system/training/model_converter.py \
    --deeplog-model .cache/deeplog.pth \
    --vocab data/processed/vocab.json \
    --output-dir hybrid_system/inference/models \
    --validate  # ONNX ëª¨ë¸ ê²€ì¦ ì‹¤í–‰
```

### Option 2: MS-CREDë„ í•¨ê»˜ ë³€í™˜

```bash
python hybrid_system/training/model_converter.py \
    --deeplog-model .cache/deeplog.pth \
    --mscred-model .cache/mscred.pth \
    --vocab data/processed/vocab.json \
    --output-dir hybrid_system/inference/models
```

### Option 3: ìˆ˜ë™ Vocab ë³€í™˜ (í•„ìš” ì‹œ)

parsed.parquetê°€ vocab.jsonê³¼ ë‹¤ë¥¸ ìœ„ì¹˜ì— ìˆëŠ” ê²½ìš°:

```bash
python hybrid_system/training/export_vocab_with_templates.py \
    /path/to/parsed.parquet \
    hybrid_system/inference/models/vocab.json
```

## ğŸ› íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### ë¬¸ì œ 1: "Loaded vocabulary: 0 templates"

**ì›ì¸**: vocab.jsonì´ ì¸ë±ìŠ¤ í˜•ì‹ì…ë‹ˆë‹¤.

**í•´ê²°**:
```bash
# ìë™ ë³€í™˜ (ê¶Œì¥)
python hybrid_system/training/model_converter.py \
    --deeplog-model .cache/deeplog.pth \
    --vocab data/processed/vocab.json \
    --output-dir hybrid_system/inference/models

# ë˜ëŠ” ìˆ˜ë™ ë³€í™˜
python hybrid_system/training/export_vocab_with_templates.py \
    data/processed/parsed.parquet \
    hybrid_system/inference/models/vocab.json
```

**ê²€ì¦**:
```bash
# vocab.json ë‚´ìš© í™•ì¸ - ê°’ì´ ë¬¸ìì—´ì´ì–´ì•¼ í•¨
cat hybrid_system/inference/models/vocab.json

# ì˜¬ë°”ë¥¸ ì˜ˆì‹œ:
# {
#   "1": "[<NUM>] usb device connected",  âœ…
#   "2": "[<NUM>] CPU temperature high"   âœ…
# }

# ì˜ëª»ëœ ì˜ˆì‹œ:
# {
#   "1": 0,  âŒ
#   "2": 1   âŒ
# }
```

### ë¬¸ì œ 2: "parsed.parquet ë˜ëŠ” preview.jsonì´ í•„ìš”í•©ë‹ˆë‹¤"

**ì›ì¸**: vocab ìë™ ë³€í™˜ì— í•„ìš”í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.

**í•´ê²°**:

Option A: parsed.parquet ìƒì„±
```bash
alog-detect parse \
    --input data/raw/system.log \
    --out-dir data/processed
```

Option B: preview.jsonì´ ìˆëŠ” ê²½ë¡œë¡œ vocab_path ì§€ì •
```bash
python hybrid_system/training/model_converter.py \
    --deeplog-model .cache/deeplog.pth \
    --vocab data/processed/vocab.json \  # preview.jsonì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆì–´ì•¼ í•¨
    --output-dir hybrid_system/inference/models
```

### ë¬¸ì œ 3: ONNX Runtime ë¹Œë“œ ì—ëŸ¬

**ì¦ìƒ**:
```
fatal error: onnxruntime_c_api.h: ê·¸ëŸ° íŒŒì¼ì´ë‚˜ ë””ë ‰í„°ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤
```

**í•´ê²°**:
```bash
# ONNX Runtime C API ì„¤ì¹˜
./scripts/install_onnxruntime.sh

# ë˜ëŠ” ìˆ˜ë™ ì„¤ì¹˜
wget https://github.com/microsoft/onnxruntime/releases/download/v1.16.0/onnxruntime-linux-x64-1.16.0.tgz
tar -xzf onnxruntime-linux-x64-1.16.0.tgz
sudo cp onnxruntime-linux-x64-1.16.0/lib/* /usr/local/lib/
sudo cp -r onnxruntime-linux-x64-1.16.0/include/* /usr/local/include/
sudo ldconfig

# ê²€ì¦
ls -l /usr/local/lib/libonnxruntime*
ls -l /usr/local/include/onnxruntime_c_api.h
```

## ğŸ“Š ë³€í™˜ ê²°ê³¼ í™•ì¸

### ONNX ëª¨ë¸ ì •ë³´

```bash
# ë©”íƒ€ë°ì´í„° í™•ì¸
cat hybrid_system/inference/models/deeplog.onnx.meta.json
```

ì¶œë ¥ ì˜ˆì‹œ:
```json
{
  "model_type": "deeplog",
  "vocab_size": 7,
  "seq_len": 3,
  "input_shape": [1, 3],
  "output_shape": [1, 7],
  "input_names": ["input_sequence"],
  "output_names": ["predictions"],
  "opset_version": 11
}
```

### Vocab í˜•ì‹ í™•ì¸

```bash
# vocab.json í™•ì¸
cat hybrid_system/inference/models/vocab.json | head -5
```

**ì˜¬ë°”ë¥¸ ì¶œë ¥**:
```json
{
  "1": "[<NUM>] usb 1-1: new high-speed USB device number <NUM> using ehci-pci",
  "2": "[<NUM>] CPU<ID>: Core temperature above threshold, cpu clock throttled",
  ...
}
```

## ğŸ¯ ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. í‘œì¤€ ì›Œí¬í”Œë¡œìš°

```bash
# 1. íŒŒì‹± (parsed.parquet + preview.json + vocab.json ìƒì„±)
alog-detect parse --input data/raw/system.log --out-dir data/processed

# 2. í•™ìŠµ (deeplog.pth ìƒì„±)
alog-detect train-deeplog --parsed data/processed/parsed.parquet --out-dir data/processed

# 3. ONNX ë³€í™˜ (ìë™ìœ¼ë¡œ ì˜¬ë°”ë¥¸ vocab.json ìƒì„±!)
python hybrid_system/training/model_converter.py \
    --deeplog-model .cache/deeplog.pth \
    --vocab data/processed/vocab.json \
    --output-dir hybrid_system/inference/models \
    --validate

# 4. C ì—”ì§„ ì‹¤í–‰
cd hybrid_system/inference
make && ./bin/inference_engine -d models/deeplog.onnx -v models/vocab.json -t
```

### 2. ë””ë ‰í† ë¦¬ êµ¬ì¡°

```
project/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ system.log              # ì›ë³¸ ë¡œê·¸
â”‚   â””â”€â”€ processed/
â”‚       â”œâ”€â”€ parsed.parquet          # íŒŒì‹± ê²°ê³¼ (í…œí”Œë¦¿ í¬í•¨!)
â”‚       â”œâ”€â”€ preview.json            # ë¯¸ë¦¬ë³´ê¸° (í…œí”Œë¦¿ í¬í•¨!)
â”‚       â”œâ”€â”€ vocab.json              # Pythonìš© (ì¸ë±ìŠ¤ í˜•ì‹)
â”‚       â””â”€â”€ sequences.parquet       # ì‹œí€€ìŠ¤ ë°ì´í„°
â”œâ”€â”€ .cache/
â”‚   â””â”€â”€ deeplog.pth                 # í•™ìŠµëœ ëª¨ë¸
â””â”€â”€ hybrid_system/
    â”œâ”€â”€ training/
    â”‚   â”œâ”€â”€ model_converter.py      # ONNX ë³€í™˜ (ìë™ vocab ë³€í™˜!)
    â”‚   â””â”€â”€ export_vocab_with_templates.py  # ìˆ˜ë™ vocab ë³€í™˜
    â””â”€â”€ inference/
        â”œâ”€â”€ models/
        â”‚   â”œâ”€â”€ deeplog.onnx        # ONNX ëª¨ë¸
        â”‚   â”œâ”€â”€ vocab.json          # C ì—”ì§„ìš© (í…œí”Œë¦¿ ë¬¸ìì—´!)
        â”‚   â””â”€â”€ *.meta.json         # ë©”íƒ€ë°ì´í„°
        â””â”€â”€ bin/
            â””â”€â”€ inference_engine    # C ì¶”ë¡  ì—”ì§„
```

### 3. ì²´í¬ë¦¬ìŠ¤íŠ¸

ONNX ë³€í™˜ ì „:
- [ ] `data/processed/parsed.parquet` ì¡´ì¬ í™•ì¸
- [ ] `data/processed/vocab.json` ì¡´ì¬ í™•ì¸
- [ ] `.cache/deeplog.pth` ì¡´ì¬ í™•ì¸

ONNX ë³€í™˜ í›„:
- [ ] `hybrid_system/inference/models/deeplog.onnx` ìƒì„± í™•ì¸
- [ ] `hybrid_system/inference/models/vocab.json` í˜•ì‹ í™•ì¸ (í…œí”Œë¦¿ ë¬¸ìì—´!)
- [ ] Vocab ë¡œê·¸ì— "âœ… C ì—”ì§„ìš© vocab í˜•ì‹" ë©”ì‹œì§€ í™•ì¸

C ì—”ì§„ ì‹¤í–‰ ì „:
- [ ] ONNX Runtime C API ì„¤ì¹˜ ì™„ë£Œ
- [ ] `make` ë¹Œë“œ ì„±ê³µ
- [ ] Test ëª¨ë“œì—ì„œ "Loaded vocabulary: N templates" (N > 0) í™•ì¸

## ğŸ”— ê´€ë ¨ ë¬¸ì„œ

- [C Inference Engine README](../../hybrid_system/inference/README.md)
- [Vocab Issue Resolved](../development/VOCAB_ISSUE_RESOLVED.md)
- [Train & Inference Guide](./TRAIN_INFERENCE_GUIDE.md)
- [Model Converter Source](../../hybrid_system/training/model_converter.py)

## ğŸ“… ì—…ë°ì´íŠ¸

- **2025-10-17**: ìë™ vocab ë³€í™˜ ê¸°ëŠ¥ ì¶”ê°€ - ë” ì´ìƒ ìˆ˜ë™ ë³€í™˜ ë¶ˆí•„ìš”!
