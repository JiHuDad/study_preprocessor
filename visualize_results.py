#!/usr/bin/env python3
"""
ê²°ê³¼ ì‹œê°í™” ë„êµ¬ (matplotlib/seaborn ì—†ì´)
"""
import pandas as pd
import json
from pathlib import Path
import argparse

def print_chart(title, data, max_width=50):
    """ê°„ë‹¨í•œ í…ìŠ¤íŠ¸ ì°¨íŠ¸ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    print(f"\nğŸ“Š {title}")
    print("="*60)
    
    if not data:
        print("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    max_val = max(data.values()) if data.values() else 0
    if max_val == 0:
        print("ëª¨ë“  ê°’ì´ 0ì…ë‹ˆë‹¤.")
        return
    
    for label, value in data.items():
        bar_length = int((value / max_val) * max_width)
        bar = "â–ˆ" * bar_length
        print(f"{label:20} |{bar:<{max_width}} {value}")

def visualize_results(data_dir: str = "data/processed"):
    """ê²°ê³¼ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    data_path = Path(data_dir)
    
    print("="*60)
    print("ğŸ“ˆ LOG ANOMALY DETECTION ì‹œê°í™”")
    print("="*60)
    
    # ê¸°ë³¸ ë°ì´í„° ë¡œë“œ
    parsed_df = pd.read_parquet(data_path / "parsed.parquet")
    baseline_scores = pd.read_parquet(data_path / "baseline_scores.parquet")
    deeplog_df = pd.read_parquet(data_path / "deeplog_infer.parquet")
    
    with open(data_path / "vocab.json") as f:
        vocab = json.load(f)
    
    # 1. í…œí”Œë¦¿ ë¶„í¬
    template_counts = parsed_df['template_id'].value_counts().head(10)
    template_data = {}
    for template_id, count in template_counts.items():
        template_data[f"T{template_id}"] = count
    
    print_chart("í…œí”Œë¦¿ ì¶œí˜„ ë¹ˆë„ (Top 10)", template_data)
    
    # 2. ì‹œê°„ëŒ€ë³„ ë¡œê·¸ ë¶„í¬
    try:
        parsed_df['hour'] = pd.to_datetime(parsed_df['timestamp']).dt.hour
        hourly_counts = parsed_df['hour'].value_counts().sort_index()
        hourly_data = {f"{h:02d}ì‹œ": count for h, count in hourly_counts.items()}
    except Exception as e:
        print(f"ì‹œê°„ëŒ€ë³„ ë¶„í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
        hourly_data = {"ì‹œê°„ì •ë³´ì—†ìŒ": len(parsed_df)}
    
    print_chart("ì‹œê°„ëŒ€ë³„ ë¡œê·¸ ë¶„í¬", hourly_data)
    
    # 3. Baseline ì´ìƒ ì ìˆ˜ ë¶„í¬
    score_bins = pd.cut(baseline_scores['score'], bins=5, labels=['ë§¤ìš°ë‚®ìŒ', 'ë‚®ìŒ', 'ë³´í†µ', 'ë†’ìŒ', 'ë§¤ìš°ë†’ìŒ'])
    score_data = score_bins.value_counts().to_dict()
    
    print_chart("Baseline ì´ìƒ ì ìˆ˜ ë¶„í¬", score_data)
    
    # 4. ì´ìƒ ìœˆë„ìš° vs ì •ìƒ ìœˆë„ìš°
    anomaly_data = {
        "ì •ìƒ ìœˆë„ìš°": len(baseline_scores[baseline_scores['is_anomaly'] == False]),
        "ì´ìƒ ìœˆë„ìš°": len(baseline_scores[baseline_scores['is_anomaly'] == True])
    }
    
    print_chart("ì´ìƒ íƒì§€ ê²°ê³¼", anomaly_data)
    
    # 5. DeepLog ì„±ëŠ¥
    # Enhanced ë²„ì „: prediction_ok ì‚¬ìš©, ê¸°ì¡´ ë²„ì „: in_topk ì‚¬ìš©
    if 'prediction_ok' in deeplog_df.columns:
        success_count = len(deeplog_df[deeplog_df['prediction_ok'] == True])
        failure_count = len(deeplog_df[deeplog_df['prediction_ok'] == False])
    elif 'in_topk' in deeplog_df.columns:
        success_count = len(deeplog_df[deeplog_df['in_topk'] == True])
        failure_count = len(deeplog_df[deeplog_df['in_topk'] == False])
    else:
        success_count = 0
        failure_count = 0

    deeplog_data = {
        "ì˜ˆì¸¡ ì„±ê³µ": success_count,
        "ì˜ˆì¸¡ ì‹¤íŒ¨": failure_count
    }

    print_chart("DeepLog ì˜ˆì¸¡ ì„±ëŠ¥", deeplog_data)

    # 6. ìƒì„¸ í†µê³„
    print(f"\nğŸ“Š ìƒì„¸ í†µê³„")
    print("="*60)
    print(f"ì´ ë¡œê·¸ ë¼ì¸ ìˆ˜: {len(parsed_df):,}")
    print(f"ë°œê²¬ëœ í…œí”Œë¦¿ ìˆ˜: {len(vocab)}")
    print(f"ë¶„ì„ ìœˆë„ìš° ìˆ˜: {len(baseline_scores)}")
    print(f"ì´ìƒë¥ : {len(baseline_scores[baseline_scores['is_anomaly']==True])/len(baseline_scores)*100:.1f}%")
    if len(deeplog_df) > 0:
        violation_rate = failure_count / len(deeplog_df) * 100
        print(f"DeepLog ìœ„ë°˜ìœ¨: {violation_rate:.1f}%")
    else:
        print("DeepLog ìœ„ë°˜ìœ¨: N/A (ì¶”ë¡  ê²°ê³¼ ì—†ìŒ)")
    print(f"í‰ê·  ì´ìƒ ì ìˆ˜: {baseline_scores['score'].mean():.3f}")
    print(f"ìµœëŒ€ ì´ìƒ ì ìˆ˜: {baseline_scores['score'].max():.3f}")
    
    # 7. ì´ìƒ ìœˆë„ìš° ìƒì„¸ ë¶„ì„
    anomalies = baseline_scores[baseline_scores['is_anomaly'] == True]
    if len(anomalies) > 0:
        print(f"\nğŸš¨ ì´ìƒ ìœˆë„ìš° ìƒì„¸ ë¶„ì„")
        print("="*60)
        
        print("ì´ìƒ ì ìˆ˜ë³„ ë¶„í¬:")
        for _, row in anomalies.sort_values('score', ascending=False).head(5).iterrows():
            start_line = int(row['window_start_line'])
            score = row['score']
            unseen_rate = row['unseen_rate']
            print(f"  ë¼ì¸ {start_line:3d}: ì ìˆ˜={score:.3f}, ë¯¸ì§€ìœ¨={unseen_rate:.3f}")
    
    # 8. í…œí”Œë¦¿ë³„ ìƒì„¸ ì •ë³´
    print(f"\nğŸ”§ ì£¼ìš” í…œí”Œë¦¿ ì •ë³´")
    print("="*60)
    
    for template_id, count in template_counts.head(5).items():
        template = parsed_df[parsed_df['template_id'] == template_id]['template'].iloc[0]
        print(f"Template {template_id} ({count}íšŒ):")
        print(f"  {template[:70]}...")
        print()

def create_summary_report(data_dir: str = "data/processed"):
    """ê°„ë‹¨í•œ ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    data_path = Path(data_dir)
    
    parsed_df = pd.read_parquet(data_path / "parsed.parquet")
    baseline_scores = pd.read_parquet(data_path / "baseline_scores.parquet")
    deeplog_df = pd.read_parquet(data_path / "deeplog_infer.parquet")
    
    anomaly_rate = len(baseline_scores[baseline_scores['is_anomaly']==True])/len(baseline_scores)*100

    # Enhanced ë²„ì „: prediction_ok ì‚¬ìš©, ê¸°ì¡´ ë²„ì „: in_topk ì‚¬ìš©
    if len(deeplog_df) > 0:
        if 'prediction_ok' in deeplog_df.columns:
            violation_rate = len(deeplog_df[deeplog_df['prediction_ok']==False])/len(deeplog_df)*100
        elif 'in_topk' in deeplog_df.columns:
            violation_rate = len(deeplog_df[deeplog_df['in_topk']==False])/len(deeplog_df)*100
        else:
            violation_rate = 0
    else:
        violation_rate = 0
    
    # ì•ˆì „í•œ ì‹œê°„ ì •ë³´ ì²˜ë¦¬
    try:
        min_time = parsed_df['timestamp'].min()
        max_time = parsed_df['timestamp'].max()
        if pd.isna(min_time) or pd.isna(max_time):
            time_range = "ì‹œê°„ ì •ë³´ ì—†ìŒ"
        else:
            time_range = f"{min_time} ~ {max_time}"
    except Exception:
        time_range = "ì‹œê°„ ì •ë³´ ì²˜ë¦¬ ì˜¤ë¥˜"
    
    report = f"""
# ë¡œê·¸ ì´ìƒ íƒì§€ ê²°ê³¼ ìš”ì•½

## ê¸°ë³¸ ì •ë³´
- ë¶„ì„ ë°ì´í„°: {data_dir}
- ì´ ë¡œê·¸ ë¼ì¸: {len(parsed_df):,}ê°œ
- ë¶„ì„ ê¸°ê°„: {time_range}

## í•µì‹¬ ì§€í‘œ
- **ì´ìƒë¥ **: {anomaly_rate:.1f}% ({len(baseline_scores[baseline_scores['is_anomaly']==True])}ê°œ / {len(baseline_scores)}ê°œ ìœˆë„ìš°)
- **DeepLog ìœ„ë°˜ìœ¨**: {violation_rate:.1f}% ({int(violation_rate * len(deeplog_df) / 100) if len(deeplog_df) > 0 else 0}ê°œ / {len(deeplog_df)}ê°œ ì‹œí€€ìŠ¤)

## í•´ì„
"""
    
    if anomaly_rate < 5:
        report += "âœ… ë‚®ì€ ì´ìƒë¥  - ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™\n"
    elif anomaly_rate < 20:
        report += "ğŸ” ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì´ìƒë¥  - ì¼ë¶€ ë¹„ì •ìƒ íŒ¨í„´ ì¡´ì¬\n"
    else:
        report += "âš ï¸ ë†’ì€ ì´ìƒë¥  - ì‹œìŠ¤í…œ ì ê²€ í•„ìš”\n"
    
    if violation_rate < 20:
        report += "âœ… ë‚®ì€ ìœ„ë°˜ìœ¨ - ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë¡œê·¸ íŒ¨í„´\n"
    elif violation_rate < 50:
        report += "ğŸ” ì¤‘ê°„ ìˆ˜ì¤€ì˜ ìœ„ë°˜ìœ¨ - ì¼ë¶€ ë³µì¡í•œ íŒ¨í„´\n"
    else:
        report += "âš ï¸ ë†’ì€ ìœ„ë°˜ìœ¨ - ë§¤ìš° ë³µì¡í•˜ê±°ë‚˜ ë¹„ì •ìƒì ì¸ íŒ¨í„´\n"
    
    return report

def main():
    parser = argparse.ArgumentParser(description="Log Anomaly Detection ê²°ê³¼ ì‹œê°í™”")
    parser.add_argument("--data-dir", default="data/processed", 
                       help="ê²°ê³¼ ë°ì´í„° ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data/processed)")
    parser.add_argument("--summary", action="store_true",
                       help="ìš”ì•½ ë¦¬í¬íŠ¸ë§Œ ì¶œë ¥")
    
    args = parser.parse_args()
    
    if args.summary:
        print(create_summary_report(args.data_dir))
    else:
        visualize_results(args.data_dir)

if __name__ == "__main__":
    main()
