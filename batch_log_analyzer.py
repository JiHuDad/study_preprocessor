#!/usr/bin/env python3
"""
ë°°ì¹˜ ë¡œê·¸ ë¶„ì„ê¸°
- í´ë” ë‚´ ì—¬ëŸ¬ ë¡œê·¸ íŒŒì¼ë“¤ì„ ìë™ ì „ì²˜ë¦¬ ë° ë¶„ì„
- íŠ¹ì • Target íŒŒì¼ê³¼ ë‹¤ë¥¸ íŒŒì¼ë“¤ì„ ë¹„êµ ë¶„ì„
- ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ë„ í•¨ê»˜ ìˆ˜í–‰
"""
import os
import sys
import subprocess
import pandas as pd
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional
import argparse
import shutil
from glob import glob

class BatchLogAnalyzer:
    def __init__(self, work_dir: str = None):
        self.work_dir = Path(work_dir or "batch_analysis")
        self.work_dir.mkdir(parents=True, exist_ok=True)
        self.processed_files = {}
        self.analysis_results = {}
        
    def find_log_files(self, input_dir: str, pattern: str = "*.log") -> List[Path]:
        """ì…ë ¥ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œê·¸ íŒŒì¼ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤."""
        input_path = Path(input_dir)
        log_files = []
        
        # íŒ¨í„´ì— ë§ëŠ” íŒŒì¼ë“¤ ì°¾ê¸°
        for pattern_variant in [pattern, "*.txt", "*.log*"]:
            files = list(input_path.glob(pattern_variant))
            log_files.extend(files)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        log_files = sorted(list(set(log_files)))
        
        print(f"ğŸ“‚ ë°œê²¬ëœ ë¡œê·¸ íŒŒì¼: {len(log_files)}ê°œ")
        for i, file_path in enumerate(log_files, 1):
            file_size = file_path.stat().st_size / (1024*1024)  # MB
            print(f"  {i:2d}. {file_path.name} ({file_size:.1f} MB)")
        
        return log_files
    
    def preprocess_log_file(self, log_file: Path) -> Dict:
        """ë‹¨ì¼ ë¡œê·¸ íŒŒì¼ì„ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤."""
        file_name = log_file.stem
        output_dir = self.work_dir / f"processed_{file_name}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ”„ ì „ì²˜ë¦¬ ì¤‘: {log_file.name}")
        
        try:
            # study-preprocess ëª…ë ¹ì–´ë¡œ ì „ì²˜ë¦¬ ì‹¤í–‰
            cmd = [
                sys.executable, "-m", "study_preprocessor.cli", "parse",
                "--input", str(log_file),
                "--out-dir", str(output_dir),
                "--drain-state", str(self.work_dir / f"drain_{file_name}.json")
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode != 0:
                print(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨: {log_file.name}")
                print(f"Error: {result.stderr}")
                return {
                    'success': False,
                    'error': result.stderr,
                    'file_path': log_file,
                    'output_dir': output_dir
                }
            
            # ê²°ê³¼ íŒŒì¼ í™•ì¸
            parsed_file = output_dir / "parsed.parquet"
            if not parsed_file.exists():
                print(f"âŒ íŒŒì‹± ê²°ê³¼ ì—†ìŒ: {log_file.name}")
                return {
                    'success': False,
                    'error': 'No parsed.parquet generated',
                    'file_path': log_file,
                    'output_dir': output_dir
                }
            
            # ê¸°ë³¸ í†µê³„ ìˆ˜ì§‘
            df = pd.read_parquet(parsed_file)
            stats = {
                'total_logs': len(df),
                'unique_templates': len(df['template_id'].unique()) if 'template_id' in df.columns else 0,
                'time_range': {
                    'start': str(df['timestamp'].min()) if 'timestamp' in df.columns else None,
                    'end': str(df['timestamp'].max()) if 'timestamp' in df.columns else None,
                },
                'hosts': list(df['host'].unique()) if 'host' in df.columns else [],
                'processes': list(df['process'].unique()) if 'process' in df.columns else []
            }
            
            print(f"âœ… ì™„ë£Œ: {stats['total_logs']:,}ê°œ ë¡œê·¸, {stats['unique_templates']}ê°œ í…œí”Œë¦¿")
            
            return {
                'success': True,
                'file_path': log_file,
                'output_dir': output_dir,
                'parsed_file': parsed_file,
                'stats': stats
            }
            
        except Exception as e:
            print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {log_file.name} - {e}")
            return {
                'success': False,
                'error': str(e),
                'file_path': log_file,
                'output_dir': output_dir
            }
    
    def run_temporal_analysis(self, target_result: Dict) -> Dict:
        """ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        if not target_result['success']:
            return {'success': False, 'error': 'Target preprocessing failed'}
        
        print(f"\nğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€: {target_result['file_path'].name}")
        
        try:
            # temporal_anomaly_detector ì‹¤í–‰
            cmd = [
                sys.executable, "temporal_anomaly_detector.py",
                "--data-dir", str(target_result['output_dir']),
                "--output-dir", str(target_result['output_dir'] / "temporal_analysis")
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode != 0:
                print(f"âŒ ì‹œê°„ ë¶„ì„ ì‹¤íŒ¨")
                return {'success': False, 'error': result.stderr}
            
            # ê²°ê³¼ ë¡œë“œ
            temporal_dir = target_result['output_dir'] / "temporal_analysis"
            anomalies_file = temporal_dir / "temporal_anomalies.json"
            
            temporal_result = {'success': True, 'anomalies': []}
            if anomalies_file.exists():
                with open(anomalies_file) as f:
                    temporal_result['anomalies'] = json.load(f)
            
            print(f"âœ… ì‹œê°„ ë¶„ì„ ì™„ë£Œ: {len(temporal_result['anomalies'])}ê°œ ì´ìƒ ë°œê²¬")
            return temporal_result
            
        except Exception as e:
            print(f"âŒ ì‹œê°„ ë¶„ì„ ì˜ˆì™¸: {e}")
            return {'success': False, 'error': str(e)}
    
    def run_comparative_analysis(self, target_result: Dict, baseline_results: List[Dict]) -> Dict:
        """íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        if not target_result['success']:
            return {'success': False, 'error': 'Target preprocessing failed'}
        
        # ì„±ê³µí•œ baseline íŒŒì¼ë“¤ë§Œ ì„ ë³„
        valid_baselines = [r for r in baseline_results if r['success']]
        if not valid_baselines:
            print("âš ï¸ ë¹„êµí•  baseline íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return {'success': False, 'error': 'No valid baseline files'}
        
        print(f"\nğŸ“Š íŒŒì¼ë³„ ë¹„êµ ë¶„ì„: {target_result['file_path'].name} vs {len(valid_baselines)}ê°œ íŒŒì¼")
        
        try:
            # baseline íŒŒì¼ ê²½ë¡œë“¤ ìˆ˜ì§‘
            baseline_paths = [str(r['parsed_file']) for r in valid_baselines]
            
            # comparative_anomaly_detector ì‹¤í–‰
            cmd = [
                sys.executable, "comparative_anomaly_detector.py",
                "--target", str(target_result['parsed_file']),
                "--baselines"] + baseline_paths + [
                "--output-dir", str(target_result['output_dir'] / "comparative_analysis")
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode != 0:
                print(f"âŒ ë¹„êµ ë¶„ì„ ì‹¤íŒ¨")
                return {'success': False, 'error': result.stderr}
            
            # ê²°ê³¼ ë¡œë“œ
            comp_dir = target_result['output_dir'] / "comparative_analysis"
            anomalies_file = comp_dir / "comparative_anomalies.json"
            
            comp_result = {'success': True, 'anomalies': [], 'baseline_count': len(valid_baselines)}
            if anomalies_file.exists():
                with open(anomalies_file) as f:
                    comp_result['anomalies'] = json.load(f)
            
            print(f"âœ… ë¹„êµ ë¶„ì„ ì™„ë£Œ: {len(comp_result['anomalies'])}ê°œ ì´ìƒ ë°œê²¬")
            return comp_result
            
        except Exception as e:
            print(f"âŒ ë¹„êµ ë¶„ì„ ì˜ˆì™¸: {e}")
            return {'success': False, 'error': str(e)}
    
    def generate_summary_report(self, target_result: Dict, baseline_results: List[Dict],
                              temporal_result: Dict, comparative_result: Dict) -> str:
        """ì „ì²´ ë¶„ì„ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        target_name = target_result['file_path'].name
        
        report = f"""# ë°°ì¹˜ ë¡œê·¸ ë¶„ì„ ìš”ì•½ ë¦¬í¬íŠ¸

**ë¶„ì„ ì‹œê°„**: {timestamp}  
**Target íŒŒì¼**: {target_name}  
**Baseline íŒŒì¼**: {len([r for r in baseline_results if r['success']])}ê°œ

## ğŸ“Š íŒŒì¼ë³„ ì „ì²˜ë¦¬ ê²°ê³¼

### Target íŒŒì¼: {target_name}
"""
        
        if target_result['success']:
            stats = target_result['stats']
            report += f"""- âœ… **ì„±ê³µ**: {stats['total_logs']:,}ê°œ ë¡œê·¸, {stats['unique_templates']}ê°œ í…œí”Œë¦¿
- **ì‹œê°„ ë²”ìœ„**: {stats['time_range']['start']} ~ {stats['time_range']['end']}
- **í˜¸ìŠ¤íŠ¸**: {len(stats['hosts'])}ê°œ ({', '.join(stats['hosts'][:3])}{'...' if len(stats['hosts']) > 3 else ''})
- **í”„ë¡œì„¸ìŠ¤**: {len(stats['processes'])}ê°œ
"""
        else:
            report += f"- âŒ **ì‹¤íŒ¨**: {target_result['error']}\n"
        
        report += "\n### Baseline íŒŒì¼ë“¤\n"
        for i, result in enumerate(baseline_results, 1):
            if result['success']:
                stats = result['stats']
                report += f"{i}. âœ… **{result['file_path'].name}**: {stats['total_logs']:,}ê°œ ë¡œê·¸, {stats['unique_templates']}ê°œ í…œí”Œë¦¿\n"
            else:
                report += f"{i}. âŒ **{result['file_path'].name}**: {result['error']}\n"
        
        # ì‹œê°„ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼
        report += "\n## ğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ê²°ê³¼\n\n"
        if temporal_result['success']:
            anomalies = temporal_result['anomalies']
            if anomalies:
                high_count = len([a for a in anomalies if a.get('severity') == 'high'])
                medium_count = len([a for a in anomalies if a.get('severity') == 'medium'])
                report += f"ğŸš¨ **ë°œê²¬ëœ ì´ìƒ**: {len(anomalies)}ê°œ (ì‹¬ê°: {high_count}ê°œ, ì£¼ì˜: {medium_count}ê°œ)\n\n"
                
                for anomaly in anomalies[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                    report += f"- **{anomaly.get('type', 'unknown')}** ({anomaly.get('hour', '?')}ì‹œ): {anomaly.get('description', 'No description')}\n"
                
                if len(anomalies) > 5:
                    report += f"- ... ë° {len(anomalies) - 5}ê°œ ì¶”ê°€\n"
            else:
                report += "âœ… ì‹œê°„ ê¸°ë°˜ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        else:
            report += f"âŒ ì‹œê°„ ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨: {temporal_result.get('error', 'Unknown error')}\n"
        
        # íŒŒì¼ë³„ ë¹„êµ ë¶„ì„ ê²°ê³¼
        report += "\n## ğŸ“Š íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€ ê²°ê³¼\n\n"
        if comparative_result['success']:
            anomalies = comparative_result['anomalies']
            baseline_count = comparative_result['baseline_count']
            report += f"**ë¹„êµ ëŒ€ìƒ**: {baseline_count}ê°œ baseline íŒŒì¼\n\n"
            
            if anomalies:
                high_count = len([a for a in anomalies if a.get('severity') == 'high'])
                medium_count = len([a for a in anomalies if a.get('severity') == 'medium'])
                report += f"ğŸš¨ **ë°œê²¬ëœ ì´ìƒ**: {len(anomalies)}ê°œ (ì‹¬ê°: {high_count}ê°œ, ì£¼ì˜: {medium_count}ê°œ)\n\n"
                
                for anomaly in anomalies[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                    report += f"- **{anomaly.get('type', 'unknown')}**: {anomaly.get('description', 'No description')}\n"
                
                if len(anomalies) > 5:
                    report += f"- ... ë° {len(anomalies) - 5}ê°œ ì¶”ê°€\n"
            else:
                report += "âœ… íŒŒì¼ë³„ ë¹„êµì—ì„œ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        else:
            report += f"âŒ íŒŒì¼ë³„ ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {comparative_result.get('error', 'Unknown error')}\n"
        
        # ê¶Œê³ ì‚¬í•­
        report += "\n## ğŸ’¡ ê¶Œê³ ì‚¬í•­\n\n"
        
        total_anomalies = 0
        if temporal_result['success']:
            total_anomalies += len(temporal_result.get('anomalies', []))
        if comparative_result['success']:
            total_anomalies += len(comparative_result.get('anomalies', []))
        
        if total_anomalies == 0:
            report += "âœ… ëª¨ë“  ë¶„ì„ì—ì„œ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œì´ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ê³  ìˆëŠ” ê²ƒìœ¼ë¡œ ë³´ì…ë‹ˆë‹¤.\n"
        elif total_anomalies < 5:
            report += "ğŸ” ì¼ë¶€ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„¸ ë¦¬í¬íŠ¸ë¥¼ ê²€í† í•˜ì—¬ ì¶”ê°€ ì¡°ì‚¬ê°€ í•„ìš”í•œì§€ í™•ì¸í•˜ì„¸ìš”.\n"
        else:
            report += "âš ï¸ ë‹¤ìˆ˜ì˜ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ì‹œìŠ¤í…œ ì ê²€ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
        
        report += f"""
## ğŸ“‚ ìƒì„¸ ê²°ê³¼ íŒŒì¼
- **Target ì „ì²˜ë¦¬ ê²°ê³¼**: `{target_result['output_dir']}/parsed.parquet`
- **ì‹œê°„ ê¸°ë°˜ ë¶„ì„**: `{target_result['output_dir']}/temporal_analysis/temporal_report.md`
- **íŒŒì¼ë³„ ë¹„êµ ë¶„ì„**: `{target_result['output_dir']}/comparative_analysis/comparative_report.md`

## ğŸ”§ ì¶”ê°€ ë¶„ì„ ëª…ë ¹ì–´
```bash
# ìƒì„¸ ë¶„ì„
python analyze_results.py --data-dir {target_result['output_dir']}

# ì‹œê°í™”
python visualize_results.py --data-dir {target_result['output_dir']}
```
"""
        
        return report
    
    def analyze_directory(self, input_dir: str, target_file: str = None, 
                         file_pattern: str = "*.log") -> Dict:
        """ë””ë ‰í† ë¦¬ ë‚´ ë¡œê·¸ íŒŒì¼ë“¤ì„ ì¼ê´„ ë¶„ì„í•©ë‹ˆë‹¤."""
        
        print("ğŸš€ ë°°ì¹˜ ë¡œê·¸ ë¶„ì„ ì‹œì‘")
        print(f"ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬: {input_dir}")
        print(f"ğŸ¯ ì‘ì—… ë””ë ‰í† ë¦¬: {self.work_dir}")
        
        # 1. ë¡œê·¸ íŒŒì¼ ì°¾ê¸°
        log_files = self.find_log_files(input_dir, file_pattern)
        if not log_files:
            print("âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {'success': False, 'error': 'No log files found'}
        
        # 2. Target íŒŒì¼ ê²°ì •
        if target_file:
            target_path = Path(target_file)
            if target_path not in log_files:
                # íŒŒì¼ëª…ìœ¼ë¡œ ë§¤ì¹­ ì‹œë„
                target_matches = [f for f in log_files if f.name == target_path.name]
                if target_matches:
                    target_path = target_matches[0]
                else:
                    print(f"âŒ Target íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_file}")
                    return {'success': False, 'error': f'Target file not found: {target_file}'}
        else:
            # ì²« ë²ˆì§¸ íŒŒì¼ì„ targetìœ¼ë¡œ ì„¤ì •
            target_path = log_files[0]
            print(f"ğŸ¯ Target íŒŒì¼ ìë™ ì„ íƒ: {target_path.name}")
        
        baseline_files = [f for f in log_files if f != target_path]
        print(f"ğŸ“Š Baseline íŒŒì¼: {len(baseline_files)}ê°œ")
        
        # 3. ëª¨ë“  íŒŒì¼ ì „ì²˜ë¦¬
        print(f"\n{'='*60}")
        print("ğŸ“‹ ì „ì²˜ë¦¬ ë‹¨ê³„")
        print(f"{'='*60}")
        
        # Target íŒŒì¼ ì „ì²˜ë¦¬
        target_result = self.preprocess_log_file(target_path)
        
        # Baseline íŒŒì¼ë“¤ ì „ì²˜ë¦¬
        baseline_results = []
        for baseline_file in baseline_files:
            result = self.preprocess_log_file(baseline_file)
            baseline_results.append(result)
        
        # 4. ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€
        print(f"\n{'='*60}")
        print("ğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€")
        print(f"{'='*60}")
        temporal_result = self.run_temporal_analysis(target_result)
        
        # 5. íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€
        print(f"\n{'='*60}")
        print("ğŸ“Š íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€")
        print(f"{'='*60}")
        comparative_result = self.run_comparative_analysis(target_result, baseline_results)
        
        # 6. ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        print(f"\n{'='*60}")
        print("ğŸ“„ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±")
        print(f"{'='*60}")
        
        summary_report = self.generate_summary_report(
            target_result, baseline_results, temporal_result, comparative_result
        )
        
        # ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥
        summary_file = self.work_dir / "BATCH_ANALYSIS_SUMMARY.md"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_report)
        
        print(f"ğŸ“‹ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {summary_file}")
        
        # ê²°ê³¼ ì¶œë ¥
        print(f"\n{'='*60}")
        print("âœ… ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ")
        print(f"{'='*60}")
        
        # ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥
        success_count = len([r for r in baseline_results if r['success']]) + (1 if target_result['success'] else 0)
        total_count = len(baseline_results) + 1
        print(f"ğŸ“Š ì „ì²˜ë¦¬ ì„±ê³µ: {success_count}/{total_count}ê°œ íŒŒì¼")
        
        if temporal_result['success']:
            temporal_anomalies = len(temporal_result.get('anomalies', []))
            print(f"ğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ: {temporal_anomalies}ê°œ")
        
        if comparative_result['success']:
            comp_anomalies = len(comparative_result.get('anomalies', []))
            print(f"ğŸ“Š ë¹„êµ ë¶„ì„ ì´ìƒ: {comp_anomalies}ê°œ")
        
        print(f"ğŸ“„ ìš”ì•½ ë¦¬í¬íŠ¸: {summary_file}")
        
        return {
            'success': True,
            'target_result': target_result,
            'baseline_results': baseline_results,
            'temporal_result': temporal_result,
            'comparative_result': comparative_result,
            'summary_file': summary_file
        }

def main():
    parser = argparse.ArgumentParser(description="ë°°ì¹˜ ë¡œê·¸ ë¶„ì„ê¸°")
    parser.add_argument("input_dir", help="ë¡œê·¸ íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬")
    parser.add_argument("--target", help="ë¶„ì„í•  target íŒŒì¼ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ì²« ë²ˆì§¸ íŒŒì¼)")
    parser.add_argument("--pattern", default="*.log", help="ë¡œê·¸ íŒŒì¼ íŒ¨í„´ (ê¸°ë³¸: *.log)")
    parser.add_argument("--work-dir", help="ì‘ì—… ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ./batch_analysis)")
    
    args = parser.parse_args()
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰
    analyzer = BatchLogAnalyzer(args.work_dir)
    result = analyzer.analyze_directory(
        args.input_dir, 
        args.target, 
        args.pattern
    )
    
    if not result['success']:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {result['error']}")
        sys.exit(1)
    
    print("\nğŸ‰ ëª¨ë“  ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    main()
