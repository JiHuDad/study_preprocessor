#!/usr/bin/env python3
"""
ê²°ê³¼ ë¶„ì„ ë° í•´ì„ ë„êµ¬
"""
import pandas as pd
import json
from pathlib import Path
import argparse

def analyze_results(data_dir: str = "data/processed"):
    """pipeline ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  í•´ì„í•©ë‹ˆë‹¤."""
    data_path = Path(data_dir)
    
    print("="*60)
    print("ğŸ” LOG ANOMALY DETECTION ê²°ê³¼ ë¶„ì„")
    print("="*60)
    
    # 1. ê¸°ë³¸ ì •ë³´ ì½ê¸° (ì•ˆì „í•œ ì²˜ë¦¬)
    vocab = {}
    preview = {}
    
    # vocab.json íŒŒì¼ í™•ì¸
    vocab_file = data_path / "vocab.json"
    if vocab_file.exists():
        with open(vocab_file) as f:
            vocab = json.load(f)
    else:
        print("âš ï¸ vocab.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. DeepLog ë¶„ì„ì´ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
    
    # preview.json íŒŒì¼ í™•ì¸
    preview_file = data_path / "preview.json"
    if preview_file.exists():
        with open(preview_file) as f:
            preview = json.load(f)
    else:
        print("âš ï¸ preview.json íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    # 2. íŒŒì‹±ëœ ë¡œê·¸ ë°ì´í„°
    parsed_file = data_path / "parsed.parquet"
    if not parsed_file.exists():
        print("âŒ parsed.parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ì „ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    parsed_df = pd.read_parquet(parsed_file)
    
    # sequences.parquet íŒŒì¼ í™•ì¸
    sequences_file = data_path / "sequences.parquet"
    if sequences_file.exists():
        sequences_df = pd.read_parquet(sequences_file)
    else:
        print("âš ï¸ sequences.parquet íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. DeepLog ë¶„ì„ì´ ì‹¤í–‰ë˜ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        sequences_df = None
    
    print(f"\nğŸ“Š **ê¸°ë³¸ í†µê³„**")
    print(f"- ì´ ë¡œê·¸ ë¼ì¸ ìˆ˜: {len(parsed_df)}")
    
    # í…œí”Œë¦¿ ìˆ˜ ê³„ì‚°
    if vocab:
        template_count = len(vocab)
    else:
        # vocabì´ ì—†ìœ¼ë©´ íŒŒì‹±ëœ ë°ì´í„°ì—ì„œ ì§ì ‘ ê³„ì‚°
        template_count = len(parsed_df['template_id'].unique()) if 'template_id' in parsed_df.columns else 0
    
    print(f"- ë°œê²¬ëœ í…œí”Œë¦¿ ìˆ˜: {template_count}")
    
    # ì•ˆì „í•œ timestamp ì²˜ë¦¬
    try:
        min_time = parsed_df['timestamp'].min()
        max_time = parsed_df['timestamp'].max()
        if pd.isna(min_time) or pd.isna(max_time):
            print(f"- ë¶„ì„ ê¸°ê°„: ì‹œê°„ ì •ë³´ ì—†ìŒ")
        else:
            print(f"- ë¶„ì„ ê¸°ê°„: {min_time} ~ {max_time}")
    except Exception as e:
        print(f"- ë¶„ì„ ê¸°ê°„: ì‹œê°„ ì •ë³´ ì²˜ë¦¬ ì˜¤ë¥˜ ({e})")
    
    # ì•ˆì „í•œ host ì²˜ë¦¬
    try:
        hosts = parsed_df['host'].dropna().unique()
        if len(hosts) > 0:
            print(f"- í˜¸ìŠ¤íŠ¸: {', '.join(str(h) for h in hosts if h is not None)}")
        else:
            print(f"- í˜¸ìŠ¤íŠ¸: ì •ë³´ ì—†ìŒ")
    except Exception as e:
        print(f"- í˜¸ìŠ¤íŠ¸: ì •ë³´ ì²˜ë¦¬ ì˜¤ë¥˜ ({e})")
    
    # 3. í…œí”Œë¦¿ ë¶„ì„
    print(f"\nğŸ”§ **ë¡œê·¸ í…œí”Œë¦¿ ë¶„ì„**")
    template_counts = parsed_df['template_id'].value_counts()
    for template_id, count in template_counts.head().items():
        template = parsed_df[parsed_df['template_id'] == template_id]['template'].iloc[0]
        print(f"- Template {template_id} (ì¶œí˜„ {count}íšŒ): {template[:80]}...")
    
    # 4. Baseline Anomaly Detection ê²°ê³¼
    baseline_file = data_path / "baseline_scores.parquet"
    if baseline_file.exists():
        baseline_scores = pd.read_parquet(baseline_file)
        print(f"\nğŸš¨ **Baseline Anomaly Detection ê²°ê³¼**")
        print(f"- ë¶„ì„ëœ ìœˆë„ìš° ìˆ˜: {len(baseline_scores)}")
        anomalies = baseline_scores[baseline_scores['is_anomaly'] == True]
        print(f"- ë°œê²¬ëœ ì´ìƒ ìœˆë„ìš°: {len(anomalies)}ê°œ (ì „ì²´ì˜ {len(anomalies)/len(baseline_scores)*100:.1f}%)")
    else:
        print(f"\nâš ï¸ **Baseline Anomaly Detection ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤**")
        baseline_scores = None
        anomalies = None
    
    if anomalies is not None and len(anomalies) > 0:
        print("\nğŸ” **ì´ìƒ ìœˆë„ìš° ìƒì„¸**:")
        for _, row in anomalies.iterrows():
            start_line = int(row['window_start_line'])
            score = row['score']
            unseen_rate = row['unseen_rate']
            print(f"  - ìœˆë„ìš° ì‹œì‘ë¼ì¸ {start_line}: ì ìˆ˜={score:.3f}, ë¯¸ì§€í…œí”Œë¦¿ë¹„ìœ¨={unseen_rate:.3f}")
            
            # í•´ë‹¹ ìœˆë„ìš°ì˜ ë¡œê·¸ ë‚´ìš© í‘œì‹œ
            window_logs = parsed_df[parsed_df['line_no'] >= start_line].head(3)  # ìœˆë„ìš° í¬ê¸° ê°€ì •
            for _, log in window_logs.iterrows():
                print(f"    Line {log['line_no']}: {log['raw'][:60]}...")
    
    # 5. DeepLog ê²°ê³¼
    deeplog_file = data_path / "deeplog_infer.parquet"
    if deeplog_file.exists():
        deeplog_df = pd.read_parquet(deeplog_file)
        print(f"\nğŸ§  **DeepLog ë”¥ëŸ¬ë‹ ëª¨ë¸ ê²°ê³¼**")
        violations = deeplog_df[deeplog_df['in_topk'] == False]
        print(f"- ì˜ˆì¸¡ ì‹¤íŒ¨ (ìœ„ë°˜): {len(violations)}ê°œ / {len(deeplog_df)}ê°œ")
    else:
        print(f"\nâš ï¸ **DeepLog ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤**")
        deeplog_df = None
        violations = None
    if deeplog_df is not None and len(deeplog_df) > 0:
        print(f"- ìœ„ë°˜ìœ¨: {len(violations)/len(deeplog_df)*100:.1f}%")
    else:
        print("- ìœ„ë°˜ìœ¨: N/A (ì¶”ë¡  ê²°ê³¼ ì—†ìŒ)")
    
    if violations is not None and len(violations) > 0:
        print("\nğŸ” **DeepLog ìœ„ë°˜ ìƒì„¸**:")
        for _, row in violations.iterrows():
            idx = int(row['idx'])
            target = int(row['target'])
            # ì‹¤ì œ ë¡œê·¸ ë¼ì¸ ì°¾ê¸°
            if idx < len(parsed_df):
                log_line = parsed_df.iloc[idx]
                target_template = vocab.get(str(target), f"Unknown({target})") if vocab else f"Template({target})"
                print(f"  - Line {log_line['line_no']}: ì˜ˆìƒ í…œí”Œë¦¿ {target}ë²ˆì´ top-kì— ì—†ìŒ")
                print(f"    ì‹¤ì œ ë¡œê·¸: {log_line['raw'][:60]}...")
    
    # 6. ì¢…í•© í•´ì„
    print(f"\nğŸ“ˆ **ì¢…í•© í•´ì„**")
    
    # ì•ˆì „í•œ ë¹„ìœ¨ ê³„ì‚°
    if baseline_scores is not None and anomalies is not None:
        baseline_anomaly_rate = len(anomalies) / len(baseline_scores) * 100
        if baseline_anomaly_rate > 20:
            print("âš ï¸  Baseline ëª¨ë¸ì—ì„œ ë†’ì€ ì´ìƒë¥  ê°ì§€ - ì‹œìŠ¤í…œì— ë¹„ì •ìƒì ì¸ íŒ¨í„´ì´ ë§ìŒ")
        elif baseline_anomaly_rate > 5:
            print("ğŸ” Baseline ëª¨ë¸ì—ì„œ ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì´ìƒ ê°ì§€ - ì¼ë¶€ ë¹„ì •ìƒ íŒ¨í„´ ì¡´ì¬")
        else:
            print("âœ… Baseline ëª¨ë¸ì—ì„œ ë‚®ì€ ì´ìƒë¥  - ëŒ€ë¶€ë¶„ ì •ìƒì ì¸ ë¡œê·¸ íŒ¨í„´")
    else:
        print("â„¹ï¸  Baseline ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ ì´ìƒë¥ ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    if deeplog_df is not None and violations is not None and len(deeplog_df) > 0:
        deeplog_violation_rate = len(violations) / len(deeplog_df) * 100
        if deeplog_violation_rate > 50:
            print("âš ï¸  DeepLog ëª¨ë¸ì—ì„œ ë†’ì€ ìœ„ë°˜ìœ¨ - ë¡œê·¸ ì‹œí€€ìŠ¤ê°€ ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ íŒ¨í„´")
        elif deeplog_violation_rate > 20:
            print("ğŸ” DeepLog ëª¨ë¸ì—ì„œ ì¤‘ê°„ ìˆ˜ì¤€ì˜ ìœ„ë°˜ìœ¨ - ì¼ë¶€ ì˜ˆì¸¡ ì–´ë ¤ìš´ ì‹œí€€ìŠ¤")
        else:
            print("âœ… DeepLog ëª¨ë¸ì—ì„œ ë‚®ì€ ìœ„ë°˜ìœ¨ - ëŒ€ë¶€ë¶„ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë¡œê·¸ ì‹œí€€ìŠ¤")
    else:
        print("â„¹ï¸  DeepLog ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ ìœ„ë°˜ìœ¨ì„ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    
    # 7. ë¦¬í¬íŠ¸ íŒŒì¼ì´ ìˆë‹¤ë©´ í‘œì‹œ
    report_file = data_path / "report.md"
    if report_file.exists():
        print(f"\nğŸ“„ **ìë™ ìƒì„±ëœ ë¦¬í¬íŠ¸**")
        with open(report_file) as f:
            print(f.read())

def main():
    parser = argparse.ArgumentParser(description="Log Anomaly Detection ê²°ê³¼ ë¶„ì„")
    parser.add_argument("--data-dir", default="data/processed", 
                       help="ê²°ê³¼ ë°ì´í„° ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data/processed)")
    
    args = parser.parse_args()
    analyze_results(args.data_dir)

if __name__ == "__main__":
    main()
