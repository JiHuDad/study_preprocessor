#!/usr/bin/env python3
"""
ê²°ê³¼ ë¶„ì„ ë° í•´ì„ ë„êµ¬
"""
import pandas as pd
import json
from pathlib import Path
import argparse

def analyze_results(data_dir: str = "data/processed"):
    """pipeline ì‹¤í–‰ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  í•´ì„í•©ë‹ˆë‹¤."""
    data_path = Path(data_dir)
    
    print("="*60)
    print("ğŸ” LOG ANOMALY DETECTION ê²°ê³¼ ë¶„ì„")
    print("="*60)
    
    # 1. ê¸°ë³¸ ì •ë³´ ì½ê¸°
    with open(data_path / "vocab.json") as f:
        vocab = json.load(f)
    
    with open(data_path / "preview.json") as f:
        preview = json.load(f)
    
    # 2. íŒŒì‹±ëœ ë¡œê·¸ ë°ì´í„°
    parsed_df = pd.read_parquet(data_path / "parsed.parquet")
    sequences_df = pd.read_parquet(data_path / "sequences.parquet")
    
    print(f"\nğŸ“Š **ê¸°ë³¸ í†µê³„**")
    print(f"- ì´ ë¡œê·¸ ë¼ì¸ ìˆ˜: {len(parsed_df)}")
    print(f"- ë°œê²¬ëœ í…œí”Œë¦¿ ìˆ˜: {len(vocab)}")
    
    # ì•ˆì „í•œ timestamp ì²˜ë¦¬
    try:
        min_time = parsed_df['timestamp'].min()
        max_time = parsed_df['timestamp'].max()
        if pd.isna(min_time) or pd.isna(max_time):
            print(f"- ë¶„ì„ ê¸°ê°„: ì‹œê°„ ì •ë³´ ì—†ìŒ")
        else:
            print(f"- ë¶„ì„ ê¸°ê°„: {min_time} ~ {max_time}")
    except Exception as e:
        print(f"- ë¶„ì„ ê¸°ê°„: ì‹œê°„ ì •ë³´ ì²˜ë¦¬ ì˜¤ë¥˜ ({e})")
    
    # ì•ˆì „í•œ host ì²˜ë¦¬
    try:
        hosts = parsed_df['host'].dropna().unique()
        if len(hosts) > 0:
            print(f"- í˜¸ìŠ¤íŠ¸: {', '.join(str(h) for h in hosts if h is not None)}")
        else:
            print(f"- í˜¸ìŠ¤íŠ¸: ì •ë³´ ì—†ìŒ")
    except Exception as e:
        print(f"- í˜¸ìŠ¤íŠ¸: ì •ë³´ ì²˜ë¦¬ ì˜¤ë¥˜ ({e})")
    
    # 3. í…œí”Œë¦¿ ë¶„ì„
    print(f"\nğŸ”§ **ë¡œê·¸ í…œí”Œë¦¿ ë¶„ì„**")
    template_counts = parsed_df['template_id'].value_counts()
    for template_id, count in template_counts.head().items():
        template = parsed_df[parsed_df['template_id'] == template_id]['template'].iloc[0]
        print(f"- Template {template_id} (ì¶œí˜„ {count}íšŒ): {template[:80]}...")
    
    # 4. Baseline Anomaly Detection ê²°ê³¼
    baseline_scores = pd.read_parquet(data_path / "baseline_scores.parquet")
    print(f"\nğŸš¨ **Baseline Anomaly Detection ê²°ê³¼**")
    print(f"- ë¶„ì„ëœ ìœˆë„ìš° ìˆ˜: {len(baseline_scores)}")
    anomalies = baseline_scores[baseline_scores['is_anomaly'] == True]
    print(f"- ë°œê²¬ëœ ì´ìƒ ìœˆë„ìš°: {len(anomalies)}ê°œ (ì „ì²´ì˜ {len(anomalies)/len(baseline_scores)*100:.1f}%)")
    
    if len(anomalies) > 0:
        print("\nğŸ” **ì´ìƒ ìœˆë„ìš° ìƒì„¸**:")
        for _, row in anomalies.iterrows():
            start_line = int(row['window_start_line'])
            score = row['score']
            unseen_rate = row['unseen_rate']
            print(f"  - ìœˆë„ìš° ì‹œì‘ë¼ì¸ {start_line}: ì ìˆ˜={score:.3f}, ë¯¸ì§€í…œí”Œë¦¿ë¹„ìœ¨={unseen_rate:.3f}")
            
            # í•´ë‹¹ ìœˆë„ìš°ì˜ ë¡œê·¸ ë‚´ìš© í‘œì‹œ
            window_logs = parsed_df[parsed_df['line_no'] >= start_line].head(3)  # ìœˆë„ìš° í¬ê¸° ê°€ì •
            for _, log in window_logs.iterrows():
                print(f"    Line {log['line_no']}: {log['raw'][:60]}...")
    
    # 5. DeepLog ê²°ê³¼
    deeplog_df = pd.read_parquet(data_path / "deeplog_infer.parquet")
    print(f"\nğŸ§  **DeepLog ë”¥ëŸ¬ë‹ ëª¨ë¸ ê²°ê³¼**")
    violations = deeplog_df[deeplog_df['in_topk'] == False]
    print(f"- ì˜ˆì¸¡ ì‹¤íŒ¨ (ìœ„ë°˜): {len(violations)}ê°œ / {len(deeplog_df)}ê°œ")
    if len(deeplog_df) > 0:
        print(f"- ìœ„ë°˜ìœ¨: {len(violations)/len(deeplog_df)*100:.1f}%")
    else:
        print("- ìœ„ë°˜ìœ¨: N/A (ì¶”ë¡  ê²°ê³¼ ì—†ìŒ)")
    
    if len(violations) > 0:
        print("\nğŸ” **DeepLog ìœ„ë°˜ ìƒì„¸**:")
        for _, row in violations.iterrows():
            idx = int(row['idx'])
            target = int(row['target'])
            # ì‹¤ì œ ë¡œê·¸ ë¼ì¸ ì°¾ê¸°
            if idx < len(parsed_df):
                log_line = parsed_df.iloc[idx]
                target_template = vocab.get(str(target), f"Unknown({target})")
                print(f"  - Line {log_line['line_no']}: ì˜ˆìƒ í…œí”Œë¦¿ {target}ë²ˆì´ top-kì— ì—†ìŒ")
                print(f"    ì‹¤ì œ ë¡œê·¸: {log_line['raw'][:60]}...")
    
    # 6. ì¢…í•© í•´ì„
    print(f"\nğŸ“ˆ **ì¢…í•© í•´ì„**")
    baseline_anomaly_rate = len(anomalies) / len(baseline_scores) * 100
    deeplog_violation_rate = len(violations) / len(deeplog_df) * 100 if len(deeplog_df) > 0 else 0
    
    if baseline_anomaly_rate > 20:
        print("âš ï¸  Baseline ëª¨ë¸ì—ì„œ ë†’ì€ ì´ìƒë¥  ê°ì§€ - ì‹œìŠ¤í…œì— ë¹„ì •ìƒì ì¸ íŒ¨í„´ì´ ë§ìŒ")
    elif baseline_anomaly_rate > 5:
        print("ğŸ” Baseline ëª¨ë¸ì—ì„œ ì¤‘ê°„ ìˆ˜ì¤€ì˜ ì´ìƒ ê°ì§€ - ì¼ë¶€ ë¹„ì •ìƒ íŒ¨í„´ ì¡´ì¬")
    else:
        print("âœ… Baseline ëª¨ë¸ì—ì„œ ë‚®ì€ ì´ìƒë¥  - ëŒ€ë¶€ë¶„ ì •ìƒì ì¸ ë¡œê·¸ íŒ¨í„´")
    
    if deeplog_violation_rate > 50:
        print("âš ï¸  DeepLog ëª¨ë¸ì—ì„œ ë†’ì€ ìœ„ë°˜ìœ¨ - ë¡œê·¸ ì‹œí€€ìŠ¤ê°€ ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ íŒ¨í„´")
    elif deeplog_violation_rate > 20:
        print("ğŸ” DeepLog ëª¨ë¸ì—ì„œ ì¤‘ê°„ ìˆ˜ì¤€ì˜ ìœ„ë°˜ìœ¨ - ì¼ë¶€ ì˜ˆì¸¡ ì–´ë ¤ìš´ ì‹œí€€ìŠ¤")
    else:
        print("âœ… DeepLog ëª¨ë¸ì—ì„œ ë‚®ì€ ìœ„ë°˜ìœ¨ - ëŒ€ë¶€ë¶„ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë¡œê·¸ ì‹œí€€ìŠ¤")
    
    # 7. ë¦¬í¬íŠ¸ íŒŒì¼ì´ ìˆë‹¤ë©´ í‘œì‹œ
    report_file = data_path / "report.md"
    if report_file.exists():
        print(f"\nğŸ“„ **ìë™ ìƒì„±ëœ ë¦¬í¬íŠ¸**")
        with open(report_file) as f:
            print(f.read())

def main():
    parser = argparse.ArgumentParser(description="Log Anomaly Detection ê²°ê³¼ ë¶„ì„")
    parser.add_argument("--data-dir", default="data/processed", 
                       help="ê²°ê³¼ ë°ì´í„° ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data/processed)")
    
    args = parser.parse_args()
    analyze_results(args.data_dir)

if __name__ == "__main__":
    main()
