#!/usr/bin/env python3
"""
í–¥ìƒëœ ë°°ì¹˜ ë¡œê·¸ ë¶„ì„ê¸°
- í•˜ìœ„ ë””ë ‰í† ë¦¬ ì¬ê·€ì  ìŠ¤ìº” ì§€ì›
- ë‚ ì§œë³„/ì¹´í…Œê³ ë¦¬ë³„ í´ë” êµ¬ì¡° ì§€ì›
- ë¡œê·¸ í˜•ì‹ ìë™ ê°ì§€ ë° ê²€ì¦
- ì „ì²˜ë¦¬ ì˜¤ë¥˜ ë””ë²„ê¹… ê°•í™”
"""
import os
import sys
import subprocess
import pandas as pd
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import argparse
import shutil
from glob import glob
import re

class EnhancedBatchAnalyzer:
    def __init__(self, work_dir: str = None):
        self.work_dir = Path(work_dir or f"enhanced_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        self.work_dir.mkdir(parents=True, exist_ok=True)
        self.processed_files = {}
        self.analysis_results = {}
        
        # ë¡œê·¸ íŒŒì¼ íŒ¨í„´ë“¤
        self.log_patterns = [
            "*.log", "*.txt", "*.out", 
            "*.log.*", "*.syslog", "*.messages",
            "*.access", "*.error", "*.debug"
        ]
        
    def find_log_files_recursive(self, input_dir: str, max_depth: int = 3) -> List[Tuple[Path, str]]:
        """í•˜ìœ„ ë””ë ‰í† ë¦¬ë¥¼ í¬í•¨í•˜ì—¬ ë¡œê·¸ íŒŒì¼ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤."""
        input_path = Path(input_dir)
        log_files = []
        
        print(f"ğŸ“‚ ë¡œê·¸ íŒŒì¼ ê²€ìƒ‰ ì¤‘: {input_dir} (ìµœëŒ€ ê¹Šì´: {max_depth})")
        
        def scan_directory(dir_path: Path, current_depth: int = 0):
            if current_depth > max_depth:
                return
                
            try:
                # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œê·¸ íŒŒì¼ ì°¾ê¸°
                for pattern in self.log_patterns:
                    for file_path in dir_path.glob(pattern):
                        if file_path.is_file() and file_path.stat().st_size > 0:
                            # ìƒëŒ€ ê²½ë¡œ ìƒì„± (ë””ë ‰í† ë¦¬ êµ¬ì¡° ì •ë³´ ë³´ì¡´)
                            rel_path = file_path.relative_to(input_path)
                            category = str(rel_path.parent) if rel_path.parent != Path('.') else "root"
                            log_files.append((file_path, category))
                
                # í•˜ìœ„ ë””ë ‰í† ë¦¬ ì¬ê·€ ìŠ¤ìº”
                for item in dir_path.iterdir():
                    if item.is_dir() and not item.name.startswith('.'):
                        scan_directory(item, current_depth + 1)
                        
            except PermissionError:
                print(f"âš ï¸  ê¶Œí•œ ì—†ìŒ: {dir_path}")
            except Exception as e:
                print(f"âš ï¸  ìŠ¤ìº” ì˜¤ë¥˜: {dir_path} - {e}")
        
        scan_directory(input_path)
        
        # íŒŒì¼ í¬ê¸°ë³„ ì •ë ¬ (í° íŒŒì¼ë¶€í„°)
        log_files.sort(key=lambda x: x[0].stat().st_size, reverse=True)
        
        print(f"ğŸ“Š ë°œê²¬ëœ ë¡œê·¸ íŒŒì¼: {len(log_files)}ê°œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í™” ì¶œë ¥
        categories = {}
        for file_path, category in log_files:
            if category not in categories:
                categories[category] = []
            categories[category].append(file_path)
        
        for category, files in categories.items():
            print(f"  ğŸ“ {category}: {len(files)}ê°œ íŒŒì¼")
            for i, file_path in enumerate(files[:3], 1):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                file_size = file_path.stat().st_size / (1024*1024)  # MB
                print(f"    {i}. {file_path.name} ({file_size:.1f} MB)")
            if len(files) > 3:
                print(f"    ... ë° {len(files) - 3}ê°œ ì¶”ê°€")
        
        return log_files
    
    def validate_log_format(self, log_file: Path) -> Dict:
        """ë¡œê·¸ íŒŒì¼ì˜ í˜•ì‹ì„ ê²€ì¦í•˜ê³  ìƒ˜í”Œì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        try:
            # íŒŒì¼ í¬ê¸° ì²´í¬
            file_size = log_file.stat().st_size
            if file_size == 0:
                return {'valid': False, 'error': 'Empty file'}
            
            # ì²˜ìŒ 10ë¼ì¸ ì½ê¸°
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                sample_lines = []
                for i, line in enumerate(f):
                    if i >= 10:
                        break
                    sample_lines.append(line.strip())
            
            if not sample_lines:
                return {'valid': False, 'error': 'No readable lines'}
            
            # ë¡œê·¸ í˜•ì‹ íŒ¨í„´ ë¶„ì„
            patterns = {
                'syslog': r'^\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}\s+\S+\s+\S+:',  # Sep 14 05:04:41 host1 kernel:
                'timestamp_iso': r'^\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}',      # 2025-09-17 10:15:32
                'timestamp_bracket': r'^\[\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}\]', # [2025-09-17 10:15:32]
                'apache_combined': r'^\S+\s+\S+\s+\S+\s+\[',                        # Apache combined log
                'json': r'^\s*\{.*\}\s*$',                                         # JSON format
                'generic': r'.',                                                    # Fallback
            }
            
            detected_format = 'unknown'
            for format_name, pattern in patterns.items():
                if any(re.match(pattern, line) for line in sample_lines):
                    detected_format = format_name
                    break
            
            # í†µê³„ ìˆ˜ì§‘
            total_lines = sum(1 for _ in open(log_file, 'r', encoding='utf-8', errors='ignore'))
            
            return {
                'valid': True,
                'format': detected_format,
                'file_size_mb': file_size / (1024*1024),
                'total_lines': total_lines,
                'sample_lines': sample_lines[:3],
                'encoding': 'utf-8'  # ê¸°ë³¸ê°’
            }
            
        except UnicodeDecodeError:
            # UTF-8 ì‹¤íŒ¨ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
            try:
                with open(log_file, 'r', encoding='latin-1') as f:
                    sample_lines = [f.readline().strip() for _ in range(3)]
                return {
                    'valid': True,
                    'format': 'binary_or_latin1',
                    'file_size_mb': file_size / (1024*1024),
                    'sample_lines': sample_lines,
                    'encoding': 'latin-1',
                    'warning': 'Non-UTF8 encoding detected'
                }
            except Exception:
                return {'valid': False, 'error': 'Encoding issues'}
        
        except Exception as e:
            return {'valid': False, 'error': str(e)}
    
    def preprocess_log_file(self, log_file: Path, category: str = "") -> Dict:
        """ë‹¨ì¼ ë¡œê·¸ íŒŒì¼ì„ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë””ë²„ê¹… ê°•í™”)."""
        file_name = log_file.stem
        if category and category != "root":
            safe_category = re.sub(r'[^\w\-_]', '_', category)
            output_dir = self.work_dir / f"processed_{safe_category}_{file_name}"
        else:
            output_dir = self.work_dir / f"processed_{file_name}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ”„ ì „ì²˜ë¦¬ ì¤‘: {log_file.name} (ì¹´í…Œê³ ë¦¬: {category or 'root'})")
        
        # ë¡œê·¸ í˜•ì‹ ê²€ì¦
        validation = self.validate_log_format(log_file)
        if not validation['valid']:
            print(f"âŒ ë¡œê·¸ í˜•ì‹ ì˜¤ë¥˜: {validation['error']}")
            return {
                'success': False,
                'error': f"Log format validation failed: {validation['error']}",
                'file_path': log_file,
                'category': category,
                'output_dir': output_dir
            }
        
        print(f"ğŸ“‹ íŒŒì¼ ì •ë³´: {validation['file_size_mb']:.1f}MB, {validation.get('total_lines', '?')}ë¼ì¸, í˜•ì‹: {validation['format']}")
        
        try:
            # study-preprocess ëª…ë ¹ì–´ë¡œ ì „ì²˜ë¦¬ ì‹¤í–‰ (ê°€ìƒí™˜ê²½ python ì‚¬ìš©)
            venv_python = Path(".venv/bin/python")
            if venv_python.exists():
                python_cmd = str(venv_python)
            else:
                python_cmd = sys.executable
            
            cmd = [
                python_cmd, "-m", "study_preprocessor.cli", "parse",
                "--input", str(log_file),
                "--out-dir", str(output_dir),
                "--drain-state", str(self.work_dir / f"drain_{file_name}.json")
            ]
            
            print(f"ğŸ”§ ì‹¤í–‰ ëª…ë ¹ì–´: {' '.join(cmd)}")
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode != 0:
                print(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨ (ì½”ë“œ: {result.returncode})")
                print(f"ğŸ“„ í‘œì¤€ ì¶œë ¥: {result.stdout}")
                print(f"ğŸ“„ í‘œì¤€ ì—ëŸ¬: {result.stderr}")
                return {
                    'success': False,
                    'error': f"Process failed with code {result.returncode}: {result.stderr}",
                    'stdout': result.stdout,
                    'stderr': result.stderr,
                    'file_path': log_file,
                    'category': category,
                    'output_dir': output_dir,
                    'validation': validation
                }
            
            # ê²°ê³¼ íŒŒì¼ í™•ì¸
            parsed_file = output_dir / "parsed.parquet"
            if not parsed_file.exists():
                print(f"âŒ íŒŒì‹± ê²°ê³¼ ì—†ìŒ: parsed.parquetì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ë‚´ìš©:")
                try:
                    for item in output_dir.iterdir():
                        print(f"  - {item.name}")
                except:
                    print("  (ë””ë ‰í† ë¦¬ ì½ê¸° ì‹¤íŒ¨)")
                
                return {
                    'success': False,
                    'error': 'No parsed.parquet generated',
                    'stdout': result.stdout,
                    'file_path': log_file,
                    'category': category,
                    'output_dir': output_dir,
                    'validation': validation
                }
            
            # ê²°ê³¼ í†µê³„ ìˆ˜ì§‘
            df = pd.read_parquet(parsed_file)
            stats = {
                'total_logs': len(df),
                'unique_templates': len(df['template_id'].unique()) if 'template_id' in df.columns else 0,
                'time_range': {
                    'start': str(df['timestamp'].min()) if 'timestamp' in df.columns else None,
                    'end': str(df['timestamp'].max()) if 'timestamp' in df.columns else None,
                },
                'hosts': list(df['host'].unique()) if 'host' in df.columns else [],
                'processes': list(df['process'].unique()) if 'process' in df.columns else []
            }
            
            print(f"âœ… ì™„ë£Œ: {stats['total_logs']:,}ê°œ ë¡œê·¸, {stats['unique_templates']}ê°œ í…œí”Œë¦¿")
            
            return {
                'success': True,
                'file_path': log_file,
                'category': category,
                'output_dir': output_dir,
                'parsed_file': parsed_file,
                'stats': stats,
                'validation': validation
            }
            
        except Exception as e:
            print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {log_file.name} - {e}")
            return {
                'success': False,
                'error': str(e),
                'file_path': log_file,
                'category': category,
                'output_dir': output_dir,
                'validation': validation
            }
    
    def select_target_and_baselines(self, log_files: List[Tuple[Path, str]], 
                                  target_file: str = None) -> Tuple[Tuple[Path, str], List[Tuple[Path, str]]]:
        """Target íŒŒì¼ê³¼ Baseline íŒŒì¼ë“¤ì„ ì„ íƒí•©ë‹ˆë‹¤."""
        
        if target_file:
            # ì§€ì •ëœ Target íŒŒì¼ ì°¾ê¸°
            target_matches = [
                (f, c) for f, c in log_files 
                if f.name == target_file or str(f) == target_file
            ]
            
            if target_matches:
                target = target_matches[0]
                baselines = [(f, c) for f, c in log_files if (f, c) != target]
            else:
                print(f"âš ï¸  ì§€ì •ëœ Target íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {target_file}")
                print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤:")
                for i, (f, c) in enumerate(log_files[:10], 1):
                    print(f"  {i}. {f.name} (ì¹´í…Œê³ ë¦¬: {c})")
                target = log_files[0] if log_files else None
                baselines = log_files[1:] if len(log_files) > 1 else []
        else:
            # ê°€ì¥ í° íŒŒì¼ì„ Targetìœ¼ë¡œ ì„ íƒ
            target = log_files[0] if log_files else None
            baselines = log_files[1:] if len(log_files) > 1 else []
        
        if target:
            print(f"ğŸ¯ Target íŒŒì¼: {target[0].name} (ì¹´í…Œê³ ë¦¬: {target[1]})")
            print(f"ğŸ“Š Baseline íŒŒì¼: {len(baselines)}ê°œ")
            
            # ì¹´í…Œê³ ë¦¬ë³„ Baseline ë¶„í¬
            baseline_categories = {}
            for _, category in baselines:
                baseline_categories[category] = baseline_categories.get(category, 0) + 1
            
            for category, count in baseline_categories.items():
                print(f"  - {category}: {count}ê°œ")
        
        return target, baselines
    
    def run_enhanced_analysis(self, input_dir: str, target_file: str = None, 
                            max_depth: int = 3, max_files: int = 20) -> Dict:
        """í–¥ìƒëœ ë°°ì¹˜ ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        print("ğŸš€ í–¥ìƒëœ ë°°ì¹˜ ë¡œê·¸ ë¶„ì„ ì‹œì‘")
        print(f"ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬: {input_dir}")
        print(f"ğŸ¯ ì‘ì—… ë””ë ‰í† ë¦¬: {self.work_dir}")
        print(f"ğŸ“Š ìµœëŒ€ ê¹Šì´: {max_depth}, ìµœëŒ€ íŒŒì¼ ìˆ˜: {max_files}")
        
        # 1. ë¡œê·¸ íŒŒì¼ ì¬ê·€ íƒìƒ‰
        log_files = self.find_log_files_recursive(input_dir, max_depth)
        if not log_files:
            print("âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {'success': False, 'error': 'No log files found'}
        
        # íŒŒì¼ ìˆ˜ ì œí•œ
        if len(log_files) > max_files:
            print(f"âš ï¸  íŒŒì¼ ìˆ˜ê°€ ë§ì•„ ìƒìœ„ {max_files}ê°œë§Œ ë¶„ì„í•©ë‹ˆë‹¤")
            log_files = log_files[:max_files]
        
        # 2. Target ë° Baseline ì„ íƒ
        target_info, baseline_infos = self.select_target_and_baselines(log_files, target_file)
        if not target_info:
            print("âŒ Target íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return {'success': False, 'error': 'No target file'}
        
        # 3. ì „ì²˜ë¦¬ ì‹¤í–‰
        print(f"\n{'='*60}")
        print("ğŸ“‹ ì „ì²˜ë¦¬ ë‹¨ê³„")
        print(f"{'='*60}")
        
        # Target ì „ì²˜ë¦¬
        target_result = self.preprocess_log_file(target_info[0], target_info[1])
        
        # Baseline ì „ì²˜ë¦¬
        baseline_results = []
        for baseline_file, baseline_category in baseline_infos:
            result = self.preprocess_log_file(baseline_file, baseline_category)
            baseline_results.append(result)
        
        # 4. ì„±ê³µí•œ íŒŒì¼ë“¤ë§Œìœ¼ë¡œ ë¶„ì„ ì§„í–‰
        successful_baselines = [r for r in baseline_results if r['success']]
        
        print(f"\nğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:")
        print(f"  - Target: {'âœ…' if target_result['success'] else 'âŒ'} {target_info[0].name}")
        print(f"  - Baseline ì„±ê³µ: {len(successful_baselines)}/{len(baseline_results)}ê°œ")
        
        # 5. ë¶„ì„ ì‹¤í–‰ (ê¸°ì¡´ ì½”ë“œì™€ ë™ì¼)
        temporal_result = {'success': False, 'error': 'Target preprocessing failed'}
        comparative_result = {'success': False, 'error': 'Target preprocessing failed'}
        
        if target_result['success']:
            # ì‹œê°„ ê¸°ë°˜ ë¶„ì„
            print(f"\n{'='*60}")
            print("ğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€")
            print(f"{'='*60}")
            temporal_result = self.run_temporal_analysis(target_result)
            
            # íŒŒì¼ë³„ ë¹„êµ ë¶„ì„
            if successful_baselines:
                print(f"\n{'='*60}")
                print("ğŸ“Š íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€")
                print(f"{'='*60}")
                comparative_result = self.run_comparative_analysis(target_result, successful_baselines)
        
        # 6. ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
        print(f"\n{'='*60}")
        print("ğŸ“„ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±")
        print(f"{'='*60}")
        
        summary_report = self.generate_enhanced_summary_report(
            target_result, baseline_results, temporal_result, comparative_result,
            input_dir, max_depth
        )
        
        summary_file = self.work_dir / "ENHANCED_ANALYSIS_SUMMARY.md"
        with open(summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_report)
        
        print(f"ğŸ“‹ ìš”ì•½ ë¦¬í¬íŠ¸ ì €ì¥: {summary_file}")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print(f"\n{'='*60}")
        print("âœ… í–¥ìƒëœ ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ")
        print(f"{'='*60}")
        
        success_count = len([r for r in baseline_results if r['success']]) + (1 if target_result['success'] else 0)
        total_count = len(baseline_results) + 1
        print(f"ğŸ“Š ì „ì²˜ë¦¬ ì„±ê³µ: {success_count}/{total_count}ê°œ íŒŒì¼")
        
        if temporal_result['success']:
            temporal_anomalies = len(temporal_result.get('anomalies', []))
            print(f"ğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ: {temporal_anomalies}ê°œ")
        
        if comparative_result['success']:
            comp_anomalies = len(comparative_result.get('anomalies', []))
            print(f"ğŸ“Š ë¹„êµ ë¶„ì„ ì´ìƒ: {comp_anomalies}ê°œ")
        
        print(f"ğŸ“„ ìš”ì•½ ë¦¬í¬íŠ¸: {summary_file}")
        
        return {
            'success': True,
            'target_result': target_result,
            'baseline_results': baseline_results,
            'temporal_result': temporal_result,
            'comparative_result': comparative_result,
            'summary_file': summary_file,
            'total_files_found': len(log_files),
            'files_processed': total_count
        }
    
    def run_temporal_analysis(self, target_result: Dict) -> Dict:
        """ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ì‹¤í–‰ (ê¸°ì¡´ê³¼ ë™ì¼)."""
        if not target_result['success']:
            return {'success': False, 'error': 'Target preprocessing failed'}
        
        print(f"ğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€: {target_result['file_path'].name}")
        
        try:
            cmd = [
                sys.executable, "temporal_anomaly_detector.py",
                "--data-dir", str(target_result['output_dir']),
                "--output-dir", str(target_result['output_dir'] / "temporal_analysis")
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode != 0:
                print(f"âŒ ì‹œê°„ ë¶„ì„ ì‹¤íŒ¨: {result.stderr}")
                return {'success': False, 'error': result.stderr}
            
            temporal_dir = target_result['output_dir'] / "temporal_analysis"
            anomalies_file = temporal_dir / "temporal_anomalies.json"
            
            temporal_result = {'success': True, 'anomalies': []}
            if anomalies_file.exists():
                with open(anomalies_file) as f:
                    temporal_result['anomalies'] = json.load(f)
            
            print(f"âœ… ì‹œê°„ ë¶„ì„ ì™„ë£Œ: {len(temporal_result['anomalies'])}ê°œ ì´ìƒ ë°œê²¬")
            return temporal_result
            
        except Exception as e:
            print(f"âŒ ì‹œê°„ ë¶„ì„ ì˜ˆì™¸: {e}")
            return {'success': False, 'error': str(e)}
    
    def run_comparative_analysis(self, target_result: Dict, baseline_results: List[Dict]) -> Dict:
        """íŒŒì¼ë³„ ë¹„êµ ë¶„ì„ ì‹¤í–‰ (ê¸°ì¡´ê³¼ ë™ì¼)."""
        if not target_result['success']:
            return {'success': False, 'error': 'Target preprocessing failed'}
        
        valid_baselines = [r for r in baseline_results if r['success']]
        if not valid_baselines:
            print("âš ï¸ ë¹„êµí•  baseline íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return {'success': False, 'error': 'No valid baseline files'}
        
        print(f"ğŸ“Š íŒŒì¼ë³„ ë¹„êµ ë¶„ì„: {target_result['file_path'].name} vs {len(valid_baselines)}ê°œ íŒŒì¼")
        
        try:
            baseline_paths = [str(r['parsed_file']) for r in valid_baselines]
            
            cmd = [
                sys.executable, "comparative_anomaly_detector.py",
                "--target", str(target_result['parsed_file']),
                "--baselines"] + baseline_paths + [
                "--output-dir", str(target_result['output_dir'] / "comparative_analysis")
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode != 0:
                print(f"âŒ ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {result.stderr}")
                return {'success': False, 'error': result.stderr}
            
            comp_dir = target_result['output_dir'] / "comparative_analysis"
            anomalies_file = comp_dir / "comparative_anomalies.json"
            
            comp_result = {'success': True, 'anomalies': [], 'baseline_count': len(valid_baselines)}
            if anomalies_file.exists():
                with open(anomalies_file) as f:
                    comp_result['anomalies'] = json.load(f)
            
            print(f"âœ… ë¹„êµ ë¶„ì„ ì™„ë£Œ: {len(comp_result['anomalies'])}ê°œ ì´ìƒ ë°œê²¬")
            return comp_result
            
        except Exception as e:
            print(f"âŒ ë¹„êµ ë¶„ì„ ì˜ˆì™¸: {e}")
            return {'success': False, 'error': str(e)}
    
    def generate_enhanced_summary_report(self, target_result: Dict, baseline_results: List[Dict],
                                       temporal_result: Dict, comparative_result: Dict,
                                       input_dir: str, max_depth: int) -> str:
        """í–¥ìƒëœ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±."""
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        target_name = target_result['file_path'].name
        
        report = f"""# í–¥ìƒëœ ë°°ì¹˜ ë¡œê·¸ ë¶„ì„ ë¦¬í¬íŠ¸

**ë¶„ì„ ì‹œê°„**: {timestamp}  
**ì…ë ¥ ë””ë ‰í† ë¦¬**: {input_dir}  
**ìŠ¤ìº” ê¹Šì´**: {max_depth}  
**Target íŒŒì¼**: {target_name}  
**ë°œê²¬ëœ íŒŒì¼**: {len(baseline_results) + 1}ê°œ  

## ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„

### Target íŒŒì¼: {target_name}
"""
        
        if target_result['success']:
            stats = target_result['stats']
            validation = target_result.get('validation', {})
            report += f"""- âœ… **ì„±ê³µ**: {stats['total_logs']:,}ê°œ ë¡œê·¸, {stats['unique_templates']}ê°œ í…œí”Œë¦¿
- **ì¹´í…Œê³ ë¦¬**: {target_result.get('category', 'root')}
- **íŒŒì¼ í¬ê¸°**: {validation.get('file_size_mb', 0):.1f}MB
- **ë¡œê·¸ í˜•ì‹**: {validation.get('format', 'unknown')}
- **ì‹œê°„ ë²”ìœ„**: {stats['time_range']['start']} ~ {stats['time_range']['end']}
- **í˜¸ìŠ¤íŠ¸**: {len(stats['hosts'])}ê°œ ({', '.join(stats['hosts'][:3])}{'...' if len(stats['hosts']) > 3 else ''})
"""
        else:
            validation = target_result.get('validation', {})
            report += f"""- âŒ **ì‹¤íŒ¨**: {target_result['error']}
- **ì¹´í…Œê³ ë¦¬**: {target_result.get('category', 'root')}
- **íŒŒì¼ í¬ê¸°**: {validation.get('file_size_mb', 0):.1f}MB
- **ë¡œê·¸ í˜•ì‹**: {validation.get('format', 'unknown')}
"""
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼ ì •ë¦¬
        categories = {}
        for result in baseline_results:
            category = result.get('category', 'root')
            if category not in categories:
                categories[category] = []
            categories[category].append(result)
        
        report += "\n### Baseline íŒŒì¼ë“¤ (ì¹´í…Œê³ ë¦¬ë³„)\n"
        for category, results in categories.items():
            report += f"\n#### ğŸ“ {category}\n"
            for i, result in enumerate(results, 1):
                if result['success']:
                    stats = result['stats']
                    validation = result.get('validation', {})
                    report += f"{i}. âœ… **{result['file_path'].name}**: {stats['total_logs']:,}ê°œ ë¡œê·¸, {stats['unique_templates']}ê°œ í…œí”Œë¦¿ ({validation.get('file_size_mb', 0):.1f}MB)\n"
                else:
                    validation = result.get('validation', {})
                    report += f"{i}. âŒ **{result['file_path'].name}**: {result['error']} ({validation.get('file_size_mb', 0):.1f}MB)\n"
        
        # ë‚˜ë¨¸ì§€ëŠ” ê¸°ì¡´ê³¼ ë™ì¼...
        report += "\n## ğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ê²°ê³¼\n\n"
        if temporal_result['success']:
            anomalies = temporal_result['anomalies']
            if anomalies:
                high_count = len([a for a in anomalies if a.get('severity') == 'high'])
                medium_count = len([a for a in anomalies if a.get('severity') == 'medium'])
                report += f"ğŸš¨ **ë°œê²¬ëœ ì´ìƒ**: {len(anomalies)}ê°œ (ì‹¬ê°: {high_count}ê°œ, ì£¼ì˜: {medium_count}ê°œ)\n\n"
                
                for anomaly in anomalies[:5]:
                    report += f"- **{anomaly.get('type', 'unknown')}** ({anomaly.get('hour', '?')}ì‹œ): {anomaly.get('description', 'No description')}\n"
                
                if len(anomalies) > 5:
                    report += f"- ... ë° {len(anomalies) - 5}ê°œ ì¶”ê°€\n"
            else:
                report += "âœ… ì‹œê°„ ê¸°ë°˜ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        else:
            report += f"âŒ ì‹œê°„ ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨: {temporal_result.get('error', 'Unknown error')}\n"
        
        report += "\n## ğŸ“Š íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€ ê²°ê³¼\n\n"
        if comparative_result['success']:
            anomalies = comparative_result['anomalies']
            baseline_count = comparative_result['baseline_count']
            report += f"**ë¹„êµ ëŒ€ìƒ**: {baseline_count}ê°œ baseline íŒŒì¼\n\n"
            
            if anomalies:
                high_count = len([a for a in anomalies if a.get('severity') == 'high'])
                medium_count = len([a for a in anomalies if a.get('severity') == 'medium'])
                report += f"ğŸš¨ **ë°œê²¬ëœ ì´ìƒ**: {len(anomalies)}ê°œ (ì‹¬ê°: {high_count}ê°œ, ì£¼ì˜: {medium_count}ê°œ)\n\n"
                
                for anomaly in anomalies[:5]:
                    report += f"- **{anomaly.get('type', 'unknown')}**: {anomaly.get('description', 'No description')}\n"
                
                if len(anomalies) > 5:
                    report += f"- ... ë° {len(anomalies) - 5}ê°œ ì¶”ê°€\n"
            else:
                report += "âœ… íŒŒì¼ë³„ ë¹„êµì—ì„œ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        else:
            report += f"âŒ íŒŒì¼ë³„ ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {comparative_result.get('error', 'Unknown error')}\n"
        
        # ê¶Œê³ ì‚¬í•­ ë° ìƒì„¸ ê²°ê³¼
        total_anomalies = 0
        if temporal_result['success']:
            total_anomalies += len(temporal_result.get('anomalies', []))
        if comparative_result['success']:
            total_anomalies += len(comparative_result.get('anomalies', []))
        
        report += "\n## ğŸ’¡ ê¶Œê³ ì‚¬í•­\n\n"
        if total_anomalies == 0:
            report += "âœ… ëª¨ë“  ë¶„ì„ì—ì„œ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        elif total_anomalies < 5:
            report += "ğŸ” ì¼ë¶€ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„¸ ë¶„ì„ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n"
        else:
            report += "âš ï¸ ë‹¤ìˆ˜ì˜ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸´ê¸‰ ì ê²€ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
        
        if target_result['success']:
            report += f"""
## ğŸ“‚ ìƒì„¸ ê²°ê³¼ íŒŒì¼
- **Target ë¶„ì„ ê²°ê³¼**: `{target_result['output_dir']}/`
- **ì‹œê°„ ê¸°ë°˜ ë¶„ì„**: `{target_result['output_dir']}/temporal_analysis/temporal_report.md`
- **íŒŒì¼ë³„ ë¹„êµ ë¶„ì„**: `{target_result['output_dir']}/comparative_analysis/comparative_report.md`

## ğŸ”§ ì¶”ê°€ ë¶„ì„ ëª…ë ¹ì–´
```bash
# ìƒì„¸ ë¶„ì„
python analyze_results.py --data-dir {target_result['output_dir']}

# ì‹œê°í™”
python visualize_results.py --data-dir {target_result['output_dir']}
```
"""
        
        return report

def main():
    parser = argparse.ArgumentParser(description="í–¥ìƒëœ ë°°ì¹˜ ë¡œê·¸ ë¶„ì„ê¸°")
    parser.add_argument("input_dir", help="ë¡œê·¸ íŒŒì¼ë“¤ì´ ìˆëŠ” ë£¨íŠ¸ ë””ë ‰í† ë¦¬")
    parser.add_argument("--target", help="ë¶„ì„í•  target íŒŒì¼ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê°€ì¥ í° íŒŒì¼)")
    parser.add_argument("--max-depth", type=int, default=3, help="í•˜ìœ„ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ìµœëŒ€ ê¹Šì´ (ê¸°ë³¸: 3)")
    parser.add_argument("--max-files", type=int, default=20, help="ìµœëŒ€ ì²˜ë¦¬ íŒŒì¼ ìˆ˜ (ê¸°ë³¸: 20)")
    parser.add_argument("--work-dir", help="ì‘ì—… ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ìë™ ìƒì„±)")
    
    args = parser.parse_args()
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰
    analyzer = EnhancedBatchAnalyzer(args.work_dir)
    result = analyzer.run_enhanced_analysis(
        args.input_dir, 
        args.target,
        args.max_depth,
        args.max_files
    )
    
    if not result['success']:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {result['error']}")
        sys.exit(1)
    
    print(f"\nğŸ‰ í–¥ìƒëœ ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“Š ì²˜ë¦¬ëœ íŒŒì¼: {result['files_processed']}/{result['total_files_found']}ê°œ")

if __name__ == "__main__":
    main()
