#!/usr/bin/env python3
"""
í–¥ìƒëœ ë°°ì¹˜ ë¡œê·¸ ë¶„ì„ê¸°
- í•˜ìœ„ ë””ë ‰í† ë¦¬ ì¬ê·€ì  ìŠ¤ìº” ì§€ì›
- ë‚ ì§œë³„/ì¹´í…Œê³ ë¦¬ë³„ í´ë” êµ¬ì¡° ì§€ì›
- ë¡œê·¸ í˜•ì‹ ìë™ ê°ì§€ ë° ê²€ì¦
- ì „ì²˜ë¦¬ ì˜¤ë¥˜ ë””ë²„ê¹… ê°•í™”
"""
import os
import sys
import subprocess
import pandas as pd
import json
from pathlib import Path
from datetime import datetime
from typing import List, Dict, Optional, Tuple
import argparse
import shutil
from glob import glob
import re

class EnhancedBatchAnalyzer:
    def __init__(self, work_dir: str = None):
        self.work_dir = Path(work_dir or f"enhanced_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}")
        self.work_dir.mkdir(parents=True, exist_ok=True)
        self.processed_files = {}
        self.analysis_results = {}
        
        # ë¡œê·¸ íŒŒì¼ íŒ¨í„´ë“¤
        self.log_patterns = [
            "*.log", "*.txt", "*.out", 
            "*.log.*", "*.syslog", "*.messages",
            "*.access", "*.error", "*.debug", "*.log*"
        ]
        
    def find_log_files_recursive(self, input_dir: str, max_depth: int = 3) -> List[Tuple[Path, str]]:
        """í•˜ìœ„ ë””ë ‰í† ë¦¬ë¥¼ í¬í•¨í•˜ì—¬ ë¡œê·¸ íŒŒì¼ë“¤ì„ ì¬ê·€ì ìœ¼ë¡œ ì°¾ìŠµë‹ˆë‹¤."""
        input_path = Path(input_dir)
        log_files = []
        
        print(f"ğŸ“‚ ë¡œê·¸ íŒŒì¼ ê²€ìƒ‰ ì¤‘: {input_dir} (ìµœëŒ€ ê¹Šì´: {max_depth})")
        
        def scan_directory(dir_path: Path, current_depth: int = 0):
            if current_depth > max_depth:
                return
                
            try:
                # í˜„ì¬ ë””ë ‰í† ë¦¬ì—ì„œ ë¡œê·¸ íŒŒì¼ ì°¾ê¸°
                for pattern in self.log_patterns:
                    for file_path in dir_path.glob(pattern):
                        if file_path.is_file() and file_path.stat().st_size > 0:
                            # ìƒëŒ€ ê²½ë¡œ ìƒì„± (ë””ë ‰í† ë¦¬ êµ¬ì¡° ì •ë³´ ë³´ì¡´)
                            rel_path = file_path.relative_to(input_path)
                            category = str(rel_path.parent) if rel_path.parent != Path('.') else "root"
                            log_files.append((file_path, category))
                
                # í•˜ìœ„ ë””ë ‰í† ë¦¬ ì¬ê·€ ìŠ¤ìº”
                for item in dir_path.iterdir():
                    if item.is_dir() and not item.name.startswith('.'):
                        scan_directory(item, current_depth + 1)
                        
            except PermissionError:
                print(f"âš ï¸  ê¶Œí•œ ì—†ìŒ: {dir_path}")
            except Exception as e:
                print(f"âš ï¸  ìŠ¤ìº” ì˜¤ë¥˜: {dir_path} - {e}")
        
        scan_directory(input_path)
        
        # íŒŒì¼ í¬ê¸°ë³„ ì •ë ¬ (í° íŒŒì¼ë¶€í„°)
        log_files.sort(key=lambda x: x[0].stat().st_size, reverse=True)
        
        print(f"ğŸ“Š ë°œê²¬ëœ ë¡œê·¸ íŒŒì¼: {len(log_files)}ê°œ")
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹í™” ì¶œë ¥
        categories = {}
        for file_path, category in log_files:
            if category not in categories:
                categories[category] = []
            categories[category].append(file_path)
        
        for category, files in categories.items():
            print(f"  ğŸ“ {category}: {len(files)}ê°œ íŒŒì¼")
            for i, file_path in enumerate(files[:3], 1):  # ìµœëŒ€ 3ê°œë§Œ í‘œì‹œ
                file_size = file_path.stat().st_size / (1024*1024)  # MB
                print(f"    {i}. {file_path.name} ({file_size:.1f} MB)")
            if len(files) > 3:
                print(f"    ... ë° {len(files) - 3}ê°œ ì¶”ê°€")
        
        return log_files
    
    def validate_log_format(self, log_file: Path) -> Dict:
        """ë¡œê·¸ íŒŒì¼ì˜ í˜•ì‹ì„ ê²€ì¦í•˜ê³  ìƒ˜í”Œì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        try:
            # íŒŒì¼ í¬ê¸° ì²´í¬
            file_size = log_file.stat().st_size
            if file_size == 0:
                return {'valid': False, 'error': 'Empty file'}
            
            # ì²˜ìŒ 10ë¼ì¸ ì½ê¸°
            with open(log_file, 'r', encoding='utf-8', errors='ignore') as f:
                sample_lines = []
                for i, line in enumerate(f):
                    if i >= 10:
                        break
                    sample_lines.append(line.strip())
            
            if not sample_lines:
                return {'valid': False, 'error': 'No readable lines'}
            
            # ë¡œê·¸ í˜•ì‹ íŒ¨í„´ ë¶„ì„
            patterns = {
                'syslog': r'^\w{3}\s+\d{1,2}\s+\d{2}:\d{2}:\d{2}\s+\S+\s+\S+:',  # Sep 14 05:04:41 host1 kernel:
                'timestamp_iso': r'^\d{4}-\d{2}-\d{2}[T\s]\d{2}:\d{2}:\d{2}',      # 2025-09-17 10:15:32
                'timestamp_bracket': r'^\[\d{4}-\d{2}-\d{2}\s+\d{2}:\d{2}:\d{2}\]', # [2025-09-17 10:15:32]
                'apache_combined': r'^\S+\s+\S+\s+\S+\s+\[',                        # Apache combined log
                'json': r'^\s*\{.*\}\s*$',                                         # JSON format
                'generic': r'.',                                                    # Fallback
            }
            
            detected_format = 'unknown'
            for format_name, pattern in patterns.items():
                if any(re.match(pattern, line) for line in sample_lines):
                    detected_format = format_name
                    break
            
            # í†µê³„ ìˆ˜ì§‘
            total_lines = sum(1 for _ in open(log_file, 'r', encoding='utf-8', errors='ignore'))
            
            return {
                'valid': True,
                'format': detected_format,
                'file_size_mb': file_size / (1024*1024),
                'total_lines': total_lines,
                'sample_lines': sample_lines[:3],
                'encoding': 'utf-8'  # ê¸°ë³¸ê°’
            }
            
        except UnicodeDecodeError:
            # UTF-8 ì‹¤íŒ¨ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
            try:
                with open(log_file, 'r', encoding='latin-1') as f:
                    sample_lines = [f.readline().strip() for _ in range(3)]
                return {
                    'valid': True,
                    'format': 'binary_or_latin1',
                    'file_size_mb': file_size / (1024*1024),
                    'sample_lines': sample_lines,
                    'encoding': 'latin-1',
                    'warning': 'Non-UTF8 encoding detected'
                }
            except Exception:
                return {'valid': False, 'error': 'Encoding issues'}
        
        except Exception as e:
            return {'valid': False, 'error': str(e)}
    
    def preprocess_log_file(self, log_file: Path, category: str = "") -> Dict:
        """ë‹¨ì¼ ë¡œê·¸ íŒŒì¼ì„ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë””ë²„ê¹… ê°•í™”)."""
        file_name = log_file.stem
        if category and category != "root":
            safe_category = re.sub(r'[^\w\-_]', '_', category)
            output_dir = self.work_dir / f"processed_{safe_category}_{file_name}"
        else:
            output_dir = self.work_dir / f"processed_{file_name}"
        output_dir.mkdir(parents=True, exist_ok=True)
        
        print(f"\nğŸ”„ ì „ì²˜ë¦¬ ì¤‘: {log_file.name} (ì¹´í…Œê³ ë¦¬: {category or 'root'})")
        
        # ë¡œê·¸ í˜•ì‹ ê²€ì¦
        validation = self.validate_log_format(log_file)
        if not validation['valid']:
            print(f"âŒ ë¡œê·¸ í˜•ì‹ ì˜¤ë¥˜: {validation['error']}")
            return {
                'success': False,
                'error': f"Log format validation failed: {validation['error']}",
                'file_path': log_file,
                'category': category,
                'output_dir': output_dir
            }
        
        print(f"ğŸ“‹ íŒŒì¼ ì •ë³´: {validation['file_size_mb']:.1f}MB, {validation.get('total_lines', '?')}ë¼ì¸, í˜•ì‹: {validation['format']}")
        
        try:
            # study-preprocess ë°”ì´ë„ˆë¦¬ë¡œ ì „ì²˜ë¦¬ ì‹¤í–‰
            cmd = [
                "study-preprocess", "parse",
                "--input", str(log_file),
                "--out-dir", str(output_dir),
                "--drain-state", str(self.work_dir / f"drain_{file_name}.json")
            ]
            
            print(f"ğŸ”§ ì‹¤í–‰ ëª…ë ¹ì–´: {' '.join(cmd)}")
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode != 0:
                print(f"âŒ ì „ì²˜ë¦¬ ì‹¤íŒ¨ (ì½”ë“œ: {result.returncode})")
                print(f"ğŸ“„ í‘œì¤€ ì¶œë ¥: {result.stdout}")
                print(f"ğŸ“„ í‘œì¤€ ì—ëŸ¬: {result.stderr}")
                return {
                    'success': False,
                    'error': f"Process failed with code {result.returncode}: {result.stderr}",
                    'stdout': result.stdout,
                    'stderr': result.stderr,
                    'file_path': log_file,
                    'category': category,
                    'output_dir': output_dir,
                    'validation': validation
                }
            
            # ê²°ê³¼ íŒŒì¼ í™•ì¸
            parsed_file = output_dir / "parsed.parquet"
            if not parsed_file.exists():
                print(f"âŒ íŒŒì‹± ê²°ê³¼ ì—†ìŒ: parsed.parquetì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
                print(f"ğŸ“ ì¶œë ¥ ë””ë ‰í† ë¦¬ ë‚´ìš©:")
                try:
                    for item in output_dir.iterdir():
                        print(f"  - {item.name}")
                except:
                    print("  (ë””ë ‰í† ë¦¬ ì½ê¸° ì‹¤íŒ¨)")
                
                return {
                    'success': False,
                    'error': 'No parsed.parquet generated',
                    'stdout': result.stdout,
                    'file_path': log_file,
                    'category': category,
                    'output_dir': output_dir,
                    'validation': validation
                }
            
            # ê²°ê³¼ í†µê³„ ìˆ˜ì§‘
            df = pd.read_parquet(parsed_file)
            stats = {
                'total_logs': len(df),
                'unique_templates': len(df['template_id'].unique()) if 'template_id' in df.columns else 0,
                'time_range': {
                    'start': str(df['timestamp'].min()) if 'timestamp' in df.columns else None,
                    'end': str(df['timestamp'].max()) if 'timestamp' in df.columns else None,
                },
                'hosts': list(df['host'].unique()) if 'host' in df.columns else [],
                'processes': list(df['process'].unique()) if 'process' in df.columns else []
            }
            
            print(f"âœ… ì™„ë£Œ: {stats['total_logs']:,}ê°œ ë¡œê·¸, {stats['unique_templates']}ê°œ í…œí”Œë¦¿")
            
            return {
                'success': True,
                'file_path': log_file,
                'category': category,
                'output_dir': output_dir,
                'parsed_file': parsed_file,
                'stats': stats,
                'validation': validation
            }
            
        except Exception as e:
            print(f"âŒ ì˜ˆì™¸ ë°œìƒ: {log_file.name} - {e}")
            return {
                'success': False,
                'error': str(e),
                'file_path': log_file,
                'category': category,
                'output_dir': output_dir,
                'validation': validation
            }
    
    def select_target_and_baselines(self, log_files: List[Tuple[Path, str]], 
                                  target_file: str = None) -> Tuple[Tuple[Path, str], List[Tuple[Path, str]]]:
        """Target íŒŒì¼ê³¼ Baseline íŒŒì¼ë“¤ì„ ì„ íƒí•©ë‹ˆë‹¤."""
        
        if target_file:
            # ì ˆëŒ€ ê²½ë¡œ ë˜ëŠ” ìƒëŒ€ ê²½ë¡œë¡œ ì§€ì •ëœ Target íŒŒì¼ ì²˜ë¦¬
            target_path = Path(target_file)
            
            # 1. ì ˆëŒ€/ìƒëŒ€ ê²½ë¡œë¡œ ì§€ì •ëœ íŒŒì¼ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
            if target_path.exists() and target_path.is_file():
                print(f"ğŸ¯ ì™¸ë¶€ Target íŒŒì¼ ë°œê²¬: {target_file}")
                # ì™¸ë¶€ íŒŒì¼ì„ Targetìœ¼ë¡œ ì‚¬ìš©, ì¹´í…Œê³ ë¦¬ëŠ” ë¶€ëª¨ ë””ë ‰í† ë¦¬ëª…
                category = target_path.parent.name
                target = (target_path, category)
                baselines = log_files  # ëª¨ë“  ë°œê²¬ëœ ë¡œê·¸ íŒŒì¼ë“¤ì„ Baselineìœ¼ë¡œ ì‚¬ìš©
            else:
                # 2. ê¸°ì¡´ ë°©ì‹: ë°œê²¬ëœ ë¡œê·¸ íŒŒì¼ë“¤ ì¤‘ì—ì„œ Target ì°¾ê¸°
                target_matches = [
                    (f, c) for f, c in log_files 
                    if f.name == target_file or str(f) == target_file
                ]
                
                if target_matches:
                    target = target_matches[0]
                    baselines = [(f, c) for f, c in log_files if (f, c) != target]
                else:
                    print(f"âŒ ì§€ì •ëœ Target íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {target_file}")
                    print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ íŒŒì¼ë“¤:")
                    for i, (f, c) in enumerate(log_files[:10], 1):
                        print(f"  {i}. {f.name} (ì¹´í…Œê³ ë¦¬: {c})")
                    print(f"\nğŸ’¡ ì˜¬ë°”ë¥¸ Target íŒŒì¼ëª…ì„ ì§€ì •í•˜ê±°ë‚˜, Target íŒŒì¼ì„ ìƒëµí•˜ì—¬ ìë™ ì„ íƒí•˜ì„¸ìš”.")
                    raise ValueError(f"Target íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {target_file}")
        else:
            # ê°€ì¥ í° íŒŒì¼ì„ Targetìœ¼ë¡œ ì„ íƒ
            target = log_files[0] if log_files else None
            baselines = log_files[1:] if len(log_files) > 1 else []
        
        if target:
            print(f"ğŸ¯ Target íŒŒì¼: {target[0].name} (ì¹´í…Œê³ ë¦¬: {target[1]})")
            print(f"ğŸ“Š Baseline íŒŒì¼: {len(baselines)}ê°œ")
            
            # ì¹´í…Œê³ ë¦¬ë³„ Baseline ë¶„í¬
            baseline_categories = {}
            for _, category in baselines:
                baseline_categories[category] = baseline_categories.get(category, 0) + 1
            
            for category, count in baseline_categories.items():
                print(f"  - {category}: {count}ê°œ")
        
        return target, baselines
    
    def _validate_baseline_quality(self, baseline_results: List[Dict]) -> List[Dict]:
        """Baseline íŒŒì¼ë“¤ì˜ í’ˆì§ˆì„ ê²€ì¦í•˜ê³  ë¬¸ì œê°€ ìˆëŠ” ê²ƒë“¤ì„ í•„í„°ë§í•©ë‹ˆë‹¤."""
        
        quality_baselines = []
        
        for result in baseline_results:
            try:
                # ê¸°ë³¸ í†µê³„ ë¡œë“œ
                df = pd.read_parquet(result['parsed_file'])
                
                # í’ˆì§ˆ ê²€ì¦ ê¸°ì¤€ë“¤
                total_logs = len(df)
                unique_templates = len(df['template_id'].unique()) if 'template_id' in df.columns else 0
                
                # ì—ëŸ¬/ê²½ê³  ë¡œê·¸ ë¶„ì„
                error_keywords = ['error', 'ERROR', 'fail', 'FAIL', 'exception', 'Exception', 'panic', 'fatal']
                warning_keywords = ['warn', 'WARN', 'warning', 'WARNING']
                
                error_logs = df[df['raw'].str.contains('|'.join(error_keywords), case=False, na=False)]
                warning_logs = df[df['raw'].str.contains('|'.join(warning_keywords), case=False, na=False)]
                
                error_rate = len(error_logs) / max(total_logs, 1)
                warning_rate = len(warning_logs) / max(total_logs, 1)
                
                # í…œí”Œë¦¿ ë¶„í¬ ë¶„ì„
                template_counts = df['template_id'].value_counts() if 'template_id' in df.columns else pd.Series()
                rare_templates = len([t for t, count in template_counts.items() if count == 1])
                rare_template_ratio = rare_templates / max(unique_templates, 1)
                
                # í’ˆì§ˆ ê¸°ì¤€ ì²´í¬
                quality_issues = []
                
                if error_rate > 0.02:  # 2% ì´ìƒ ì—ëŸ¬ìœ¨
                    quality_issues.append(f"ë†’ì€ ì—ëŸ¬ìœ¨: {error_rate:.2%}")
                
                if warning_rate > 0.05:  # 5% ì´ìƒ ê²½ê³ ìœ¨
                    quality_issues.append(f"ë†’ì€ ê²½ê³ ìœ¨: {warning_rate:.2%}")
                
                if unique_templates < 10:  # ìµœì†Œ 10ê°œ í…œí”Œë¦¿
                    quality_issues.append(f"í…œí”Œë¦¿ ë¶€ì¡±: {unique_templates}ê°œ")
                
                if total_logs < 100:  # ìµœì†Œ 100ê°œ ë¡œê·¸
                    quality_issues.append(f"ë¡œê·¸ ìˆ˜ ë¶€ì¡±: {total_logs}ê°œ")
                
                if rare_template_ratio > 0.3:  # í¬ê·€ í…œí”Œë¦¿ 30% ì´ìƒ
                    quality_issues.append(f"í¬ê·€ í…œí”Œë¦¿ ê³¼ë‹¤: {rare_template_ratio:.1%}")
                
                # í’ˆì§ˆ ê¸°ì¤€ í†µê³¼ ì—¬ë¶€
                if len(quality_issues) <= 1:  # ìµœëŒ€ 1ê°œ ë¬¸ì œê¹Œì§€ í—ˆìš©
                    quality_baselines.append(result)
                    if quality_issues:
                        print(f"   âš ï¸  {result['file_path'].name}: {quality_issues[0]} (ê²½ë¯¸í•¨)")
                    else:
                        print(f"   âœ… {result['file_path'].name}: í’ˆì§ˆ ì–‘í˜¸")
                else:
                    print(f"   âŒ {result['file_path'].name}: {', '.join(quality_issues)}")
                
            except Exception as e:
                print(f"   âŒ {result['file_path'].name}: ê²€ì¦ ì˜¤ë¥˜ ({e})")
        
        return quality_baselines
    
    def run_log_sample_analysis(self, target_result: Dict) -> Dict:
        """Target íŒŒì¼ì˜ ì´ìƒ ë¡œê·¸ ìƒ˜í”Œì„ ë¶„ì„í•©ë‹ˆë‹¤."""
        if not target_result['success']:
            return {'success': False, 'error': 'Target preprocessing failed'}
        
        print(f"ğŸ” ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„: {target_result['file_path'].name}")
        
        try:
            # ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ì‹¤í–‰
            cmd = [
                sys.executable, "log_sample_analyzer.py",
                str(target_result['output_dir']),
                "--output-dir", str(target_result['output_dir'] / "log_samples_analysis"),
                "--max-samples", "20",
                "--context-lines", "3"
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode != 0:
                print(f"âŒ ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ì‹¤íŒ¨: {result.stderr}")
                return {'success': False, 'error': result.stderr}
            
            # ê²°ê³¼ ë¡œë“œ
            sample_data_file = target_result['output_dir'] / "log_samples_analysis" / "anomaly_samples.json"
            sample_report_file = target_result['output_dir'] / "log_samples_analysis" / "anomaly_analysis_report.md"
            
            sample_result = {'success': True, 'total_anomalies': 0, 'analysis_summary': {}}
            
            if sample_data_file.exists():
                with open(sample_data_file, 'r') as f:
                    sample_data = json.load(f)
                
                # ìš”ì•½ í†µê³„ ê³„ì‚°
                for method, results in sample_data.items():
                    anomaly_count = results.get('anomaly_count', 0)
                    sample_result['total_anomalies'] += anomaly_count
                    sample_result['analysis_summary'][method] = {
                        'anomaly_count': anomaly_count,
                        'analyzed_count': results.get('analyzed_count', 0),
                        'method_description': results.get('method', 'Unknown')
                    }
                
                sample_result['data_file'] = sample_data_file
                sample_result['report_file'] = sample_report_file
            
            print(f"âœ… ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ì™„ë£Œ: ì´ {sample_result['total_anomalies']}ê°œ ì´ìƒ ë°œê²¬")
            return sample_result
            
        except Exception as e:
            print(f"âŒ ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ì˜ˆì™¸: {e}")
            return {'success': False, 'error': str(e)}

    def run_cli_report(self, target_result: Dict) -> Dict:
        """CLI ë¦¬í¬íŠ¸ ìƒì„± ì‹¤í–‰ (ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ í¬í•¨)."""
        if not target_result['success']:
            return {'success': False, 'error': 'Target preprocessing failed'}
        
        print(f"ğŸ“„ CLI ë¦¬í¬íŠ¸ ìƒì„±: {target_result['file_path'].name}")
        
        try:
            # CLI report ì‹¤í–‰ (ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ í¬í•¨)
            print("  ğŸ“Š CLI ë¦¬í¬íŠ¸ ë° ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ìƒì„± ì¤‘...")
            
            cmd = [
                sys.executable, "-c", 
                f"""
import sys
sys.path.append('.')
from study_preprocessor.cli import main
import click
from pathlib import Path

# CLI report ì‹¤í–‰
ctx = click.Context(main)
ctx.invoke(main.commands['report'], 
          processed_dir=Path('{target_result['output_dir']}'), 
          with_samples=True)  # ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ì„ ê¸°ë³¸ìœ¼ë¡œ í¬í•¨
print("CLI report generation completed")
"""
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode != 0:
                print(f"âŒ CLI ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {result.stderr}")
                return {'success': False, 'error': result.stderr}
            
            # ê²°ê³¼ íŒŒì¼ í™•ì¸
            report_file = target_result['output_dir'] / "report.md"
            
            if not report_file.exists():
                return {'success': False, 'error': 'CLI report file not generated'}
            
            print(f"  âœ… CLI ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: {report_file.name}")
            
            return {
                'success': True,
                'report_file': report_file
            }
            
        except Exception as e:
            print(f"âŒ CLI ë¦¬í¬íŠ¸ ìƒì„± ì˜ˆì™¸: {e}")
            return {'success': False, 'error': str(e)}
    
    def run_enhanced_analysis(self, input_dir: str, target_file: str = None, 
                            max_depth: int = 3, max_files: int = 20) -> Dict:
        """í–¥ìƒëœ ë°°ì¹˜ ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        
        print("ğŸš€ í–¥ìƒëœ ë°°ì¹˜ ë¡œê·¸ ë¶„ì„ ì‹œì‘")
        print(f"ğŸ“‚ ì…ë ¥ ë””ë ‰í† ë¦¬: {input_dir}")
        print(f"ğŸ¯ ì‘ì—… ë””ë ‰í† ë¦¬: {self.work_dir}")
        print(f"ğŸ“Š ìµœëŒ€ ê¹Šì´: {max_depth}, ìµœëŒ€ íŒŒì¼ ìˆ˜: {max_files}")
        
        # 1. ë¡œê·¸ íŒŒì¼ ì¬ê·€ íƒìƒ‰
        log_files = self.find_log_files_recursive(input_dir, max_depth)
        if not log_files:
            print("âŒ ë¡œê·¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return {'success': False, 'error': 'No log files found'}
        
        # 2. Target ë° Baseline ì„ íƒ (íŒŒì¼ ìˆ˜ ì œí•œ ì „ì— ìˆ˜í–‰)
        target_info, baseline_infos = self.select_target_and_baselines(log_files, target_file)
        if not target_info:
            print("âŒ Target íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return {'success': False, 'error': 'No target file'}
        
        # 3. íŒŒì¼ ìˆ˜ ì œí•œ (Targetì€ í•­ìƒ í¬í•¨, Baselineë§Œ ì œí•œ)
        if len(baseline_infos) > max_files - 1:  # Target 1ê°œ ì œì™¸í•˜ê³  ì œí•œ
            print(f"âš ï¸  Baseline íŒŒì¼ì´ ë§ì•„ ìƒìœ„ {max_files - 1}ê°œë§Œ ë¶„ì„í•©ë‹ˆë‹¤")
            baseline_infos = baseline_infos[:max_files - 1]
        
        # 4. ì „ì²˜ë¦¬ ì‹¤í–‰
        print(f"\n{'='*60}")
        print("ğŸ“‹ ì „ì²˜ë¦¬ ë‹¨ê³„")
        print(f"{'='*60}")
        
        # Target ì „ì²˜ë¦¬
        target_result = self.preprocess_log_file(target_info[0], target_info[1])
        
        # Baseline ì „ì²˜ë¦¬
        baseline_results = []
        for baseline_file, baseline_category in baseline_infos:
            result = self.preprocess_log_file(baseline_file, baseline_category)
            baseline_results.append(result)
        
        # 5. ì„±ê³µí•œ íŒŒì¼ë“¤ë§Œìœ¼ë¡œ ë¶„ì„ ì§„í–‰
        successful_baselines = [r for r in baseline_results if r['success']]
        
        print(f"\nğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½:")
        print(f"  - Target: {'âœ…' if target_result['success'] else 'âŒ'} {target_info[0].name}")
        print(f"  - Baseline ì„±ê³µ: {len(successful_baselines)}/{len(baseline_results)}ê°œ")
        
        # 6. Target íŒŒì¼ Full Pipeline ë¶„ì„ ì‹¤í–‰
        target_baseline_result = {'success': False, 'error': 'Target preprocessing failed'}
        target_deeplog_result = {'success': False, 'error': 'Target preprocessing failed'}
        target_mscred_result = {'success': False, 'error': 'Target preprocessing failed'}
        target_temporal_result = {'success': False, 'error': 'Target preprocessing failed'}
        comparative_result = {'success': False, 'error': 'Target preprocessing failed'}
        
        if target_result['success']:
            print(f"\n{'='*60}")
            print(f"ğŸ¯ Target íŒŒì¼ ë¶„ì„: {target_result['file_path'].name}")
            print(f"{'='*60}")
            
            # Target Baseline ì´ìƒ íƒì§€
            print("ğŸ“ˆ Baseline ì´ìƒ íƒì§€ ì¤‘...")
            target_baseline_result = self.run_baseline_analysis(target_result)
            
            # Target DeepLog ë¶„ì„ 
            print("ğŸ§  DeepLog ë”¥ëŸ¬ë‹ ë¶„ì„ ì¤‘...")
            target_deeplog_result = self.run_deeplog_analysis(target_result)
            
            # Target MS-CRED ì…ë ¥ ìƒì„±
            print("ğŸ“Š MS-CRED ì…ë ¥ ìƒì„± ì¤‘...")
            target_mscred_build_result = self.run_mscred_build(target_result)
            
            # Target MS-CRED í•™ìŠµ ë° ì¶”ë¡ 
            target_mscred_result = {'success': False, 'error': 'MS-CRED build failed'}
            if target_mscred_build_result['success']:
                print("ğŸ”¬ MS-CRED í•™ìŠµ ë° ì´ìƒíƒì§€ ì¤‘...")
                target_mscred_result = self.run_mscred_analysis(target_result)
            
            # Target ì‹œê°„ ê¸°ë°˜ ë¶„ì„
            print("ğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ì¤‘...")
            target_temporal_result = self.run_temporal_analysis(target_result)
        
        # 7. Baseline íŒŒì¼ë“¤ ê°œë³„ ë¶„ì„ ì‹¤í–‰
        baseline_analysis_results = []
        for i, baseline_result in enumerate(successful_baselines):
            print(f"\n{'='*60}")
            print(f"ğŸ“‚ Baseline íŒŒì¼ ë¶„ì„ {i+1}/{len(successful_baselines)}: {baseline_result['file_path'].name}")
            print(f"{'='*60}")
            
            baseline_analysis = {
                'file_info': baseline_result,
                'baseline_result': {'success': False},
                'deeplog_result': {'success': False},
                'mscred_result': {'success': False},
                'temporal_result': {'success': False}
            }
            
            # Baseline ì´ìƒ íƒì§€
            print("ğŸ“ˆ Baseline ì´ìƒ íƒì§€ ì¤‘...")
            baseline_analysis['baseline_result'] = self.run_baseline_analysis(baseline_result)
            
            # DeepLog ë¶„ì„
            print("ğŸ§  DeepLog ë”¥ëŸ¬ë‹ ë¶„ì„ ì¤‘...")
            baseline_analysis['deeplog_result'] = self.run_deeplog_analysis(baseline_result)
            
            # MS-CRED ì…ë ¥ ìƒì„± ë° ë¶„ì„
            print("ğŸ“Š MS-CRED ì…ë ¥ ìƒì„± ì¤‘...")
            mscred_build_result = self.run_mscred_build(baseline_result)
            
            if mscred_build_result['success']:
                print("ğŸ”¬ MS-CRED í•™ìŠµ ë° ì´ìƒíƒì§€ ì¤‘...")
                baseline_analysis['mscred_result'] = self.run_mscred_analysis(baseline_result)
            else:
                baseline_analysis['mscred_result'] = {'success': False, 'error': 'MS-CRED build failed'}
            
            # ì‹œê°„ ê¸°ë°˜ ë¶„ì„
            print("ğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ì¤‘...")
            baseline_analysis['temporal_result'] = self.run_temporal_analysis(baseline_result)
            
            baseline_analysis_results.append(baseline_analysis)
        
        # 8. íŒŒì¼ë³„ ë¹„êµ ë¶„ì„ (ëª¨ë“  íŒŒì¼ ëŒ€ìƒ)
        if successful_baselines and target_result['success']:
            print(f"\n{'='*60}")
            print("ğŸ“Š íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€")
            print(f"{'='*60}")
            comparative_result = self.run_comparative_analysis(target_result, successful_baselines)
        
        # 9. CLI ë¦¬í¬íŠ¸ ìƒì„± (ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ í¬í•¨)
        print(f"\n{'='*60}")
        print("ğŸ“„ CLI ë¦¬í¬íŠ¸ ë° ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„")
        print(f"{'='*60}")
        
        # Target CLI ë¦¬í¬íŠ¸
        target_cli_report_result = {'success': False, 'error': 'Target preprocessing failed'}
        if target_result['success']:
            print(f"ğŸ“„ Target CLI ë¦¬í¬íŠ¸: {target_result['file_path'].name}")
            target_cli_report_result = self.run_cli_report(target_result)
        
        # Baseline CLI ë¦¬í¬íŠ¸ë“¤
        baseline_cli_reports = []
        for i, baseline_analysis in enumerate(baseline_analysis_results):
            baseline_file_info = baseline_analysis['file_info']
            print(f"ğŸ“„ Baseline CLI ë¦¬í¬íŠ¸ {i+1}/{len(baseline_analysis_results)}: {baseline_file_info['file_path'].name}")
            cli_report = self.run_cli_report(baseline_file_info)
            baseline_cli_reports.append(cli_report)
        
        # 10. ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±
        print(f"\n{'='*60}")
        print("ğŸ“„ ì¢…í•© ë¦¬í¬íŠ¸ ìƒì„±")
        print(f"{'='*60}")
        
        summary_report = self.generate_comprehensive_report(
            target_result, baseline_results, target_baseline_result, target_deeplog_result, target_mscred_result, 
            target_temporal_result, comparative_result, target_cli_report_result, input_dir, max_depth,
            baseline_analysis_results, baseline_cli_reports
        )
        
        comprehensive_report_file = self.work_dir / "COMPREHENSIVE_ANALYSIS_REPORT.md"
        with open(comprehensive_report_file, 'w', encoding='utf-8') as f:
            f.write(summary_report)
        
        # ê¸°ì¡´ ìš”ì•½ íŒŒì¼ë„ í˜¸í™˜ì„±ì„ ìœ„í•´ ìƒì„±
        legacy_summary_file = self.work_dir / "ENHANCED_ANALYSIS_SUMMARY.md"
        with open(legacy_summary_file, 'w', encoding='utf-8') as f:
            f.write(summary_report)
        
        print(f"ğŸ“‹ ì¢…í•© ë¦¬í¬íŠ¸ ì €ì¥: {comprehensive_report_file}")
        print(f"ğŸ“‹ í˜¸í™˜ì„± ë¦¬í¬íŠ¸: {legacy_summary_file}")
        
        # ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print(f"\n{'='*60}")
        print("âœ… í–¥ìƒëœ ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ")
        print(f"{'='*60}")
        
        success_count = len([r for r in baseline_results if r['success']]) + (1 if target_result['success'] else 0)
        total_count = len(baseline_results) + 1
        print(f"ğŸ“Š ì „ì²˜ë¦¬ ì„±ê³µ: {success_count}/{total_count}ê°œ íŒŒì¼")
        
        # Target ë¶„ì„ ê²°ê³¼ ìš”ì•½
        if target_baseline_result['success']:
            baseline_anomalies = target_baseline_result['anomaly_windows']
            baseline_rate = target_baseline_result['anomaly_rate']
            print(f"ğŸ¯ Target Baseline ì´ìƒ: {baseline_anomalies}ê°œ ìœˆë„ìš° ({baseline_rate:.1%})")
        
        if target_deeplog_result['success']:
            deeplog_violations = target_deeplog_result.get('violations', 0)
            deeplog_total = target_deeplog_result.get('total_sequences', 0)
            print(f"ğŸ¯ Target DeepLog ìœ„ë°˜: {deeplog_violations}/{deeplog_total}ê°œ ì‹œí€€ìŠ¤")
        
        if target_mscred_result['success']:
            mscred_anomalies = target_mscred_result.get('anomalies', 0)
            mscred_total = target_mscred_result.get('total_windows', 0)
            print(f"ğŸ¯ Target MS-CRED ì´ìƒ: {mscred_anomalies}/{mscred_total}ê°œ ìœˆë„ìš°")
        
        if target_temporal_result['success']:
            temporal_anomalies = len(target_temporal_result.get('anomalies', []))
            print(f"ğŸ¯ Target ì‹œê°„ ê¸°ë°˜ ì´ìƒ: {temporal_anomalies}ê°œ")
        
        # Baseline ë¶„ì„ ê²°ê³¼ ìš”ì•½
        baseline_total_anomalies = 0
        baseline_successful_analyses = 0
        for baseline_analysis in baseline_analysis_results:
            if baseline_analysis['baseline_result']['success']:
                baseline_total_anomalies += baseline_analysis['baseline_result'].get('anomaly_windows', 0)
                baseline_successful_analyses += 1
        
        if baseline_successful_analyses > 0:
            print(f"ğŸ“‚ Baseline íŒŒì¼ë“¤ ì´ìƒ: ì´ {baseline_total_anomalies}ê°œ ìœˆë„ìš° ({baseline_successful_analyses}ê°œ íŒŒì¼)")
        
        if comparative_result['success']:
            comp_anomalies = len(comparative_result.get('anomalies', []))
            print(f"ğŸ“Š ë¹„êµ ë¶„ì„ ì´ìƒ: {comp_anomalies}ê°œ")
        
        if target_cli_report_result['success']:
            print(f"ğŸ“„ Target CLI ë¦¬í¬íŠ¸: {target_cli_report_result['report_file']}")
        
        print(f"ğŸ“„ ì¢…í•© ë¦¬í¬íŠ¸: {comprehensive_report_file}")
        print(f"ğŸ“Š ë¶„ì„ëœ íŒŒì¼: Target 1ê°œ + Baseline {len(baseline_analysis_results)}ê°œ = ì´ {1 + len(baseline_analysis_results)}ê°œ")
        
        return {
            'success': True,
            'target_result': target_result,
            'baseline_results': baseline_results,
            'target_baseline_result': target_baseline_result,
            'target_deeplog_result': target_deeplog_result,
            'target_mscred_result': target_mscred_result,
            'target_temporal_result': target_temporal_result,
            'baseline_analysis_results': baseline_analysis_results,
            'comparative_result': comparative_result,
            'target_cli_report_result': target_cli_report_result,
            'baseline_cli_reports': baseline_cli_reports,
            'comprehensive_report_file': comprehensive_report_file,
            'summary_file': legacy_summary_file,  # í˜¸í™˜ì„±
            'total_files_found': len(log_files),
            'files_processed': total_count
        }
    
    def run_baseline_analysis(self, target_result: Dict) -> Dict:
        """Baseline ì´ìƒ íƒì§€ ì‹¤í–‰."""
        if not target_result['success']:
            return {'success': False, 'error': 'Target preprocessing failed'}
        
        print(f"ğŸ“ˆ Baseline ì´ìƒ íƒì§€: {target_result['file_path'].name}")
        
        try:
            parsed_file = target_result['output_dir'] / "parsed.parquet"
            
            # Baseline ì´ìƒ íƒì§€ ì‹¤í–‰
            print("  ğŸ“Š Window ê¸°ë°˜ ì´ìƒ íƒì§€ ì¤‘...")
            
            cmd = [
                sys.executable, "-c", 
                f"""
import sys
sys.path.append('.')
from study_preprocessor.detect import baseline_detect
from study_preprocessor.detect import BaselineParams

# Baseline ì´ìƒ íƒì§€ ì‹¤í–‰
baseline_detect(
    parsed_parquet='{parsed_file}',
    out_dir='{target_result['output_dir']}',
    params=BaselineParams(
        window_size=50,
        stride=25,
        ewm_alpha=0.3,
        anomaly_quantile=0.95
    )
)
print("Baseline detection completed")
"""
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode != 0:
                print(f"âŒ Baseline ì´ìƒ íƒì§€ ì‹¤íŒ¨: {result.stderr}")
                return {'success': False, 'error': result.stderr}
            
            # ê²°ê³¼ íŒŒì¼ í™•ì¸
            baseline_scores_file = target_result['output_dir'] / "baseline_scores.parquet"
            baseline_preview_file = target_result['output_dir'] / "baseline_preview.json"
            
            if not baseline_scores_file.exists():
                return {'success': False, 'error': 'Baseline scores file not generated'}
            
            # ê²°ê³¼ ìš”ì•½
            import pandas as pd
            scores_df = pd.read_parquet(baseline_scores_file)
            total_windows = len(scores_df)
            anomaly_windows = len(scores_df[scores_df['is_anomaly'] == True])
            anomaly_rate = anomaly_windows / total_windows if total_windows > 0 else 0
            
            print(f"  âœ… Baseline ì´ìƒ íƒì§€ ì™„ë£Œ: {anomaly_windows}/{total_windows} ìœˆë„ìš° ì´ìƒ ({anomaly_rate:.1%})")
            
            return {
                'success': True,
                'scores_file': baseline_scores_file,
                'preview_file': baseline_preview_file,
                'total_windows': total_windows,
                'anomaly_windows': anomaly_windows,
                'anomaly_rate': anomaly_rate
            }
            
        except Exception as e:
            print(f"âŒ Baseline ì´ìƒ íƒì§€ ì˜ˆì™¸: {e}")
            return {'success': False, 'error': str(e)}

    def run_deeplog_analysis(self, target_result: Dict) -> Dict:
        """DeepLog í•™ìŠµ ë° ì¶”ë¡  ì‹¤í–‰."""
        if not target_result['success']:
            return {'success': False, 'error': 'Target preprocessing failed'}
        
        print(f"ğŸ§  DeepLog ë¶„ì„: {target_result['file_path'].name}")
        
        try:
            parsed_file = target_result['output_dir'] / "parsed.parquet"
            
            # 1. DeepLog ì…ë ¥ ìƒì„± (vocab.json, sequences.parquet)
            print("  ğŸ“Š DeepLog ì…ë ¥ ìƒì„± ì¤‘...")
            
            try:
                from study_preprocessor.builders.deeplog import build_deeplog_inputs
                build_deeplog_inputs(str(parsed_file), str(target_result['output_dir']))
                
                # í•„ìˆ˜ íŒŒì¼ í™•ì¸
                vocab_file = target_result['output_dir'] / "vocab.json"
                sequences_file = target_result['output_dir'] / "sequences.parquet"
                
                if not vocab_file.exists() or not sequences_file.exists():
                    return {'success': False, 'error': 'DeepLog input files not generated'}
                
                print(f"  âœ… ì…ë ¥ íŒŒì¼ ìƒì„±: vocab.json, sequences.parquet")
                
            except Exception as e:
                print(f"âŒ DeepLog ì…ë ¥ ìƒì„± ì‹¤íŒ¨: {e}")
                return {'success': False, 'error': f'DeepLog input build failed: {e}'}
            
            # 2. DeepLog í•™ìŠµ
            print("  ğŸ¯ DeepLog ëª¨ë¸ í•™ìŠµ ì¤‘...")
            model_path = target_result['output_dir'] / "deeplog.pth"
            
            try:
                from study_preprocessor.builders.deeplog import train_deeplog
                train_deeplog(
                    str(sequences_file),
                    str(vocab_file), 
                    str(model_path),
                    seq_len=50,
                    epochs=2  # ë¹ ë¥¸ ì‹¤í–‰ì„ ìœ„í•´ 2ë¡œ ê°ì†Œ
                )
                
                if not model_path.exists():
                    return {'success': False, 'error': 'DeepLog model file not generated'}
                
                print(f"  âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {model_path.name}")
                
            except Exception as e:
                print(f"âŒ DeepLog í•™ìŠµ ì‹¤íŒ¨: {e}")
                return {'success': False, 'error': f'DeepLog training failed: {e}'}
            
            # 3. DeepLog ì¶”ë¡  (ë©”ëª¨ë¦¬ ìµœì í™”ë¥¼ ìœ„í•´ ì¡°ê±´ë¶€ ì‹¤í–‰)
            print("  ğŸ” DeepLog ì¶”ë¡  ì¤‘...")
            
            try:
                # ì‹œí€€ìŠ¤ í¬ê¸° í™•ì¸
                import pandas as pd
                df_check = pd.read_parquet(sequences_file)
                total_sequences = len(df_check)
                
                # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ í° íŒŒì¼ì€ ìƒ˜í”Œë§
                if total_sequences > 50000:
                    print(f"  âš ï¸ ëŒ€ìš©ëŸ‰ ì‹œí€€ìŠ¤ ({total_sequences:,}ê°œ) - ìƒ˜í”Œë§í•˜ì—¬ ì¶”ë¡ ")
                    sample_df = df_check.sample(n=min(50000, total_sequences), random_state=42)
                    sample_file = target_result['output_dir'] / "sequences_sample.parquet"
                    sample_df.to_parquet(sample_file, index=False)
                    inference_input = str(sample_file)
                else:
                    inference_input = str(sequences_file)
                
                from study_preprocessor.builders.deeplog import infer_deeplog_topk
                infer_df = infer_deeplog_topk(inference_input, str(model_path), k=3)
                
                # ê²°ê³¼ ì €ì¥
                infer_file = target_result['output_dir'] / "deeplog_infer.parquet"
                infer_df.to_parquet(infer_file, index=False)
                
                if not infer_file.exists():
                    return {'success': False, 'error': 'DeepLog inference file not generated'}
                
                print(f"  âœ… ì¶”ë¡  ì™„ë£Œ: {len(infer_df):,}ê°œ ì‹œí€€ìŠ¤ ë¶„ì„")
                
            except Exception as e:
                print(f"âŒ DeepLog ì¶”ë¡  ì‹¤íŒ¨: {e}")
                return {'success': False, 'error': f'DeepLog inference failed: {e}'}
            
            # ê²°ê³¼ ìš”ì•½
            import pandas as pd
            deeplog_df = pd.read_parquet(infer_file)
            violations = deeplog_df[deeplog_df['in_topk'] == False]
            violation_rate = len(violations) / len(deeplog_df) if len(deeplog_df) > 0 else 0
            
            print(f"âœ… DeepLog ë¶„ì„ ì™„ë£Œ: ìœ„ë°˜ìœ¨ {violation_rate:.1%} ({len(violations)}/{len(deeplog_df)})")
            
            return {
                'success': True,
                'model_path': model_path,
                'inference_file': infer_file,
                'total_sequences': len(deeplog_df),
                'violations': len(violations),
                'violation_rate': violation_rate
            }
            
        except Exception as e:
            print(f"âŒ DeepLog ë¶„ì„ ì˜ˆì™¸: {e}")
            return {'success': False, 'error': str(e)}

    def run_mscred_build(self, target_result: Dict) -> Dict:
        """MS-CRED ì…ë ¥ ìƒì„± ì‹¤í–‰."""
        if not target_result['success']:
            return {'success': False, 'error': 'Target preprocessing failed'}
        
        print(f"ğŸ“Š MS-CRED ì…ë ¥ ìƒì„±: {target_result['file_path'].name}")
        
        try:
            parsed_file = target_result['output_dir'] / "parsed.parquet"
            
            # MS-CRED ì…ë ¥ ìƒì„±
            print("  ğŸ“Š ìœˆë„ìš° ì¹´ìš´íŠ¸ ë²¡í„° ìƒì„± ì¤‘...")
            
            cmd = [
                sys.executable, "-c", 
                f"""
import sys
sys.path.append('.')
from study_preprocessor.builders.mscred import build_mscred_window_counts

# MS-CRED ì…ë ¥ ìƒì„±
build_mscred_window_counts(
    parsed_parquet='{parsed_file}',
    out_dir='{target_result['output_dir']}',
    window_size=50,
    stride=25
)
print("MS-CRED input generation completed")
"""
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode != 0:
                print(f"âŒ MS-CRED ì…ë ¥ ìƒì„± ì‹¤íŒ¨: {result.stderr}")
                return {'success': False, 'error': result.stderr}
            
            # ê²°ê³¼ íŒŒì¼ í™•ì¸
            window_counts_file = target_result['output_dir'] / "window_counts.parquet"
            
            if not window_counts_file.exists():
                return {'success': False, 'error': 'MS-CRED window counts file not generated'}
            
            # ê²°ê³¼ ìš”ì•½
            import pandas as pd
            counts_df = pd.read_parquet(window_counts_file)
            num_windows = len(counts_df)
            num_templates = len(counts_df.columns) - 1 if 'window_start' in counts_df.columns else len(counts_df.columns)
            
            print(f"  âœ… MS-CRED ì…ë ¥ ìƒì„± ì™„ë£Œ: {num_windows}ê°œ ìœˆë„ìš° Ã— {num_templates}ê°œ í…œí”Œë¦¿")
            
            return {
                'success': True,
                'window_counts_file': window_counts_file,
                'num_windows': num_windows,
                'num_templates': num_templates
            }
            
        except Exception as e:
            print(f"âŒ MS-CRED ì…ë ¥ ìƒì„± ì˜ˆì™¸: {e}")
            return {'success': False, 'error': str(e)}

    def run_mscred_analysis(self, target_result: Dict) -> Dict:
        """MS-CRED í•™ìŠµ ë° ì¶”ë¡  ì‹¤í–‰."""
        if not target_result['success']:
            return {'success': False, 'error': 'Target preprocessing failed'}
        
        print(f"ğŸ§  MS-CRED í•™ìŠµ/ì¶”ë¡ : {target_result['file_path'].name}")
        
        try:
            output_dir = target_result['output_dir']
            window_counts_file = output_dir / "window_counts.parquet"
            
            if not window_counts_file.exists():
                return {'success': False, 'error': 'Window counts file not found'}
            
            # MS-CRED ëª¨ë¸ ê²½ë¡œ
            model_file = self.work_dir / f"mscred_{target_result['file_path'].stem}.pth"
            infer_file = output_dir / "mscred_infer.parquet"
            
            # 1. MS-CRED í•™ìŠµ
            print("  ğŸ§  MS-CRED ëª¨ë¸ í•™ìŠµ ì¤‘...")
            
            cmd = [
                sys.executable, "-c", 
                f"""
import sys
sys.path.append('.')
from study_preprocessor.mscred_model import train_mscred

# MS-CRED í•™ìŠµ
try:
    stats = train_mscred(
        window_counts_path='{window_counts_file}',
        model_output_path='{model_file}',
        epochs=30  # ë°°ì¹˜ ë¶„ì„ìš©ìœ¼ë¡œ ì ë‹¹í•œ ì—í¬í¬
    )
    print(f"í•™ìŠµ ì™„ë£Œ - ìµœì¢… ì†ì‹¤: {{stats['final_train_loss']:.4f}}")
except Exception as e:
    print(f"í•™ìŠµ ì‹¤íŒ¨: {{e}}")
    raise
"""
            ]
            
            result = subprocess.run(cmd, cwd=self.work_dir, capture_output=True, text=True, timeout=600)
            
            if result.returncode != 0:
                print(f"âŒ MS-CRED í•™ìŠµ ì‹¤íŒ¨:")
                print(f"  stdout: {result.stdout}")
                print(f"  stderr: {result.stderr}")
                return {'success': False, 'error': 'MS-CRED training failed'}
            
            if not model_file.exists():
                return {'success': False, 'error': 'MS-CRED model not created'}
            
            print(f"  âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {model_file}")
            
            # 2. MS-CRED ì¶”ë¡ 
            print("  ğŸ” MS-CRED ì´ìƒíƒì§€ ì¶”ë¡  ì¤‘...")
            
            cmd = [
                sys.executable, "-c", 
                f"""
import sys
sys.path.append('.')
from study_preprocessor.mscred_model import infer_mscred

# MS-CRED ì¶”ë¡ 
try:
    results_df = infer_mscred(
        window_counts_path='{window_counts_file}',
        model_path='{model_file}',
        output_path='{infer_file}',
        threshold_percentile=95.0
    )
    print(f"ì¶”ë¡  ì™„ë£Œ - ì´ìƒíƒì§€ìœ¨: {{results_df['is_anomaly'].mean():.1%}}")
except Exception as e:
    print(f"ì¶”ë¡  ì‹¤íŒ¨: {{e}}")
    raise
"""
            ]
            
            result = subprocess.run(cmd, cwd=self.work_dir, capture_output=True, text=True, timeout=300)
            
            if result.returncode != 0:
                print(f"âŒ MS-CRED ì¶”ë¡  ì‹¤íŒ¨:")
                print(f"  stdout: {result.stdout}")
                print(f"  stderr: {result.stderr}")
                return {'success': False, 'error': 'MS-CRED inference failed'}
            
            if not infer_file.exists():
                return {'success': False, 'error': 'MS-CRED inference file not created'}
            
            print(f"  âœ… ì¶”ë¡  ì™„ë£Œ: {infer_file}")
            
            # ê²°ê³¼ ìš”ì•½
            import pandas as pd
            mscred_df = pd.read_parquet(infer_file)
            anomalies = mscred_df[mscred_df['is_anomaly'] == True]
            anomaly_rate = len(anomalies) / len(mscred_df) if len(mscred_df) > 0 else 0
            
            print(f"âœ… MS-CRED ë¶„ì„ ì™„ë£Œ: ì´ìƒíƒì§€ìœ¨ {anomaly_rate:.1%} ({len(anomalies)}/{len(mscred_df)})")
            
            return {
                'success': True,
                'model_path': model_file,
                'inference_file': infer_file,
                'total_windows': len(mscred_df),
                'anomalies': len(anomalies),
                'anomaly_rate': anomaly_rate
            }
            
        except Exception as e:
            print(f"âŒ MS-CRED ë¶„ì„ ì˜ˆì™¸: {e}")
            return {'success': False, 'error': str(e)}
    
    def run_temporal_analysis(self, target_result: Dict) -> Dict:
        """ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ì‹¤í–‰ (ê¸°ì¡´ê³¼ ë™ì¼)."""
        if not target_result['success']:
            return {'success': False, 'error': 'Target preprocessing failed'}
        
        print(f"ğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€: {target_result['file_path'].name}")
        
        try:
            cmd = [
                sys.executable, "temporal_anomaly_detector.py",
                "--data-dir", str(target_result['output_dir']),
                "--output-dir", str(target_result['output_dir'] / "temporal_analysis")
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode != 0:
                print(f"âŒ ì‹œê°„ ë¶„ì„ ì‹¤íŒ¨: {result.stderr}")
                return {'success': False, 'error': result.stderr}
            
            temporal_dir = target_result['output_dir'] / "temporal_analysis"
            anomalies_file = temporal_dir / "temporal_anomalies.json"
            
            temporal_result = {'success': True, 'anomalies': []}
            if anomalies_file.exists():
                with open(anomalies_file) as f:
                    temporal_result['anomalies'] = json.load(f)
            
            print(f"âœ… ì‹œê°„ ë¶„ì„ ì™„ë£Œ: {len(temporal_result['anomalies'])}ê°œ ì´ìƒ ë°œê²¬")
            return temporal_result
            
        except Exception as e:
            print(f"âŒ ì‹œê°„ ë¶„ì„ ì˜ˆì™¸: {e}")
            return {'success': False, 'error': str(e)}
    
    def run_comparative_analysis(self, target_result: Dict, baseline_results: List[Dict]) -> Dict:
        """íŒŒì¼ë³„ ë¹„êµ ë¶„ì„ ì‹¤í–‰ (baseline í’ˆì§ˆ ê²€ì¦ ì¶”ê°€)."""
        if not target_result['success']:
            return {'success': False, 'error': 'Target preprocessing failed'}
        
        valid_baselines = [r for r in baseline_results if r['success']]
        if not valid_baselines:
            print("âš ï¸ ë¹„êµí•  baseline íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return {'success': False, 'error': 'No valid baseline files'}
        
        # Baseline í’ˆì§ˆ ê²€ì¦ ì¶”ê°€
        print(f"ğŸ” {len(valid_baselines)}ê°œ baseline íŒŒì¼ í’ˆì§ˆ ê²€ì¦ ì¤‘...")
        validated_baselines = self._validate_baseline_quality(valid_baselines)
        
        if len(validated_baselines) < len(valid_baselines):
            filtered_count = len(valid_baselines) - len(validated_baselines)
            print(f"âš ï¸  í’ˆì§ˆ ë¬¸ì œë¡œ {filtered_count}ê°œ baseline íŒŒì¼ ì œì™¸ë¨")
        
        if not validated_baselines:
            print("âŒ í’ˆì§ˆ ê¸°ì¤€ì„ ë§Œì¡±í•˜ëŠ” baseline íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
            return {'success': False, 'error': 'No quality baselines after validation'}
        
        print(f"ğŸ“Š íŒŒì¼ë³„ ë¹„êµ ë¶„ì„: {target_result['file_path'].name} vs {len(validated_baselines)}ê°œ ê²€ì¦ëœ íŒŒì¼")
        
        try:
            baseline_paths = [str(r['parsed_file']) for r in validated_baselines]
            
            cmd = [
                sys.executable, "comparative_anomaly_detector.py",
                "--target", str(target_result['parsed_file']),
                "--baselines"] + baseline_paths + [
                "--output-dir", str(target_result['output_dir'] / "comparative_analysis")
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, cwd=os.getcwd())
            
            if result.returncode != 0:
                print(f"âŒ ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {result.stderr}")
                return {'success': False, 'error': result.stderr}
            
            comp_dir = target_result['output_dir'] / "comparative_analysis"
            anomalies_file = comp_dir / "comparative_anomalies.json"
            
            comp_result = {'success': True, 'anomalies': [], 'baseline_count': len(validated_baselines)}
            if anomalies_file.exists():
                with open(anomalies_file) as f:
                    comp_result['anomalies'] = json.load(f)
            
            print(f"âœ… ë¹„êµ ë¶„ì„ ì™„ë£Œ: {len(comp_result['anomalies'])}ê°œ ì´ìƒ ë°œê²¬")
            return comp_result
            
        except Exception as e:
            print(f"âŒ ë¹„êµ ë¶„ì„ ì˜ˆì™¸: {e}")
            return {'success': False, 'error': str(e)}
    
    def generate_comprehensive_report(self, target_result: Dict, baseline_results: List[Dict],
                                     target_baseline_result: Dict, target_deeplog_result: Dict, target_mscred_result: Dict, 
                                     target_temporal_result: Dict, comparative_result: Dict, target_cli_report_result: Dict, 
                                     input_dir: str, max_depth: int, baseline_analysis_results: List[Dict] = None, 
                                     baseline_cli_reports: List[Dict] = None) -> str:
        """ì¢…í•© í†µí•© ë¦¬í¬íŠ¸ ìƒì„± - ëª¨ë“  ë¶„ì„ ê²°ê³¼ë¥¼ í•˜ë‚˜ì˜ ë¦¬í¬íŠ¸ë¡œ í†µí•©."""
        
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        target_name = target_result['file_path'].name
        
        report = f"""# í–¥ìƒëœ ë°°ì¹˜ ë¡œê·¸ ë¶„ì„ ë¦¬í¬íŠ¸

**ë¶„ì„ ì‹œê°„**: {timestamp}  
**ì…ë ¥ ë””ë ‰í† ë¦¬**: {input_dir}  
**ìŠ¤ìº” ê¹Šì´**: {max_depth}  
**Target íŒŒì¼**: {target_name}  
**ë°œê²¬ëœ íŒŒì¼**: {len(baseline_results) + 1}ê°œ  

## ğŸ“‚ ë””ë ‰í† ë¦¬ êµ¬ì¡° ë¶„ì„

### Target íŒŒì¼: {target_name}
"""
        
        if target_result['success']:
            stats = target_result['stats']
            validation = target_result.get('validation', {})
            
            # ì•ˆì „í•œ ì‹œê°„ ë²”ìœ„ ì²˜ë¦¬
            time_range = stats.get('time_range', {})
            start_time = time_range.get('start') if time_range else None
            end_time = time_range.get('end') if time_range else None
            if start_time and end_time and start_time != 'None' and end_time != 'None':
                time_range_str = f"{start_time} ~ {end_time}"
            else:
                time_range_str = "ì‹œê°„ ì •ë³´ ì—†ìŒ"
            
            # ì•ˆì „í•œ í˜¸ìŠ¤íŠ¸ ì •ë³´ ì²˜ë¦¬
            hosts = stats.get('hosts', [])
            if hosts and any(h for h in hosts if h is not None):
                valid_hosts = [str(h) for h in hosts if h is not None]
                hosts_str = f"{len(valid_hosts)}ê°œ ({', '.join(valid_hosts[:3])}{'...' if len(valid_hosts) > 3 else ''})"
            else:
                hosts_str = "í˜¸ìŠ¤íŠ¸ ì •ë³´ ì—†ìŒ"
            
            report += f"""- âœ… **ì„±ê³µ**: {stats['total_logs']:,}ê°œ ë¡œê·¸, {stats['unique_templates']}ê°œ í…œí”Œë¦¿
- **ì¹´í…Œê³ ë¦¬**: {target_result.get('category', 'root')}
- **íŒŒì¼ í¬ê¸°**: {validation.get('file_size_mb', 0):.1f}MB
- **ë¡œê·¸ í˜•ì‹**: {validation.get('format', 'unknown')}
- **ì‹œê°„ ë²”ìœ„**: {time_range_str}
- **í˜¸ìŠ¤íŠ¸**: {hosts_str}
"""
        else:
            validation = target_result.get('validation', {})
            report += f"""- âŒ **ì‹¤íŒ¨**: {target_result['error']}
- **ì¹´í…Œê³ ë¦¬**: {target_result.get('category', 'root')}
- **íŒŒì¼ í¬ê¸°**: {validation.get('file_size_mb', 0):.1f}MB
- **ë¡œê·¸ í˜•ì‹**: {validation.get('format', 'unknown')}
"""
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê²°ê³¼ ì •ë¦¬
        categories = {}
        for result in baseline_results:
            category = result.get('category', 'root')
            if category not in categories:
                categories[category] = []
            categories[category].append(result)
        
        report += "\n### Baseline íŒŒì¼ë“¤ (ì¹´í…Œê³ ë¦¬ë³„)\n"
        for category, results in categories.items():
            report += f"\n#### ğŸ“ {category}\n"
            for i, result in enumerate(results, 1):
                if result['success']:
                    stats = result['stats']
                    validation = result.get('validation', {})
                    report += f"{i}. âœ… **{result['file_path'].name}**: {stats['total_logs']:,}ê°œ ë¡œê·¸, {stats['unique_templates']}ê°œ í…œí”Œë¦¿ ({validation.get('file_size_mb', 0):.1f}MB)\n"
                else:
                    validation = result.get('validation', {})
                    report += f"{i}. âŒ **{result['file_path'].name}**: {result['error']} ({validation.get('file_size_mb', 0):.1f}MB)\n"
        
        # Target Baseline ê²°ê³¼ ì¶”ê°€
        report += "\n## ğŸ“ˆ Target íŒŒì¼ Baseline ì´ìƒ íƒì§€ ê²°ê³¼\n\n"
        if target_baseline_result['success']:
            total_windows = target_baseline_result['total_windows']
            anomaly_windows = target_baseline_result['anomaly_windows']
            anomaly_rate = target_baseline_result['anomaly_rate']
            
            report += f"**ë¶„ì„ ìœˆë„ìš°**: {total_windows:,}ê°œ\n"
            report += f"**ì´ìƒ ìœˆë„ìš°**: {anomaly_windows:,}ê°œ\n"
            report += f"**ì´ìƒìœ¨**: {anomaly_rate:.1%}\n\n"
            
            if anomaly_rate > 0.1:  # 10% ì´ìƒ
                report += "ğŸš¨ **ë†’ì€ ì´ìƒìœ¨**: ë‹¤ìˆ˜ì˜ ìœˆë„ìš°ì—ì„œ ì´ìƒ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
            elif anomaly_rate > 0.05:  # 5% ì´ìƒ
                report += "âš ï¸ **ì¤‘ê°„ ì´ìƒìœ¨**: ì¼ë¶€ ìœˆë„ìš°ì—ì„œ ì´ìƒ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
            else:
                report += "âœ… **ë‚®ì€ ì´ìƒìœ¨**: ëŒ€ë¶€ë¶„ ì •ìƒì ì¸ ë¡œê·¸ íŒ¨í„´ì…ë‹ˆë‹¤.\n"
        else:
            report += f"âŒ **Target Baseline ë¶„ì„ ì‹¤íŒ¨**: {target_baseline_result['error']}\n"
        
        # Baseline íŒŒì¼ë“¤ Baseline ê²°ê³¼ ì¶”ê°€
        if baseline_analysis_results:
            report += "\n## ğŸ“‚ Baseline íŒŒì¼ë“¤ ì´ìƒ íƒì§€ ê²°ê³¼\n\n"
            baseline_total_anomalies = 0
            baseline_total_windows = 0
            baseline_successful = 0
            
            for i, baseline_analysis in enumerate(baseline_analysis_results, 1):
                file_info = baseline_analysis['file_info']
                baseline_result = baseline_analysis['baseline_result']
                
                report += f"### {i}. {file_info['file_path'].name}\n"
                if baseline_result['success']:
                    anomaly_windows = baseline_result['anomaly_windows']
                    total_windows = baseline_result['total_windows']
                    anomaly_rate = baseline_result['anomaly_rate']
                    
                    baseline_total_anomalies += anomaly_windows
                    baseline_total_windows += total_windows
                    baseline_successful += 1
                    
                    report += f"- ì´ìƒ ìœˆë„ìš°: {anomaly_windows:,}/{total_windows:,}ê°œ ({anomaly_rate:.1%})\n"
                    if anomaly_rate > 0.1:
                        report += "- ğŸš¨ ë†’ì€ ì´ìƒìœ¨ ê°ì§€\n"
                    elif anomaly_rate > 0.05:
                        report += "- âš ï¸ ì¤‘ê°„ ì´ìƒìœ¨ ê°ì§€\n"
                    else:
                        report += "- âœ… ì •ìƒ ìˆ˜ì¤€\n"
                else:
                    report += f"- âŒ ë¶„ì„ ì‹¤íŒ¨: {baseline_result['error']}\n"
                report += "\n"
            
            if baseline_successful > 0:
                overall_rate = baseline_total_anomalies / max(baseline_total_windows, 1)
                report += f"**ì „ì²´ Baseline íŒŒì¼ ìš”ì•½**: {baseline_total_anomalies:,}/{baseline_total_windows:,}ê°œ ì´ìƒ ìœˆë„ìš° ({overall_rate:.1%})\n\n"
        
        # Target DeepLog ê²°ê³¼ ì¶”ê°€
        report += "\n## ğŸ§  Target íŒŒì¼ DeepLog ë”¥ëŸ¬ë‹ ë¶„ì„ ê²°ê³¼\n\n"
        if target_deeplog_result['success']:
            total_sequences = target_deeplog_result['total_sequences']
            violations = target_deeplog_result['violations']
            violation_rate = target_deeplog_result['violation_rate']
            
            report += f"**ì „ì²´ ì‹œí€€ìŠ¤**: {total_sequences:,}ê°œ\n"
            report += f"**ì˜ˆì¸¡ ì‹¤íŒ¨**: {violations:,}ê°œ\n"
            report += f"**ìœ„ë°˜ìœ¨**: {violation_rate:.1%}\n\n"
            
            if violation_rate > 0.5:  # 50% ì´ìƒ
                report += "ğŸš¨ **ë†’ì€ ìœ„ë°˜ìœ¨**: ë¡œê·¸ íŒ¨í„´ì´ ë§¤ìš° ì˜ˆì¸¡í•˜ê¸° ì–´ë ¤ìš´ ìƒíƒœì…ë‹ˆë‹¤.\n"
            elif violation_rate > 0.2:  # 20% ì´ìƒ
                report += "ğŸ” **ì¤‘ê°„ ìœ„ë°˜ìœ¨**: ì¼ë¶€ ì˜ˆì¸¡ ì–´ë ¤ìš´ ë¡œê·¸ íŒ¨í„´ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\n"
            else:
                report += "âœ… **ë‚®ì€ ìœ„ë°˜ìœ¨**: ëŒ€ë¶€ë¶„ ì˜ˆì¸¡ ê°€ëŠ¥í•œ ë¡œê·¸ íŒ¨í„´ì…ë‹ˆë‹¤.\n"
        else:
            report += f"âŒ Target DeepLog ë¶„ì„ ì‹¤íŒ¨: {target_deeplog_result.get('error', 'Unknown error')}\n"
        
        # Baseline íŒŒì¼ë“¤ DeepLog ê²°ê³¼ ì¶”ê°€
        if baseline_analysis_results:
            report += "\n## ğŸ“‚ Baseline íŒŒì¼ë“¤ DeepLog ë¶„ì„ ê²°ê³¼\n\n"
            deeplog_total_violations = 0
            deeplog_total_sequences = 0
            deeplog_successful = 0
            
            for i, baseline_analysis in enumerate(baseline_analysis_results, 1):
                file_info = baseline_analysis['file_info']
                deeplog_result = baseline_analysis['deeplog_result']
                
                report += f"### {i}. {file_info['file_path'].name}\n"
                if deeplog_result['success']:
                    violations = deeplog_result['violations']
                    total_sequences = deeplog_result['total_sequences']
                    violation_rate = deeplog_result['violation_rate']
                    
                    deeplog_total_violations += violations
                    deeplog_total_sequences += total_sequences
                    deeplog_successful += 1
                    
                    report += f"- ì˜ˆì¸¡ ì‹¤íŒ¨: {violations:,}/{total_sequences:,}ê°œ ì‹œí€€ìŠ¤ ({violation_rate:.1%})\n"
                    if violation_rate > 0.1:
                        report += "- ğŸš¨ ë†’ì€ ìœ„ë°˜ìœ¨ ê°ì§€\n"
                    elif violation_rate > 0.05:
                        report += "- âš ï¸ ì¤‘ê°„ ìœ„ë°˜ìœ¨ ê°ì§€\n"
                    else:
                        report += "- âœ… ì •ìƒ ìˆ˜ì¤€\n"
                else:
                    report += f"- âŒ ë¶„ì„ ì‹¤íŒ¨: {deeplog_result['error']}\n"
                report += "\n"
            
            if deeplog_successful > 0:
                overall_violation_rate = deeplog_total_violations / max(deeplog_total_sequences, 1)
                report += f"**ì „ì²´ Baseline íŒŒì¼ ìš”ì•½**: {deeplog_total_violations:,}/{deeplog_total_sequences:,}ê°œ ìœ„ë°˜ ì‹œí€€ìŠ¤ ({overall_violation_rate:.1%})\n\n"
        
        # Target MS-CRED ê²°ê³¼
        report += "\n## ğŸ”¬ Target íŒŒì¼ MS-CRED ë©€í‹°ìŠ¤ì¼€ì¼ ë¶„ì„ ê²°ê³¼\n\n"
        if target_mscred_result['success']:
            total_windows = target_mscred_result['total_windows']
            anomalies = target_mscred_result['anomalies']
            anomaly_rate = target_mscred_result['anomaly_rate']
            
            report += f"**ì „ì²´ ìœˆë„ìš°**: {total_windows:,}ê°œ\n"
            report += f"**ì´ìƒ ìœˆë„ìš°**: {anomalies:,}ê°œ\n"
            report += f"**ì´ìƒíƒì§€ìœ¨**: {anomaly_rate:.1%}\n\n"
            
            if anomaly_rate > 0.2:  # 20% ì´ìƒ
                report += "ğŸš¨ **ë†’ì€ ì´ìƒë¥ **: ë§ì€ ìœˆë„ìš°ì—ì„œ ë¹„ì •ìƒì ì¸ íŒ¨í„´ì´ ê°ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
            elif anomaly_rate > 0.05:  # 5% ì´ìƒ
                report += "ğŸ” **ì¤‘ê°„ ì´ìƒë¥ **: ì¼ë¶€ ìœˆë„ìš°ì—ì„œ ì£¼ëª©í•  ë§Œí•œ íŒ¨í„´ ë³€í™”ê°€ ìˆìŠµë‹ˆë‹¤.\n"
            else:
                report += "âœ… **ë‚®ì€ ì´ìƒë¥ **: ëŒ€ë¶€ë¶„ ì •ìƒì ì¸ ë¡œê·¸ íŒ¨í„´ì„ ë³´ì…ë‹ˆë‹¤.\n"
        else:
            report += f"âŒ Target MS-CRED ë¶„ì„ ì‹¤íŒ¨: {target_mscred_result.get('error', 'Unknown error')}\n"
        
        # Baseline íŒŒì¼ë“¤ MS-CRED ê²°ê³¼ ì¶”ê°€
        if baseline_analysis_results:
            report += "\n## ğŸ“‚ Baseline íŒŒì¼ë“¤ MS-CRED ë¶„ì„ ê²°ê³¼\n\n"
            mscred_total_anomalies = 0
            mscred_total_windows = 0
            mscred_successful = 0
            
            for i, baseline_analysis in enumerate(baseline_analysis_results, 1):
                file_info = baseline_analysis['file_info']
                mscred_result = baseline_analysis['mscred_result']
                
                report += f"### {i}. {file_info['file_path'].name}\n"
                if mscred_result['success']:
                    anomalies = mscred_result['anomalies']
                    total_windows = mscred_result['total_windows']
                    anomaly_rate = mscred_result['anomaly_rate']
                    
                    mscred_total_anomalies += anomalies
                    mscred_total_windows += total_windows
                    mscred_successful += 1
                    
                    report += f"- ì´ìƒ ìœˆë„ìš°: {anomalies:,}/{total_windows:,}ê°œ ({anomaly_rate:.1%})\n"
                    if anomaly_rate > 0.2:
                        report += "- ğŸš¨ ë†’ì€ ì´ìƒë¥  ê°ì§€\n"
                    elif anomaly_rate > 0.05:
                        report += "- âš ï¸ ì¤‘ê°„ ì´ìƒë¥  ê°ì§€\n"
                    else:
                        report += "- âœ… ì •ìƒ ìˆ˜ì¤€\n"
                else:
                    report += f"- âŒ ë¶„ì„ ì‹¤íŒ¨: {mscred_result['error']}\n"
                report += "\n"
            
            if mscred_successful > 0:
                overall_mscred_rate = mscred_total_anomalies / max(mscred_total_windows, 1)
                report += f"**ì „ì²´ Baseline íŒŒì¼ ìš”ì•½**: {mscred_total_anomalies:,}/{mscred_total_windows:,}ê°œ ì´ìƒ ìœˆë„ìš° ({overall_mscred_rate:.1%})\n\n"
        
        # Target ì‹œê°„ ê¸°ë°˜ ë¶„ì„
        report += "\n## ğŸ• Target íŒŒì¼ ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ê²°ê³¼\n\n"
        if target_temporal_result['success']:
            anomalies = target_temporal_result['anomalies']
            if anomalies:
                high_count = len([a for a in anomalies if a.get('severity') == 'high'])
                medium_count = len([a for a in anomalies if a.get('severity') == 'medium'])
                report += f"ğŸš¨ **ë°œê²¬ëœ ì´ìƒ**: {len(anomalies)}ê°œ (ì‹¬ê°: {high_count}ê°œ, ì£¼ì˜: {medium_count}ê°œ)\n\n"
                
                for anomaly in anomalies[:5]:
                    report += f"- **{anomaly.get('type', 'unknown')}** ({anomaly.get('hour', '?')}ì‹œ): {anomaly.get('description', 'No description')}\n"
                
                if len(anomalies) > 5:
                    report += f"- ... ë° {len(anomalies) - 5}ê°œ ì¶”ê°€\n"
            else:
                report += "âœ… ì‹œê°„ ê¸°ë°˜ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        else:
            report += f"âŒ Target ì‹œê°„ ê¸°ë°˜ ë¶„ì„ ì‹¤íŒ¨: {target_temporal_result.get('error', 'Unknown error')}\n"
        
        # Baseline íŒŒì¼ë“¤ ì‹œê°„ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼ ì¶”ê°€
        if baseline_analysis_results:
            report += "\n## ğŸ“‚ Baseline íŒŒì¼ë“¤ ì‹œê°„ ê¸°ë°˜ ë¶„ì„ ê²°ê³¼\n\n"
            temporal_total_anomalies = 0
            temporal_successful = 0
            
            for i, baseline_analysis in enumerate(baseline_analysis_results, 1):
                file_info = baseline_analysis['file_info']
                temporal_result = baseline_analysis['temporal_result']
                
                report += f"### {i}. {file_info['file_path'].name}\n"
                if temporal_result['success']:
                    anomalies = temporal_result['anomalies']
                    temporal_total_anomalies += len(anomalies)
                    temporal_successful += 1
                    
                    if anomalies:
                        high_count = len([a for a in anomalies if a.get('severity') == 'high'])
                        medium_count = len([a for a in anomalies if a.get('severity') == 'medium'])
                        report += f"- ë°œê²¬ëœ ì´ìƒ: {len(anomalies)}ê°œ (ì‹¬ê°: {high_count}ê°œ, ì£¼ì˜: {medium_count}ê°œ)\n"
                        if high_count > 0:
                            report += "- ğŸš¨ ì‹¬ê°í•œ ì‹œê°„ íŒ¨í„´ ì´ìƒ ê°ì§€\n"
                        elif medium_count > 0:
                            report += "- âš ï¸ ì¤‘ê°„ ìˆ˜ì¤€ ì‹œê°„ íŒ¨í„´ ì´ìƒ ê°ì§€\n"
                        else:
                            report += "- âœ… ê²½ë¯¸í•œ ì‹œê°„ íŒ¨í„´ ë³€í™”\n"
                    else:
                        report += "- âœ… ì‹œê°„ íŒ¨í„´ ì •ìƒ\n"
                else:
                    report += f"- âŒ ë¶„ì„ ì‹¤íŒ¨: {temporal_result['error']}\n"
                report += "\n"
            
            if temporal_successful > 0:
                report += f"**ì „ì²´ Baseline íŒŒì¼ ìš”ì•½**: ì´ {temporal_total_anomalies}ê°œ ì‹œê°„ ê¸°ë°˜ ì´ìƒ ê°ì§€ ({temporal_successful}ê°œ íŒŒì¼)\n\n"
        
        report += "\n## ğŸ“Š íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€ ê²°ê³¼\n\n"
        if comparative_result['success']:
            anomalies = comparative_result['anomalies']
            baseline_count = comparative_result['baseline_count']
            report += f"**ë¹„êµ ëŒ€ìƒ**: {baseline_count}ê°œ baseline íŒŒì¼\n\n"
            
            if anomalies:
                high_count = len([a for a in anomalies if a.get('severity') == 'high'])
                medium_count = len([a for a in anomalies if a.get('severity') == 'medium'])
                report += f"ğŸš¨ **ë°œê²¬ëœ ì´ìƒ**: {len(anomalies)}ê°œ (ì‹¬ê°: {high_count}ê°œ, ì£¼ì˜: {medium_count}ê°œ)\n\n"
                
                for anomaly in anomalies[:5]:
                    report += f"- **{anomaly.get('type', 'unknown')}**: {anomaly.get('description', 'No description')}\n"
                
                if len(anomalies) > 5:
                    report += f"- ... ë° {len(anomalies) - 5}ê°œ ì¶”ê°€\n"
            else:
                report += "âœ… íŒŒì¼ë³„ ë¹„êµì—ì„œ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        else:
            report += f"âŒ íŒŒì¼ë³„ ë¹„êµ ë¶„ì„ ì‹¤íŒ¨: {comparative_result.get('error', 'Unknown error')}\n"
        
        # CLI ë¦¬í¬íŠ¸ ìƒì„± ê²°ê³¼ ì¶”ê°€ (ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ í¬í•¨)
        report += "\n## ğŸ“„ CLI ë¦¬í¬íŠ¸ ë° ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ê²°ê³¼\n\n"
        if target_cli_report_result['success']:
            report_file = target_cli_report_result.get('report_file')
            if report_file:
                report += f"**CLI ë¦¬í¬íŠ¸**: `{report_file}`\n"
                report += "â†’ ê¸°ë³¸ íƒì§€ ê²°ê³¼ ë° í†µê³„ ì •ë³´ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
            
            # ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ íŒŒì¼ í™•ì¸
            target_dir = target_result['output_dir']
            sample_analysis_dir = target_dir / "log_samples_analysis"
            if sample_analysis_dir.exists():
                sample_report = sample_analysis_dir / "anomaly_analysis_report.md"
                sample_data = sample_analysis_dir / "anomaly_samples.json"
                
                if sample_report.exists():
                    report += f"**ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ë¦¬í¬íŠ¸**: `{sample_report}`\n"
                    report += "â†’ ì‹¤ì œ ë¬¸ì œ ë¡œê·¸ë“¤ê³¼ ì „í›„ ë§¥ë½ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                
                if sample_data.exists():
                    report += f"**ìƒì„¸ ìƒ˜í”Œ ë°ì´í„°**: `{sample_data}`\n"
                    report += "â†’ êµ¬ì¡°í™”ëœ ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ ë°ì´í„°ì…ë‹ˆë‹¤.\n\n"
            
            report += "âœ… CLI ë¦¬í¬íŠ¸ ë° ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ì´ ì •ìƒì ìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n"
        else:
            report += f"âŒ Target CLI ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {target_cli_report_result.get('error', 'Unknown error')}\n"
        
        # Baseline CLI ë¦¬í¬íŠ¸ë“¤ ìš”ì•½
        if baseline_cli_reports:
            successful_baseline_reports = [r for r in baseline_cli_reports if r['success']]
            report += f"\nğŸ“‚ **Baseline CLI ë¦¬í¬íŠ¸**: {len(successful_baseline_reports)}/{len(baseline_cli_reports)}ê°œ ì„±ê³µ\n"
        
        # ì‹¤ì œ ë¡œê·¸ ìƒ˜í”Œ í†µí•©
        report += self._add_log_samples_to_report(target_result, target_baseline_result, target_deeplog_result, target_mscred_result, target_temporal_result, comparative_result)
        
        # ê¶Œê³ ì‚¬í•­ ë° ìƒì„¸ ê²°ê³¼
        total_anomalies = 0
        if target_temporal_result['success']:
            total_anomalies += len(target_temporal_result.get('anomalies', []))
        if comparative_result['success']:
            total_anomalies += len(comparative_result.get('anomalies', []))
        if target_baseline_result['success']:
            total_anomalies += target_baseline_result.get('anomaly_windows', 0)
        
        report += "\n## ğŸ’¡ ê¶Œê³ ì‚¬í•­\n\n"
        if total_anomalies == 0:
            report += "âœ… ëª¨ë“  ë¶„ì„ì—ì„œ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n"
        elif total_anomalies < 5:
            report += "ğŸ” ì¼ë¶€ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„¸ ë¶„ì„ì„ ê¶Œì¥í•©ë‹ˆë‹¤.\n"
        else:
            report += "âš ï¸ ë‹¤ìˆ˜ì˜ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. ê¸´ê¸‰ ì ê²€ì´ í•„ìš”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
        
        if target_result['success']:
            report += f"""
## ğŸ“‚ ìƒì„¸ ê²°ê³¼ íŒŒì¼
- **Target ë¶„ì„ ê²°ê³¼**: `{target_result['output_dir']}/`
- **ì‹œê°„ ê¸°ë°˜ ë¶„ì„**: `{target_result['output_dir']}/temporal_analysis/temporal_report.md`
- **íŒŒì¼ë³„ ë¹„êµ ë¶„ì„**: `{target_result['output_dir']}/comparative_analysis/comparative_report.md`

## ğŸ”§ ì¶”ê°€ ë¶„ì„ ëª…ë ¹ì–´
```bash
# ìƒì„¸ ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ (ë‹¨ë… ì‹¤í–‰)
study-preprocess analyze-samples --processed-dir {target_result['output_dir']}

# ë¡œê·¸ ìƒ˜í”Œ í¬í•¨ ë¦¬í¬íŠ¸ ìƒì„±
study-preprocess report --processed-dir {target_result['output_dir']} --with-samples

# ìƒì„¸ ë¶„ì„
python analyze_results.py --data-dir {target_result['output_dir']}

# ì‹œê°í™”
python visualize_results.py --data-dir {target_result['output_dir']}
```
"""
        
        return report
    
    def _add_log_samples_to_report(self, target_result: Dict, baseline_result: Dict, deeplog_result: Dict, 
                                   mscred_result: Dict, temporal_result: Dict, comparative_result: Dict) -> str:
        """ì‹¤ì œ ë¡œê·¸ ìƒ˜í”Œë“¤ì„ ë¦¬í¬íŠ¸ì— ì§ì ‘ í¬í•¨í•©ë‹ˆë‹¤."""
        if not target_result['success']:
            return "\n## ğŸ” ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„\n\nâŒ Target ì „ì²˜ë¦¬ ì‹¤íŒ¨ë¡œ ë¡œê·¸ ìƒ˜í”Œì„ ë¶„ì„í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
        
        report = "\n## ğŸ” ì‹¤ì œ ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„\n\n"
        report += "ë‹¤ìŒì€ ê° ë¶„ì„ ë°©ë²•ìœ¼ë¡œ ë°œê²¬ëœ ì‹¤ì œ ë¬¸ì œ ë¡œê·¸ë“¤ì˜ ìƒ˜í”Œì…ë‹ˆë‹¤.\n\n"
        
        # Baseline ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ
        if baseline_result['success'] and baseline_result.get('anomaly_windows', 0) > 0:
            report += self._extract_baseline_samples(target_result)
        
        # DeepLog ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ  
        if deeplog_result['success'] and deeplog_result.get('violations', 0) > 0:
            report += self._extract_deeplog_samples(target_result)
        
        # MS-CRED ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ
        if mscred_result['success'] and mscred_result.get('anomalies', 0) > 0:
            report += self._extract_mscred_samples(target_result)
        
        # ì‹œê°„ ê¸°ë°˜ ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ
        if temporal_result['success'] and len(temporal_result.get('anomalies', [])) > 0:
            report += self._extract_temporal_samples(target_result, temporal_result)
        
        # ë¹„êµ ë¶„ì„ ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ
        if comparative_result['success'] and len(comparative_result.get('anomalies', [])) > 0:
            report += self._extract_comparative_samples(target_result, comparative_result)
        
        return report
    
    def _extract_baseline_samples(self, target_result: Dict) -> str:
        """Baseline ì´ìƒ ë¡œê·¸ ìƒ˜í”Œì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            import pandas as pd
            
            baseline_scores_file = target_result['output_dir'] / "baseline_scores.parquet"
            parsed_file = target_result['output_dir'] / "parsed.parquet"
            
            if not baseline_scores_file.exists() or not parsed_file.exists():
                return ""
            
            scores_df = pd.read_parquet(baseline_scores_file)
            parsed_df = pd.read_parquet(parsed_file)
            
            anomaly_windows = scores_df[scores_df['is_anomaly'] == True].head(3)
            
            if len(anomaly_windows) == 0:
                return ""
            
            sample_text = "### ğŸ“ˆ Baseline ì´ìƒ íƒì§€ ìƒ˜í”Œ\n\n"
            
            for _, window in anomaly_windows.iterrows():
                start_line = int(window['window_start_line'])
                score = window['score']
                
                # í•´ë‹¹ ìœˆë„ìš°ì˜ ë¡œê·¸ë“¤ ì¶”ì¶œ
                window_logs = parsed_df[
                    (parsed_df['line_no'] >= start_line) & 
                    (parsed_df['line_no'] < start_line + 50)
                ].head(5)
                
                sample_text += f"**ìœˆë„ìš° ì‹œì‘ë¼ì¸ {start_line}** (ì ìˆ˜: {score:.3f})\n"
                sample_text += "```\n"
                for _, log in window_logs.iterrows():
                    log_text = str(log.get('raw', ''))[:100]
                    sample_text += f"Line {log['line_no']}: {log_text}...\n"
                sample_text += "```\n\n"
            
            return sample_text
            
        except Exception as e:
            return f"âš ï¸ Baseline ìƒ˜í”Œ ì¶”ì¶œ ì‹¤íŒ¨: {e}\n\n"
    
    def _extract_deeplog_samples(self, target_result: Dict) -> str:
        """DeepLog ì´ìƒ ë¡œê·¸ ìƒ˜í”Œì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            import pandas as pd
            
            deeplog_file = target_result['output_dir'] / "deeplog_infer.parquet"
            parsed_file = target_result['output_dir'] / "parsed.parquet"
            
            if not deeplog_file.exists() or not parsed_file.exists():
                return ""
            
            deeplog_df = pd.read_parquet(deeplog_file)
            parsed_df = pd.read_parquet(parsed_file)
            
            violations = deeplog_df[deeplog_df['in_topk'] == False].head(5)
            
            if len(violations) == 0:
                return ""
            
            sample_text = "### ğŸ§  DeepLog ì´ìƒ íƒì§€ ìƒ˜í”Œ\n\n"
            
            for _, violation in violations.iterrows():
                line_no = violation['line_no']
                
                # í•´ë‹¹ ë¼ì¸ì˜ ë¡œê·¸ ì¶”ì¶œ
                log_line = parsed_df[parsed_df['line_no'] == line_no]
                
                if len(log_line) > 0:
                    log = log_line.iloc[0]
                    log_text = str(log.get('raw', ''))
                    template_id = log.get('template_id', 'Unknown')
                    
                    sample_text += f"**Line {line_no}** (Template ID: {template_id})\n"
                    sample_text += f"```\n{log_text}\n```\n"
                    sample_text += f"â†’ ì˜ˆì¸¡ ì‹¤íŒ¨: ì´ ë¡œê·¸ íŒ¨í„´ì´ ì˜ˆìƒë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n"
            
            return sample_text
            
        except Exception as e:
            return f"âš ï¸ DeepLog ìƒ˜í”Œ ì¶”ì¶œ ì‹¤íŒ¨: {e}\n\n"
    
    def _extract_mscred_samples(self, target_result: Dict) -> str:
        """MS-CRED ì´ìƒ ë¡œê·¸ ìƒ˜í”Œì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            import pandas as pd
            
            mscred_file = target_result['output_dir'] / "mscred_infer.parquet"
            parsed_file = target_result['output_dir'] / "parsed.parquet"
            window_counts_file = target_result['output_dir'] / "window_counts.parquet"
            
            if not mscred_file.exists() or not parsed_file.exists():
                return ""
            
            mscred_df = pd.read_parquet(mscred_file)
            parsed_df = pd.read_parquet(parsed_file)
            
            # ìƒìœ„ 5ê°œ ì´ìƒ ìœˆë„ìš° ì¶”ì¶œ (ì¬êµ¬ì„± ì˜¤ì°¨ê°€ ë†’ì€ ìˆœ)
            anomaly_windows = mscred_df[mscred_df['is_anomaly'] == True].nlargest(5, 'reconstruction_error')
            
            if len(anomaly_windows) == 0:
                return ""
            
            sample_text = "### ğŸ”¬ MS-CRED ì´ìƒ íƒì§€ ìƒ˜í”Œ\n\n"
            
            for i, (_, anomaly) in enumerate(anomaly_windows.iterrows(), 1):
                window_idx = int(anomaly['window_idx'])
                start_index = int(anomaly.get('start_index', window_idx * 25))  # ê¸°ë³¸ stride=25
                reconstruction_error = float(anomaly['reconstruction_error'])
                
                # ìœˆë„ìš° ë²”ìœ„ì˜ ë¡œê·¸ë“¤ ì¶”ì¶œ (50ê°œ ë¼ì¸ ê¸°ë³¸)
                window_logs = parsed_df[
                    (parsed_df['line_no'] >= start_index) & 
                    (parsed_df['line_no'] < start_index + 50)
                ].copy()
                
                if len(window_logs) == 0:
                    continue
                
                # ì—ëŸ¬ ë¡œê·¸ì™€ ì¼ë°˜ ë¡œê·¸ ë¶„ë¦¬
                error_logs = window_logs[
                    window_logs['raw'].str.contains(
                        r'error|Error|ERROR|fail|Fail|FAIL|exception|Exception|EXCEPTION|warning|Warning|WARNING|critical|Critical|CRITICAL',
                        case=False, na=False, regex=True
                    )
                ]
                
                # ìœˆë„ìš° ì •ë³´
                sample_text += f"**ì´ìƒ ìœˆë„ìš° #{i}** (ìœˆë„ìš° ID: {window_idx}, ì‹œì‘ ë¼ì¸: {start_index})\n"
                sample_text += f"- ì¬êµ¬ì„± ì˜¤ì°¨: {reconstruction_error:.4f}\n"
                sample_text += f"- ì´ ë¡œê·¸ ìˆ˜: {len(window_logs)}\n"
                sample_text += f"- ì—ëŸ¬ ë¡œê·¸ ìˆ˜: {len(error_logs)}\n\n"
                
                # ì—ëŸ¬ ë¡œê·¸ ìƒ˜í”Œ (ìµœëŒ€ 3ê°œ)
                if len(error_logs) > 0:
                    sample_text += "**ğŸš¨ ì—ëŸ¬ ë¡œê·¸ ìƒ˜í”Œ:**\n"
                    for _, error_log in error_logs.head(3).iterrows():
                        line_no = error_log['line_no']
                        log_text = str(error_log.get('raw', ''))
                        template_id = error_log.get('template_id', 'Unknown')
                        
                        sample_text += f"- Line {line_no} (Template: {template_id})\n"
                        sample_text += f"```\n{log_text[:200]}{'...' if len(log_text) > 200 else ''}\n```\n"
                    sample_text += "\n"
                
                # ì¼ë°˜ ë¡œê·¸ ìƒ˜í”Œ (ìµœëŒ€ 2ê°œ)
                normal_logs = window_logs[~window_logs.index.isin(error_logs.index)]
                if len(normal_logs) > 0:
                    sample_text += "**ğŸ“„ ìœˆë„ìš° ë‚´ ë‹¤ë¥¸ ë¡œê·¸ë“¤:**\n"
                    for _, normal_log in normal_logs.head(2).iterrows():
                        line_no = normal_log['line_no']
                        log_text = str(normal_log.get('raw', ''))
                        
                        sample_text += f"- Line {line_no}\n"
                        sample_text += f"```\n{log_text[:150]}{'...' if len(log_text) > 150 else ''}\n```\n"
                    sample_text += "\n"
                
                # í…œí”Œë¦¿ ë¶„í¬ ì •ë³´
                template_counts = window_logs['template_id'].value_counts().head(3)
                if len(template_counts) > 0:
                    sample_text += "**ğŸ“Š ì£¼ìš” í…œí”Œë¦¿:**\n"
                    for template_id, count in template_counts.items():
                        sample_text += f"- Template {template_id}: {count}íšŒ\n"
                    sample_text += "\n"
                
                sample_text += f"â†’ **ë¶„ì„**: ì´ ìœˆë„ìš°ëŠ” ì •ìƒ íŒ¨í„´ê³¼ ë‹¬ë¦¬ ì¬êµ¬ì„± ì˜¤ì°¨ê°€ {reconstruction_error:.4f}ë¡œ ë†’ê²Œ ë‚˜íƒ€ë‚¬ìŠµë‹ˆë‹¤.\n\n"
                sample_text += "---\n\n"
            
            return sample_text
            
        except Exception as e:
            return f"âš ï¸ MS-CRED ìƒ˜í”Œ ì¶”ì¶œ ì‹¤íŒ¨: {e}\n\n"
    
    def _extract_temporal_samples(self, target_result: Dict, temporal_result: Dict) -> str:
        """ì‹œê°„ ê¸°ë°˜ ì´ìƒ ë¡œê·¸ ìƒ˜í”Œì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            anomalies = temporal_result.get('anomalies', [])[:3]
            if not anomalies:
                return ""
            
            sample_text = "### ğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ìƒ˜í”Œ\n\n"
            
            for anomaly in anomalies:
                hour = anomaly.get('hour', 'Unknown')
                anomaly_type = anomaly.get('type', 'Unknown')
                description = anomaly.get('description', 'No description')
                severity = anomaly.get('severity', 'medium')
                
                severity_icon = {'high': 'ğŸš¨', 'medium': 'âš ï¸', 'low': 'ğŸ”'}.get(severity, 'âš ï¸')
                
                sample_text += f"**{severity_icon} {hour}ì‹œ ì´ìƒ í˜„ìƒ**\n"
                sample_text += f"- **ìœ í˜•**: {anomaly_type}\n"
                sample_text += f"- **ì„¤ëª…**: {description}\n"
                sample_text += f"- **ì‹¬ê°ë„**: {severity}\n\n"
            
            return sample_text
            
        except Exception as e:
            return f"âš ï¸ ì‹œê°„ ê¸°ë°˜ ìƒ˜í”Œ ì¶”ì¶œ ì‹¤íŒ¨: {e}\n\n"
    
    def _extract_comparative_samples(self, target_result: Dict, comparative_result: Dict) -> str:
        """ë¹„êµ ë¶„ì„ ì´ìƒ ë¡œê·¸ ìƒ˜í”Œì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        try:
            anomalies = comparative_result.get('anomalies', [])[:3]
            if not anomalies:
                return ""
            
            sample_text = "### ğŸ“Š ë¹„êµ ë¶„ì„ ì´ìƒ íƒì§€ ìƒ˜í”Œ\n\n"
            
            for anomaly in anomalies:
                anomaly_type = anomaly.get('type', 'Unknown')
                description = anomaly.get('description', 'No description')
                severity = anomaly.get('severity', 'medium')
                
                severity_icon = {'high': 'ğŸš¨', 'medium': 'âš ï¸', 'low': 'ğŸ”'}.get(severity, 'âš ï¸')
                
                sample_text += f"**{severity_icon} {anomaly_type} ì´ìƒ**\n"
                sample_text += f"- **ì„¤ëª…**: {description}\n"
                sample_text += f"- **ì‹¬ê°ë„**: {severity}\n\n"
            
            return sample_text
            
        except Exception as e:
            return f"âš ï¸ ë¹„êµ ë¶„ì„ ìƒ˜í”Œ ì¶”ì¶œ ì‹¤íŒ¨: {e}\n\n"

def main():
    parser = argparse.ArgumentParser(description="í–¥ìƒëœ ë°°ì¹˜ ë¡œê·¸ ë¶„ì„ê¸°")
    parser.add_argument("input_dir", help="ë¡œê·¸ íŒŒì¼ë“¤ì´ ìˆëŠ” ë£¨íŠ¸ ë””ë ‰í† ë¦¬")
    parser.add_argument("--target", help="ë¶„ì„í•  target íŒŒì¼ (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ ê°€ì¥ í° íŒŒì¼)")
    parser.add_argument("--max-depth", type=int, default=3, help="í•˜ìœ„ ë””ë ‰í† ë¦¬ ìŠ¤ìº” ìµœëŒ€ ê¹Šì´ (ê¸°ë³¸: 3)")
    parser.add_argument("--max-files", type=int, default=20, help="ìµœëŒ€ ì²˜ë¦¬ íŒŒì¼ ìˆ˜ (ê¸°ë³¸: 20)")
    parser.add_argument("--work-dir", help="ì‘ì—… ë””ë ‰í† ë¦¬ (ê¸°ë³¸: ìë™ ìƒì„±)")
    
    args = parser.parse_args()
    
    # ë¶„ì„ê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰
    analyzer = EnhancedBatchAnalyzer(args.work_dir)
    result = analyzer.run_enhanced_analysis(
        args.input_dir, 
        args.target,
        args.max_depth,
        args.max_files
    )
    
    if not result['success']:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {result['error']}")
        sys.exit(1)
    
    print(f"\nğŸ‰ í–¥ìƒëœ ë°°ì¹˜ ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“Š ì²˜ë¦¬ëœ íŒŒì¼: {result['files_processed']}/{result['total_files_found']}ê°œ")

if __name__ == "__main__":
    main()
