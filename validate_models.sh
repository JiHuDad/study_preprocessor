#!/bin/bash

# ëª¨ë¸ ê²€ì¦ ë„êµ¬
# í•™ìŠµëœ ëª¨ë¸ë“¤ì˜ í’ˆì§ˆê³¼ ì„±ëŠ¥ì„ ìë™ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤.
# ì‚¬ìš©ë²•: ./validate_models.sh <ëª¨ë¸ë””ë ‰í† ë¦¬> [ê²€ì¦ë¡œê·¸íŒŒì¼] [ê²°ê³¼ë””ë ‰í† ë¦¬]

set -e  # ì—ëŸ¬ ë°œìƒì‹œ ì¦‰ì‹œ ì¤‘ë‹¨

# ê¸°ë³¸ê°’ ì„¤ì •
MODEL_DIR="$1"
VALIDATION_LOG="$2"
RESULT_DIR="${3:-validation_$(date +%Y%m%d_%H%M%S)}"

# ì¸ìˆ˜ í™•ì¸
if [ -z "$MODEL_DIR" ]; then
    echo "âŒ ì‚¬ìš©ë²•: $0 <ëª¨ë¸ë””ë ‰í† ë¦¬> [ê²€ì¦ë¡œê·¸íŒŒì¼] [ê²°ê³¼ë””ë ‰í† ë¦¬]"
    echo ""
    echo "ì˜ˆì‹œ:"
    echo "  $0 models_20241002_143022"
    echo "  $0 models_20241002_143022 /var/log/validation.log"
    echo "  $0 models_20241002_143022 /var/log/validation.log validation_results"
    echo ""
    echo "ğŸ“‹ ì„¤ëª…:"
    echo "  - ëª¨ë¸ë””ë ‰í† ë¦¬: ê²€ì¦í•  í•™ìŠµëœ ëª¨ë¸ë“¤ì´ ìˆëŠ” í´ë”"
    echo "  - ê²€ì¦ë¡œê·¸íŒŒì¼: ê²€ì¦ìš© ë¡œê·¸ íŒŒì¼ (ìƒëµì‹œ í•©ì„± ë°ì´í„° ì‚¬ìš©)"
    echo "  - ê²°ê³¼ë””ë ‰í† ë¦¬: ê²€ì¦ ê²°ê³¼ë¥¼ ì €ì¥í•  í´ë” (ìƒëµì‹œ ìë™ ìƒì„±)"
    echo ""
    echo "ğŸ’¡ ê²€ì¦ í•­ëª©:"
    echo "  - ğŸ“ ëª¨ë¸ íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬"
    echo "  - ğŸ”§ ëª¨ë¸ ë¡œë”© ë° ì‹¤í–‰ ê°€ëŠ¥ì„± ê²€ì¦"
    echo "  - ğŸ“Š ì¶”ë¡  ì„±ëŠ¥ ë° ì†ë„ ì¸¡ì •"
    echo "  - ğŸ¯ ì´ìƒíƒì§€ ì •í™•ë„ í‰ê°€"
    echo "  - ğŸ“ˆ ëª¨ë¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"
    echo "  - ğŸ” ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"
    echo "  - ğŸ“‹ ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"
    exit 1
fi

if [ ! -d "$MODEL_DIR" ]; then
    echo "âŒ ëª¨ë¸ ë””ë ‰í† ë¦¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: $MODEL_DIR"
    exit 1
fi

# ê°€ìƒí™˜ê²½ ìë™ ê°ì§€ ë° í™œì„±í™”
VENV_ACTIVATED=false

if [ -n "$VIRTUAL_ENV" ]; then
    echo "ğŸ”µ ê¸°ì¡´ ê°€ìƒí™˜ê²½ ê°ì§€ë¨: $VIRTUAL_ENV"
    VENV_ACTIVATED=true
elif [ -f ".venv/bin/activate" ]; then
    echo "ğŸ”µ .venv ê°€ìƒí™˜ê²½ ë°œê²¬, í™œì„±í™” ì¤‘..."
    source .venv/bin/activate
    VENV_ACTIVATED=true
    echo "âœ… ê°€ìƒí™˜ê²½ í™œì„±í™”ë¨: $VIRTUAL_ENV"
elif [ -f "venv/bin/activate" ]; then
    echo "ğŸ”µ venv ê°€ìƒí™˜ê²½ ë°œê²¬, í™œì„±í™” ì¤‘..."
    source venv/bin/activate
    VENV_ACTIVATED=true
    echo "âœ… ê°€ìƒí™˜ê²½ í™œì„±í™”ë¨: $VIRTUAL_ENV"
fi

# Python ëª…ë ¹ì–´ ì„¤ì •
PYTHON_CMD="python"
if [ "$VENV_ACTIVATED" = false ]; then
    PYTHON_CMD="python3"
fi

# í”„ë¡œì íŠ¸ ì„¤ì¹˜ í™•ì¸
if ! $PYTHON_CMD -c "import study_preprocessor" 2>/dev/null; then
    echo "ğŸ”§ study_preprocessor íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
    .venv/bin/pip install -e . || {
        echo "âŒ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤íŒ¨"
        exit 1
    }
    echo "âœ… íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì™„ë£Œ"
fi

# ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p "$RESULT_DIR"

echo "ğŸš€ ëª¨ë¸ ê²€ì¦ ì‹œì‘"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "ğŸ“‚ ëª¨ë¸ ë””ë ‰í† ë¦¬: $MODEL_DIR"
if [ -n "$VALIDATION_LOG" ] && [ -f "$VALIDATION_LOG" ]; then
    echo "ğŸ¯ ê²€ì¦ ë¡œê·¸: $VALIDATION_LOG"
else
    echo "ğŸ¯ ê²€ì¦ ë¡œê·¸: í•©ì„± ë°ì´í„° ì‚¬ìš©"
fi
echo "ğŸ“ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬: $RESULT_DIR"
echo "ğŸ Python ì‹¤í–‰: $PYTHON_CMD"
echo ""
echo "ğŸ”„ ìˆ˜í–‰í•  ê²€ì¦ ë‹¨ê³„:"
echo "  1ï¸âƒ£  ëª¨ë¸ íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬"
echo "  2ï¸âƒ£  ëª¨ë¸ ë¡œë”© ë° ì‹¤í–‰ ê°€ëŠ¥ì„± ê²€ì¦"
echo "  3ï¸âƒ£  ê²€ì¦ ë°ì´í„° ì¤€ë¹„"
echo "  4ï¸âƒ£  ì¶”ë¡  ì„±ëŠ¥ ë° ì†ë„ ì¸¡ì •"
echo "  5ï¸âƒ£  ì´ìƒíƒì§€ ì •í™•ë„ í‰ê°€"
echo "  6ï¸âƒ£  ëª¨ë¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"
echo "  7ï¸âƒ£  ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"
echo "  8ï¸âƒ£  ì¢…í•© ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±"
echo ""
echo "â±ï¸  ì˜ˆìƒ ì†Œìš” ì‹œê°„: 5-15ë¶„ (ëª¨ë¸ í¬ê¸°ì— ë”°ë¼)"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo ""

# ì‹œì‘ ì‹œê°„ ê¸°ë¡
START_TIME=$(date +%s)

# 1ë‹¨ê³„: ëª¨ë¸ íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬
echo "1ï¸âƒ£  ëª¨ë¸ íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬ ì¤‘..."

$PYTHON_CMD -c "
import os
import json
import hashlib
from pathlib import Path

def calculate_file_hash(filepath):
    \"\"\"íŒŒì¼ì˜ SHA256 í•´ì‹œ ê³„ì‚°\"\"\"
    hash_sha256 = hashlib.sha256()
    try:
        with open(filepath, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b''):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()
    except Exception as e:
        return f'ERROR: {str(e)}'

def check_file_integrity(model_dir):
    \"\"\"ëª¨ë¸ íŒŒì¼ë“¤ì˜ ë¬´ê²°ì„± ê²€ì‚¬\"\"\"
    model_files = {
        'deeplog.pth': {'required': False, 'type': 'model'},
        'mscred.pth': {'required': False, 'type': 'model'},
        'vocab.json': {'required': True, 'type': 'config'},
        'baseline_stats.json': {'required': False, 'type': 'stats'},
        'drain3_state.json': {'required': True, 'type': 'state'},
        'metadata.json': {'required': False, 'type': 'metadata'}
    }
    
    integrity_report = {
        'model_directory': model_dir,
        'files': {},
        'summary': {
            'total_files': 0,
            'existing_files': 0,
            'required_missing': 0,
            'corrupted_files': 0,
            'total_size': 0
        }
    }
    
    for filename, info in model_files.items():
        filepath = os.path.join(model_dir, filename)
        file_info = {
            'exists': os.path.exists(filepath),
            'required': info['required'],
            'type': info['type'],
            'size': 0,
            'hash': None,
            'readable': False,
            'valid_format': False
        }
        
        if file_info['exists']:
            try:
                file_info['size'] = os.path.getsize(filepath)
                file_info['hash'] = calculate_file_hash(filepath)
                file_info['readable'] = True
                integrity_report['summary']['total_size'] += file_info['size']
                
                # íŒŒì¼ í˜•ì‹ ê²€ì¦
                if filename.endswith('.json'):
                    try:
                        with open(filepath, 'r') as f:
                            json.load(f)
                        file_info['valid_format'] = True
                    except:
                        file_info['valid_format'] = False
                elif filename.endswith('.pth'):
                    # PyTorch ëª¨ë¸ íŒŒì¼ ê¸°ë³¸ ê²€ì¦
                    file_info['valid_format'] = file_info['size'] > 1024  # ìµœì†Œ í¬ê¸° ì²´í¬
                else:
                    file_info['valid_format'] = True
                    
            except Exception as e:
                file_info['error'] = str(e)
                integrity_report['summary']['corrupted_files'] += 1
        else:
            if info['required']:
                integrity_report['summary']['required_missing'] += 1
        
        integrity_report['files'][filename] = file_info
        integrity_report['summary']['total_files'] += 1
        if file_info['exists']:
            integrity_report['summary']['existing_files'] += 1
    
    return integrity_report

# ë¬´ê²°ì„± ê²€ì‚¬ ì‹¤í–‰
report = check_file_integrity('$MODEL_DIR')

# ê²°ê³¼ ì €ì¥
with open('$RESULT_DIR/integrity_report.json', 'w') as f:
    json.dump(report, f, indent=2, ensure_ascii=False)

# ìš”ì•½ ì¶œë ¥
print('âœ… ëª¨ë¸ íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬ ì™„ë£Œ')
print(f'   ğŸ“Š ì´ íŒŒì¼: {report[\"summary\"][\"total_files\"]}ê°œ')
print(f'   âœ… ì¡´ì¬í•˜ëŠ” íŒŒì¼: {report[\"summary\"][\"existing_files\"]}ê°œ')
print(f'   âŒ í•„ìˆ˜ íŒŒì¼ ëˆ„ë½: {report[\"summary\"][\"required_missing\"]}ê°œ')
print(f'   ğŸš¨ ì†ìƒëœ íŒŒì¼: {report[\"summary\"][\"corrupted_files\"]}ê°œ')
print(f'   ğŸ’¾ ì´ í¬ê¸°: {report[\"summary\"][\"total_size\"]:,} bytes')

# ê°œë³„ íŒŒì¼ ìƒíƒœ
for filename, info in report['files'].items():
    status = 'âœ…' if info['exists'] and info['readable'] and info['valid_format'] else 'âŒ'
    required = '(í•„ìˆ˜)' if info['required'] else ''
    size = f'({info[\"size\"]:,} bytes)' if info['exists'] else ''
    print(f'   {status} {filename} {required} {size}')
"
echo ""

# 2ë‹¨ê³„: ëª¨ë¸ ë¡œë”© ë° ì‹¤í–‰ ê°€ëŠ¥ì„± ê²€ì¦
echo "2ï¸âƒ£  ëª¨ë¸ ë¡œë”© ë° ì‹¤í–‰ ê°€ëŠ¥ì„± ê²€ì¦ ì¤‘..."

$PYTHON_CMD -c "
import os
import json
import torch
import traceback
from pathlib import Path

def test_model_loading(model_dir):
    \"\"\"ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸\"\"\"
    loading_report = {
        'deeplog': {'loadable': False, 'error': None, 'model_info': {}},
        'mscred': {'loadable': False, 'error': None, 'model_info': {}},
        'vocab': {'loadable': False, 'error': None, 'vocab_size': 0},
        'baseline_stats': {'loadable': False, 'error': None, 'stats_count': 0},
        'drain3_state': {'loadable': False, 'error': None, 'template_count': 0}
    }
    
    # DeepLog ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    deeplog_path = os.path.join(model_dir, 'deeplog.pth')
    if os.path.exists(deeplog_path):
        try:
            model_state = torch.load(deeplog_path, map_location='cpu')
            loading_report['deeplog']['loadable'] = True
            loading_report['deeplog']['model_info'] = {
                'state_dict_keys': len(model_state.keys()) if isinstance(model_state, dict) else 'Unknown',
                'file_size': os.path.getsize(deeplog_path)
            }
        except Exception as e:
            loading_report['deeplog']['error'] = str(e)
    
    # MS-CRED ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸
    mscred_path = os.path.join(model_dir, 'mscred.pth')
    if os.path.exists(mscred_path):
        try:
            model_state = torch.load(mscred_path, map_location='cpu')
            loading_report['mscred']['loadable'] = True
            loading_report['mscred']['model_info'] = {
                'state_dict_keys': len(model_state.keys()) if isinstance(model_state, dict) else 'Unknown',
                'file_size': os.path.getsize(mscred_path)
            }
        except Exception as e:
            loading_report['mscred']['error'] = str(e)
    
    # Vocab íŒŒì¼ ë¡œë”© í…ŒìŠ¤íŠ¸
    vocab_path = os.path.join(model_dir, 'vocab.json')
    if os.path.exists(vocab_path):
        try:
            with open(vocab_path, 'r') as f:
                vocab_data = json.load(f)
            loading_report['vocab']['loadable'] = True
            loading_report['vocab']['vocab_size'] = len(vocab_data) if isinstance(vocab_data, dict) else 0
        except Exception as e:
            loading_report['vocab']['error'] = str(e)
    
    # ë² ì´ìŠ¤ë¼ì¸ í†µê³„ ë¡œë”© í…ŒìŠ¤íŠ¸
    baseline_path = os.path.join(model_dir, 'baseline_stats.json')
    if os.path.exists(baseline_path):
        try:
            with open(baseline_path, 'r') as f:
                stats_data = json.load(f)
            loading_report['baseline_stats']['loadable'] = True
            loading_report['baseline_stats']['stats_count'] = len(stats_data) if isinstance(stats_data, dict) else 0
        except Exception as e:
            loading_report['baseline_stats']['error'] = str(e)
    
    # Drain3 ìƒíƒœ ë¡œë”© í…ŒìŠ¤íŠ¸
    drain3_path = os.path.join(model_dir, 'drain3_state.json')
    if os.path.exists(drain3_path):
        try:
            with open(drain3_path, 'r') as f:
                drain3_data = json.load(f)
            loading_report['drain3_state']['loadable'] = True
            # Drain3 ìƒíƒœì—ì„œ í…œí”Œë¦¿ ê°œìˆ˜ ì¶”ì¶œ ì‹œë„
            template_count = 0
            if isinstance(drain3_data, dict) and 'clusters' in drain3_data:
                template_count = len(drain3_data['clusters'])
            loading_report['drain3_state']['template_count'] = template_count
        except Exception as e:
            loading_report['drain3_state']['error'] = str(e)
    
    return loading_report

# ëª¨ë¸ ë¡œë”© í…ŒìŠ¤íŠ¸ ì‹¤í–‰
loading_report = test_model_loading('$MODEL_DIR')

# ê²°ê³¼ ì €ì¥
with open('$RESULT_DIR/loading_report.json', 'w') as f:
    json.dump(loading_report, f, indent=2, ensure_ascii=False)

# ìš”ì•½ ì¶œë ¥
print('âœ… ëª¨ë¸ ë¡œë”© ê²€ì¦ ì™„ë£Œ')

for component, info in loading_report.items():
    if os.path.exists(os.path.join('$MODEL_DIR', f'{component}.pth')) or os.path.exists(os.path.join('$MODEL_DIR', f'{component}.json')):
        status = 'âœ…' if info['loadable'] else 'âŒ'
        error_msg = f' (ì˜¤ë¥˜: {info[\"error\"]})' if info['error'] else ''
        
        if component == 'deeplog' and info['loadable']:
            print(f'   {status} DeepLog ëª¨ë¸: ë¡œë”© ê°€ëŠ¥{error_msg}')
        elif component == 'mscred' and info['loadable']:
            print(f'   {status} MS-CRED ëª¨ë¸: ë¡œë”© ê°€ëŠ¥{error_msg}')
        elif component == 'vocab' and info['loadable']:
            print(f'   {status} ì–´íœ˜ ì‚¬ì „: {info[\"vocab_size\"]}ê°œ í…œí”Œë¦¿{error_msg}')
        elif component == 'baseline_stats' and info['loadable']:
            print(f'   {status} ë² ì´ìŠ¤ë¼ì¸ í†µê³„: ë¡œë”© ê°€ëŠ¥{error_msg}')
        elif component == 'drain3_state' and info['loadable']:
            print(f'   {status} Drain3 ìƒíƒœ: {info[\"template_count\"]}ê°œ í´ëŸ¬ìŠ¤í„°{error_msg}')
        elif not info['loadable']:
            print(f'   {status} {component}: ë¡œë”© ì‹¤íŒ¨{error_msg}')
"
echo ""

# 3ë‹¨ê³„: ê²€ì¦ ë°ì´í„° ì¤€ë¹„
echo "3ï¸âƒ£  ê²€ì¦ ë°ì´í„° ì¤€ë¹„ ì¤‘..."

if [ -n "$VALIDATION_LOG" ] && [ -f "$VALIDATION_LOG" ]; then
    echo "   ì‚¬ìš©ì ì œê³µ ê²€ì¦ ë¡œê·¸ ì‚¬ìš©: $VALIDATION_LOG"
    TEST_DATA="$VALIDATION_LOG"
else
    echo "   ê²€ì¦ìš© í•©ì„± ë°ì´í„° ìƒì„± ì¤‘..."
    # ê²€ì¦ìš© í•©ì„± ë°ì´í„° ìƒì„± (ì •ìƒ/ì´ìƒ ë¼ë²¨ë§ëœ ë°ì´í„°)
    $PYTHON_CMD -c "
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import random
import json

# ê²€ì¦ìš© í•©ì„± ë¡œê·¸ ë°ì´í„° ìƒì„± (ë¼ë²¨ë§ëœ ë°ì´í„°)
np.random.seed(42)
random.seed(42)

# ì •ìƒ ë¡œê·¸ í…œí”Œë¦¿ë“¤
normal_templates = [
    'INFO: User <*> logged in successfully',
    'INFO: Processing request <*> completed',
    'INFO: Database query executed in <*>ms',
    'INFO: Cache hit for key <*>',
    'INFO: Service <*> started successfully',
    'DEBUG: Memory usage: <*>%',
    'DEBUG: CPU usage: <*>%'
]

# ì´ìƒ ë¡œê·¸ í…œí”Œë¦¿ë“¤
anomaly_templates = [
    'ERROR: Authentication failed for user <*>',
    'CRITICAL: Database connection timeout',
    'ERROR: Out of memory exception',
    'FATAL: Service <*> crashed unexpectedly',
    'ERROR: Security breach attempt detected',
    'CRITICAL: Disk space full',
    'ERROR: Network connection lost'
]

# ê²€ì¦ ë°ì´í„° ìƒì„±
validation_logs = []
labels = []
start_time = datetime.now() - timedelta(hours=2)

# ì •ìƒ ë¡œê·¸ (70%)
for i in range(1400):
    timestamp = start_time + timedelta(seconds=i*5)
    template = random.choice(normal_templates)
    log_line = f'{timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")} {template}'
    validation_logs.append(log_line)
    labels.append(0)  # ì •ìƒ = 0

# ì´ìƒ ë¡œê·¸ (30%)
for i in range(600):
    timestamp = start_time + timedelta(seconds=random.randint(0, 7000))
    template = random.choice(anomaly_templates)
    log_line = f'{timestamp.strftime(\"%Y-%m-%d %H:%M:%S\")} {template}'
    validation_logs.append(log_line)
    labels.append(1)  # ì´ìƒ = 1

# ì‹œê°„ìˆœ ì •ë ¬
combined = list(zip(validation_logs, labels))
combined.sort(key=lambda x: x[0])
validation_logs, labels = zip(*combined)

# ê²€ì¦ ë¡œê·¸ íŒŒì¼ ì €ì¥
with open('$RESULT_DIR/validation_data.log', 'w') as f:
    for log in validation_logs:
        f.write(log + '\\n')

# ë¼ë²¨ íŒŒì¼ ì €ì¥
label_data = {
    'labels': list(labels),
    'total_logs': len(labels),
    'normal_count': labels.count(0),
    'anomaly_count': labels.count(1),
    'anomaly_rate': labels.count(1) / len(labels)
}

with open('$RESULT_DIR/validation_labels.json', 'w') as f:
    json.dump(label_data, f, indent=2)

print(f'âœ… ê²€ì¦ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(validation_logs):,} ë¼ì¸')
print(f'   - ì •ìƒ ë¡œê·¸: {labels.count(0):,}ê°œ ({100*labels.count(0)/len(labels):.1f}%)')
print(f'   - ì´ìƒ ë¡œê·¸: {labels.count(1):,}ê°œ ({100*labels.count(1)/len(labels):.1f}%)')
"
    TEST_DATA="$RESULT_DIR/validation_data.log"
fi
echo ""

# 4ë‹¨ê³„: ì¶”ë¡  ì„±ëŠ¥ ë° ì†ë„ ì¸¡ì •
echo "4ï¸âƒ£  ì¶”ë¡  ì„±ëŠ¥ ë° ì†ë„ ì¸¡ì • ì¤‘..."

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§ì„ ìœ„í•œ í•¨ìˆ˜
monitor_memory() {
    local pid=$1
    local output_file=$2
    while kill -0 $pid 2>/dev/null; do
        ps -p $pid -o pid,vsz,rss,pcpu --no-headers >> "$output_file" 2>/dev/null || break
        sleep 1
    done
}

# ì¶”ë¡  ì‹¤í–‰ ë° ì„±ëŠ¥ ì¸¡ì •
INFERENCE_RESULT="$RESULT_DIR/inference_performance"
MEMORY_LOG="$RESULT_DIR/memory_usage.log"

echo "   ì¶”ë¡  ì‹¤í–‰ ì¤‘ (ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í¬í•¨)..."
start_time=$(date +%s)

# ë°±ê·¸ë¼ìš´ë“œì—ì„œ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
echo "PID VSZ RSS CPU" > "$MEMORY_LOG"

# ì¶”ë¡  ì‹¤í–‰
./run_inference.sh "$MODEL_DIR" "$TEST_DATA" "$INFERENCE_RESULT" > "$RESULT_DIR/inference.log" 2>&1 &
INFERENCE_PID=$!

# ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì‹œì‘
monitor_memory $INFERENCE_PID "$MEMORY_LOG" &
MONITOR_PID=$!

# ì¶”ë¡  ì™„ë£Œ ëŒ€ê¸°
wait $INFERENCE_PID
INFERENCE_EXIT_CODE=$?

# ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì¢…ë£Œ
kill $MONITOR_PID 2>/dev/null || true

end_time=$(date +%s)
inference_duration=$((end_time - start_time))

echo "âœ… ì¶”ë¡  ì„±ëŠ¥ ì¸¡ì • ì™„ë£Œ"
echo "   â±ï¸  ì¶”ë¡  ì‹œê°„: ${inference_duration}ì´ˆ"
echo "   ğŸ“Š ì¢…ë£Œ ì½”ë“œ: $INFERENCE_EXIT_CODE"

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
if [ -f "$MEMORY_LOG" ] && [ $(wc -l < "$MEMORY_LOG") -gt 1 ]; then
    $PYTHON_CMD -c "
import pandas as pd
import json

# ë©”ëª¨ë¦¬ ë¡œê·¸ ë¶„ì„
try:
    df = pd.read_csv('$MEMORY_LOG', sep='\s+', skiprows=1, names=['PID', 'VSZ', 'RSS', 'CPU'])
    if len(df) > 0:
        memory_stats = {
            'max_memory_mb': float(df['RSS'].max() / 1024),
            'avg_memory_mb': float(df['RSS'].mean() / 1024),
            'max_cpu_percent': float(df['CPU'].max()),
            'avg_cpu_percent': float(df['CPU'].mean()),
            'samples': len(df)
        }
        
        with open('$RESULT_DIR/memory_stats.json', 'w') as f:
            json.dump(memory_stats, f, indent=2)
        
        print(f'   ğŸ’¾ ìµœëŒ€ ë©”ëª¨ë¦¬: {memory_stats[\"max_memory_mb\"]:.1f} MB')
        print(f'   ğŸ’¾ í‰ê·  ë©”ëª¨ë¦¬: {memory_stats[\"avg_memory_mb\"]:.1f} MB')
        print(f'   ğŸ–¥ï¸  ìµœëŒ€ CPU: {memory_stats[\"max_cpu_percent\"]:.1f}%')
    else:
        print('   âš ï¸  ë©”ëª¨ë¦¬ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨')
except Exception as e:
    print(f'   âš ï¸  ë©”ëª¨ë¦¬ ë¶„ì„ ì˜¤ë¥˜: {e}')
"
fi
echo ""

# 5ë‹¨ê³„: ì´ìƒíƒì§€ ì •í™•ë„ í‰ê°€
echo "5ï¸âƒ£  ì´ìƒíƒì§€ ì •í™•ë„ í‰ê°€ ì¤‘..."

if [ -f "$RESULT_DIR/validation_labels.json" ]; then
    echo "   ë¼ë²¨ë§ëœ ë°ì´í„°ë¡œ ì •í™•ë„ ê³„ì‚° ì¤‘..."
    
    $PYTHON_CMD -c "
import pandas as pd
import json
import numpy as np
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
import os

def evaluate_model_accuracy(inference_dir, labels_file):
    \"\"\"ëª¨ë¸ë³„ ì •í™•ë„ í‰ê°€\"\"\"
    
    # ë¼ë²¨ ë°ì´í„° ë¡œë“œ
    with open(labels_file, 'r') as f:
        label_data = json.load(f)
    
    true_labels = np.array(label_data['labels'])
    
    accuracy_report = {
        'total_samples': len(true_labels),
        'true_anomaly_rate': float(np.mean(true_labels)),
        'models': {}
    }
    
    # ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ í‰ê°€
    baseline_file = os.path.join(inference_dir, 'baseline_scores_enhanced.parquet')
    if os.path.exists(baseline_file):
        try:
            df = pd.read_parquet(baseline_file)
            # ìœˆë„ìš° ë‹¨ìœ„ë¥¼ ë¡œê·¸ ë‹¨ìœ„ë¡œ ë³€í™˜ (ê°„ë‹¨í•œ ë§¤í•‘)
            window_size = 50
            predicted_labels = []
            
            for i in range(len(true_labels)):
                window_idx = i // window_size
                if window_idx < len(df):
                    # ê°•í•œ ì´ìƒì´ ìˆìœ¼ë©´ ì´ìƒìœ¼ë¡œ íŒì •
                    is_anomaly = df.iloc[window_idx].get('is_strong_anomaly', df.iloc[window_idx].get('is_anomaly', False))
                    predicted_labels.append(1 if is_anomaly else 0)
                else:
                    predicted_labels.append(0)
            
            predicted_labels = np.array(predicted_labels[:len(true_labels)])
            
            accuracy_report['models']['baseline'] = {
                'accuracy': float(accuracy_score(true_labels, predicted_labels)),
                'precision': float(precision_score(true_labels, predicted_labels, zero_division=0)),
                'recall': float(recall_score(true_labels, predicted_labels, zero_division=0)),
                'f1_score': float(f1_score(true_labels, predicted_labels, zero_division=0)),
                'predicted_anomaly_rate': float(np.mean(predicted_labels))
            }
        except Exception as e:
            accuracy_report['models']['baseline'] = {'error': str(e)}
    
    # DeepLog ëª¨ë¸ í‰ê°€
    deeplog_file = os.path.join(inference_dir, 'deeplog_infer.parquet')
    if os.path.exists(deeplog_file):
        try:
            df = pd.read_parquet(deeplog_file)
            # ì‹œí€€ìŠ¤ ë‹¨ìœ„ë¥¼ ë¡œê·¸ ë‹¨ìœ„ë¡œ ë³€í™˜
            seq_len = 50
            predicted_labels = []
            
            for i in range(len(true_labels)):
                seq_idx = max(0, i - seq_len + 1)
                if seq_idx < len(df):
                    # Top-Kì— ì—†ìœ¼ë©´ ì´ìƒìœ¼ë¡œ íŒì •
                    is_anomaly = not df.iloc[seq_idx].get('in_topk', True)
                    predicted_labels.append(1 if is_anomaly else 0)
                else:
                    predicted_labels.append(0)
            
            predicted_labels = np.array(predicted_labels[:len(true_labels)])
            
            accuracy_report['models']['deeplog'] = {
                'accuracy': float(accuracy_score(true_labels, predicted_labels)),
                'precision': float(precision_score(true_labels, predicted_labels, zero_division=0)),
                'recall': float(recall_score(true_labels, predicted_labels, zero_division=0)),
                'f1_score': float(f1_score(true_labels, predicted_labels, zero_division=0)),
                'predicted_anomaly_rate': float(np.mean(predicted_labels))
            }
        except Exception as e:
            accuracy_report['models']['deeplog'] = {'error': str(e)}
    
    # MS-CRED ëª¨ë¸ í‰ê°€
    mscred_file = os.path.join(inference_dir, 'mscred_infer.parquet')
    if os.path.exists(mscred_file):
        try:
            df = pd.read_parquet(mscred_file)
            # ìœˆë„ìš° ë‹¨ìœ„ë¥¼ ë¡œê·¸ ë‹¨ìœ„ë¡œ ë³€í™˜
            window_size = 50
            predicted_labels = []
            
            for i in range(len(true_labels)):
                window_idx = i // window_size
                if window_idx < len(df):
                    is_anomaly = df.iloc[window_idx].get('is_anomaly', False)
                    predicted_labels.append(1 if is_anomaly else 0)
                else:
                    predicted_labels.append(0)
            
            predicted_labels = np.array(predicted_labels[:len(true_labels)])
            
            accuracy_report['models']['mscred'] = {
                'accuracy': float(accuracy_score(true_labels, predicted_labels)),
                'precision': float(precision_score(true_labels, predicted_labels, zero_division=0)),
                'recall': float(recall_score(true_labels, predicted_labels, zero_division=0)),
                'f1_score': float(f1_score(true_labels, predicted_labels, zero_division=0)),
                'predicted_anomaly_rate': float(np.mean(predicted_labels))
            }
        except Exception as e:
            accuracy_report['models']['mscred'] = {'error': str(e)}
    
    return accuracy_report

# ì •í™•ë„ í‰ê°€ ì‹¤í–‰
if os.path.exists('$INFERENCE_RESULT'):
    accuracy_report = evaluate_model_accuracy('$INFERENCE_RESULT', '$RESULT_DIR/validation_labels.json')
    
    # ê²°ê³¼ ì €ì¥
    with open('$RESULT_DIR/accuracy_report.json', 'w') as f:
        json.dump(accuracy_report, f, indent=2, ensure_ascii=False)
    
    print('âœ… ì´ìƒíƒì§€ ì •í™•ë„ í‰ê°€ ì™„ë£Œ')
    print(f'   ğŸ“Š ì´ ìƒ˜í”Œ: {accuracy_report[\"total_samples\"]:,}ê°œ')
    print(f'   ğŸ¯ ì‹¤ì œ ì´ìƒìœ¨: {accuracy_report[\"true_anomaly_rate\"]:.1%}')
    
    for model_name, metrics in accuracy_report['models'].items():
        if 'error' not in metrics:
            print(f'   ğŸ¤– {model_name.upper()}:')
            print(f'      ì •í™•ë„: {metrics[\"accuracy\"]:.3f}')
            print(f'      ì •ë°€ë„: {metrics[\"precision\"]:.3f}')
            print(f'      ì¬í˜„ìœ¨: {metrics[\"recall\"]:.3f}')
            print(f'      F1ì ìˆ˜: {metrics[\"f1_score\"]:.3f}')
        else:
            print(f'   âŒ {model_name.upper()}: í‰ê°€ ì‹¤íŒ¨ ({metrics[\"error\"]})')
else:
    print('âš ï¸  ì¶”ë¡  ê²°ê³¼ê°€ ì—†ì–´ ì •í™•ë„ í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.')
"
else
    echo "   ë¼ë²¨ ë°ì´í„°ê°€ ì—†ì–´ ì •í™•ë„ í‰ê°€ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤."
fi
echo ""

# 6ë‹¨ê³„: ëª¨ë¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸
echo "6ï¸âƒ£  ëª¨ë¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì¤‘..."

$PYTHON_CMD -c "
import os
import json
import time
import traceback
from pathlib import Path

def stability_test(model_dir, test_runs=5):
    \"\"\"ëª¨ë¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸\"\"\"
    
    stability_report = {
        'test_runs': test_runs,
        'models': {
            'deeplog': {'success_rate': 0, 'avg_load_time': 0, 'errors': []},
            'mscred': {'success_rate': 0, 'avg_load_time': 0, 'errors': []},
            'vocab': {'success_rate': 0, 'avg_load_time': 0, 'errors': []},
            'baseline_stats': {'success_rate': 0, 'avg_load_time': 0, 'errors': []},
            'drain3_state': {'success_rate': 0, 'avg_load_time': 0, 'errors': []}
        }
    }
    
    # ê° ëª¨ë¸ì— ëŒ€í•´ ë°˜ë³µ ë¡œë”© í…ŒìŠ¤íŠ¸
    for model_name in stability_report['models'].keys():
        success_count = 0
        load_times = []
        
        for run in range(test_runs):
            try:
                start_time = time.time()
                
                if model_name == 'deeplog':
                    model_path = os.path.join(model_dir, 'deeplog.pth')
                    if os.path.exists(model_path):
                        import torch
                        torch.load(model_path, map_location='cpu')
                    else:
                        continue
                        
                elif model_name == 'mscred':
                    model_path = os.path.join(model_dir, 'mscred.pth')
                    if os.path.exists(model_path):
                        import torch
                        torch.load(model_path, map_location='cpu')
                    else:
                        continue
                        
                elif model_name in ['vocab', 'baseline_stats', 'drain3_state']:
                    file_path = os.path.join(model_dir, f'{model_name}.json')
                    if os.path.exists(file_path):
                        with open(file_path, 'r') as f:
                            json.load(f)
                    else:
                        continue
                
                load_time = time.time() - start_time
                load_times.append(load_time)
                success_count += 1
                
            except Exception as e:
                stability_report['models'][model_name]['errors'].append(f'Run {run+1}: {str(e)}')
        
        if success_count > 0:
            stability_report['models'][model_name]['success_rate'] = success_count / test_runs
            stability_report['models'][model_name]['avg_load_time'] = sum(load_times) / len(load_times)
    
    return stability_report

# ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì‹¤í–‰
stability_report = stability_test('$MODEL_DIR', test_runs=3)

# ê²°ê³¼ ì €ì¥
with open('$RESULT_DIR/stability_report.json', 'w') as f:
    json.dump(stability_report, f, indent=2, ensure_ascii=False)

print('âœ… ëª¨ë¸ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ì™„ë£Œ')

for model_name, metrics in stability_report['models'].items():
    if metrics['success_rate'] > 0:
        print(f'   ğŸ¤– {model_name.upper()}:')
        print(f'      ì„±ê³µë¥ : {metrics[\"success_rate\"]:.1%}')
        print(f'      í‰ê·  ë¡œë”©ì‹œê°„: {metrics[\"avg_load_time\"]:.3f}ì´ˆ')
        if metrics['errors']:
            print(f'      ì˜¤ë¥˜ ìˆ˜: {len(metrics[\"errors\"])}ê°œ')
"
echo ""

# 7ë‹¨ê³„: ì¢…í•© ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±
echo "7ï¸âƒ£  ì¢…í•© ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘..."

$PYTHON_CMD -c "
import json
import os
from datetime import datetime

# ëª¨ë“  ê²€ì¦ ê²°ê³¼ ë¡œë“œ
reports = {}

report_files = {
    'integrity': '$RESULT_DIR/integrity_report.json',
    'loading': '$RESULT_DIR/loading_report.json',
    'accuracy': '$RESULT_DIR/accuracy_report.json',
    'stability': '$RESULT_DIR/stability_report.json',
    'memory': '$RESULT_DIR/memory_stats.json'
}

for report_name, file_path in report_files.items():
    if os.path.exists(file_path):
        with open(file_path, 'r') as f:
            reports[report_name] = json.load(f)

# ì¢…í•© ì ìˆ˜ ê³„ì‚°
def calculate_quality_score(reports):
    \"\"\"ëª¨ë¸ í’ˆì§ˆ ì¢…í•© ì ìˆ˜ ê³„ì‚° (0-100ì )\"\"\"
    
    score = 0
    max_score = 100
    
    # íŒŒì¼ ë¬´ê²°ì„± ì ìˆ˜ (20ì )
    if 'integrity' in reports:
        integrity = reports['integrity']
        file_score = 0
        if integrity['summary']['required_missing'] == 0:
            file_score += 10  # í•„ìˆ˜ íŒŒì¼ ëª¨ë‘ ì¡´ì¬
        if integrity['summary']['corrupted_files'] == 0:
            file_score += 10  # ì†ìƒëœ íŒŒì¼ ì—†ìŒ
        score += file_score
    
    # ëª¨ë¸ ë¡œë”© ì ìˆ˜ (20ì )
    if 'loading' in reports:
        loading = reports['loading']
        loading_score = 0
        loadable_models = sum(1 for model in loading.values() if model.get('loadable', False))
        total_models = len([k for k in loading.keys() if os.path.exists(os.path.join('$MODEL_DIR', f'{k}.pth')) or os.path.exists(os.path.join('$MODEL_DIR', f'{k}.json'))])
        if total_models > 0:
            loading_score = (loadable_models / total_models) * 20
        score += loading_score
    
    # ì •í™•ë„ ì ìˆ˜ (30ì )
    if 'accuracy' in reports:
        accuracy = reports['accuracy']
        accuracy_score = 0
        model_count = 0
        total_f1 = 0
        
        for model_name, metrics in accuracy.get('models', {}).items():
            if 'f1_score' in metrics:
                total_f1 += metrics['f1_score']
                model_count += 1
        
        if model_count > 0:
            avg_f1 = total_f1 / model_count
            accuracy_score = avg_f1 * 30  # F1 ì ìˆ˜ ê¸°ë°˜
        score += accuracy_score
    
    # ì•ˆì •ì„± ì ìˆ˜ (20ì )
    if 'stability' in reports:
        stability = reports['stability']
        stability_score = 0
        model_count = 0
        total_success_rate = 0
        
        for model_name, metrics in stability.get('models', {}).items():
            if metrics.get('success_rate', 0) > 0:
                total_success_rate += metrics['success_rate']
                model_count += 1
        
        if model_count > 0:
            avg_success_rate = total_success_rate / model_count
            stability_score = avg_success_rate * 20
        score += stability_score
    
    # ì„±ëŠ¥ ì ìˆ˜ (10ì )
    performance_score = 10  # ê¸°ë³¸ ì ìˆ˜ (ì¶”ë¡ ì´ ì™„ë£Œë˜ë©´)
    if 'memory' in reports:
        memory = reports['memory']
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì´ ì ì ˆí•˜ë©´ ì¶”ê°€ ì ìˆ˜
        if memory.get('max_memory_mb', 0) < 1000:  # 1GB ë¯¸ë§Œ
            performance_score = 10
        elif memory.get('max_memory_mb', 0) < 2000:  # 2GB ë¯¸ë§Œ
            performance_score = 8
        else:
            performance_score = 5
    score += performance_score
    
    return min(score, max_score)

# ì¢…í•© ì ìˆ˜ ê³„ì‚°
quality_score = calculate_quality_score(reports)

# ë¦¬í¬íŠ¸ ìƒì„±
report_lines = []
report_lines.append('# ğŸ” ëª¨ë¸ ê²€ì¦ ë¦¬í¬íŠ¸')
report_lines.append('')
report_lines.append(f'**ìƒì„± ì‹œê°„**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}')
report_lines.append(f'**ëª¨ë¸ ë””ë ‰í† ë¦¬**: $MODEL_DIR')
report_lines.append(f'**ê²€ì¦ ë°ì´í„°**: $TEST_DATA')
report_lines.append('')

# ì¢…í•© ì ìˆ˜
report_lines.append(f'## ğŸ¯ ì¢…í•© í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f}/100')
report_lines.append('')

if quality_score >= 90:
    report_lines.append('âœ… **ìš°ìˆ˜**: ëª¨ë¸ì´ ë§¤ìš° ì•ˆì •ì ì´ê³  ì •í™•í•©ë‹ˆë‹¤.')
elif quality_score >= 70:
    report_lines.append('ğŸŸ¡ **ì–‘í˜¸**: ëª¨ë¸ì´ ëŒ€ì²´ë¡œ ì•ˆì •ì ì…ë‹ˆë‹¤.')
elif quality_score >= 50:
    report_lines.append('ğŸŸ  **ë³´í†µ**: ì¼ë¶€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.')
else:
    report_lines.append('ğŸ”´ **ë¶ˆëŸ‰**: ëª¨ë¸ì— ì‹¬ê°í•œ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.')

report_lines.append('')

# íŒŒì¼ ë¬´ê²°ì„± ê²°ê³¼
if 'integrity' in reports:
    integrity = reports['integrity']
    report_lines.append('## ğŸ“ íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬')
    report_lines.append('')
    report_lines.append(f'- **ì´ íŒŒì¼**: {integrity[\"summary\"][\"total_files\"]}ê°œ')
    report_lines.append(f'- **ì¡´ì¬í•˜ëŠ” íŒŒì¼**: {integrity[\"summary\"][\"existing_files\"]}ê°œ')
    report_lines.append(f'- **í•„ìˆ˜ íŒŒì¼ ëˆ„ë½**: {integrity[\"summary\"][\"required_missing\"]}ê°œ')
    report_lines.append(f'- **ì†ìƒëœ íŒŒì¼**: {integrity[\"summary\"][\"corrupted_files\"]}ê°œ')
    report_lines.append(f'- **ì´ í¬ê¸°**: {integrity[\"summary\"][\"total_size\"]:,} bytes')
    report_lines.append('')

# ëª¨ë¸ ë¡œë”© ê²°ê³¼
if 'loading' in reports:
    loading = reports['loading']
    report_lines.append('## ğŸ”§ ëª¨ë¸ ë¡œë”© ê²€ì¦')
    report_lines.append('')
    
    for model_name, info in loading.items():
        if os.path.exists(os.path.join('$MODEL_DIR', f'{model_name}.pth')) or os.path.exists(os.path.join('$MODEL_DIR', f'{model_name}.json')):
            status = 'âœ…' if info.get('loadable', False) else 'âŒ'
            model_display = model_name.upper().replace('_', ' ')
            
            if info.get('loadable', False):
                report_lines.append(f'- {status} **{model_display}**: ë¡œë”© ì„±ê³µ')
            else:
                error_msg = info.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜')
                report_lines.append(f'- {status} **{model_display}**: ë¡œë”© ì‹¤íŒ¨ ({error_msg})')
    
    report_lines.append('')

# ì •í™•ë„ í‰ê°€ ê²°ê³¼
if 'accuracy' in reports:
    accuracy = reports['accuracy']
    report_lines.append('## ğŸ¯ ì´ìƒíƒì§€ ì •í™•ë„')
    report_lines.append('')
    report_lines.append(f'- **ì´ ìƒ˜í”Œ**: {accuracy[\"total_samples\"]:,}ê°œ')
    report_lines.append(f'- **ì‹¤ì œ ì´ìƒìœ¨**: {accuracy[\"true_anomaly_rate\"]:.1%}')
    report_lines.append('')
    
    report_lines.append('| ëª¨ë¸ | ì •í™•ë„ | ì •ë°€ë„ | ì¬í˜„ìœ¨ | F1ì ìˆ˜ |')
    report_lines.append('|------|--------|--------|--------|--------|')
    
    for model_name, metrics in accuracy.get('models', {}).items():
        if 'error' not in metrics:
            model_display = model_name.upper()
            acc = f'{metrics[\"accuracy\"]:.3f}'
            prec = f'{metrics[\"precision\"]:.3f}'
            rec = f'{metrics[\"recall\"]:.3f}'
            f1 = f'{metrics[\"f1_score\"]:.3f}'
            report_lines.append(f'| {model_display} | {acc} | {prec} | {rec} | {f1} |')
        else:
            model_display = model_name.upper()
            report_lines.append(f'| {model_display} | ì˜¤ë¥˜ | ì˜¤ë¥˜ | ì˜¤ë¥˜ | ì˜¤ë¥˜ |')
    
    report_lines.append('')

# ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼
if 'stability' in reports:
    stability = reports['stability']
    report_lines.append('## ğŸ›¡ï¸ ëª¨ë¸ ì•ˆì •ì„±')
    report_lines.append('')
    
    for model_name, metrics in stability.get('models', {}).items():
        if metrics.get('success_rate', 0) > 0:
            model_display = model_name.upper().replace('_', ' ')
            success_rate = f'{metrics[\"success_rate\"]:.1%}'
            load_time = f'{metrics[\"avg_load_time\"]:.3f}ì´ˆ'
            report_lines.append(f'- **{model_display}**: ì„±ê³µë¥  {success_rate}, í‰ê·  ë¡œë”©ì‹œê°„ {load_time}')
    
    report_lines.append('')

# ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê²°ê³¼
if 'memory' in reports:
    memory = reports['memory']
    report_lines.append('## ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰')
    report_lines.append('')
    report_lines.append(f'- **ìµœëŒ€ ë©”ëª¨ë¦¬**: {memory[\"max_memory_mb\"]:.1f} MB')
    report_lines.append(f'- **í‰ê·  ë©”ëª¨ë¦¬**: {memory[\"avg_memory_mb\"]:.1f} MB')
    report_lines.append(f'- **ìµœëŒ€ CPU**: {memory[\"max_cpu_percent\"]:.1f}%')
    report_lines.append('')

# ê¶Œì¥ì‚¬í•­
report_lines.append('## ğŸ’¡ ê¶Œì¥ì‚¬í•­')
report_lines.append('')

if quality_score >= 90:
    report_lines.append('- âœ… ëª¨ë¸ì´ ìš°ìˆ˜í•œ ìƒíƒœì…ë‹ˆë‹¤. í”„ë¡œë•ì…˜ í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.')
elif quality_score >= 70:
    report_lines.append('- ğŸŸ¡ ëª¨ë¸ì´ ì–‘í˜¸í•œ ìƒíƒœì…ë‹ˆë‹¤. ì¼ë¶€ ì„±ëŠ¥ ìµœì í™”ë¥¼ ê³ ë ¤í•´ë³´ì„¸ìš”.')
else:
    report_lines.append('- ğŸ”´ ëª¨ë¸ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒ ì‚¬í•­ì„ í™•ì¸í•˜ì„¸ìš”:')
    
    if 'integrity' in reports and reports['integrity']['summary']['required_missing'] > 0:
        report_lines.append('  - í•„ìˆ˜ ëª¨ë¸ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.')
    
    if 'loading' in reports:
        failed_models = [name for name, info in reports['loading'].items() 
                        if not info.get('loadable', False) and 
                        (os.path.exists(os.path.join('$MODEL_DIR', f'{name}.pth')) or 
                         os.path.exists(os.path.join('$MODEL_DIR', f'{name}.json')))]
        if failed_models:
            report_lines.append(f'  - ë‹¤ìŒ ëª¨ë¸ë“¤ì˜ ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {', '.join(failed_models)}')
    
    if 'accuracy' in reports:
        low_accuracy_models = [name for name, metrics in reports['accuracy'].get('models', {}).items() 
                              if 'f1_score' in metrics and metrics['f1_score'] < 0.5]
        if low_accuracy_models:
            report_lines.append(f'  - ë‹¤ìŒ ëª¨ë¸ë“¤ì˜ ì •í™•ë„ê°€ ë‚®ìŠµë‹ˆë‹¤: {', '.join(low_accuracy_models)}')

report_lines.append('')

# ìƒì„±ëœ íŒŒì¼ë“¤
report_lines.append('## ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤')
report_lines.append('')
for report_name, file_path in report_files.items():
    if os.path.exists(file_path):
        rel_path = os.path.relpath(file_path, '$RESULT_DIR')
        report_lines.append(f'- **{report_name}_report**: {rel_path}')

# ë¦¬í¬íŠ¸ ì €ì¥
with open('$RESULT_DIR/validation_report.md', 'w', encoding='utf-8') as f:
    f.write('\\n'.join(report_lines))

print('âœ… ì¢…í•© ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ')
print(f'   ğŸ¯ ì¢…í•© í’ˆì§ˆ ì ìˆ˜: {quality_score:.1f}/100')
"

# ì¢…ë£Œ ì‹œê°„ ë° ì†Œìš” ì‹œê°„ ê³„ì‚°
END_TIME=$(date +%s)
DURATION=$((END_TIME - START_TIME))
MINUTES=$((DURATION / 60))
SECONDS=$((DURATION % 60))

echo ""
echo "ğŸ‰ ëª¨ë¸ ê²€ì¦ ì™„ë£Œ!"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
echo "â±ï¸  ì´ ì†Œìš” ì‹œê°„: ${MINUTES}ë¶„ ${SECONDS}ì´ˆ"
echo ""

# ê²°ê³¼ ìš”ì•½ ì¶œë ¥
echo "ğŸ“Š ê²€ì¦ ê²°ê³¼ ìš”ì•½:"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# ì¢…í•© ì ìˆ˜ ì¶œë ¥
if [ -f "$RESULT_DIR/validation_report.md" ]; then
    quality_score=$(grep "ì¢…í•© í’ˆì§ˆ ì ìˆ˜:" "$RESULT_DIR/validation_report.md" | grep -o '[0-9]*\.[0-9]*' | head -1)
    if [ -n "$quality_score" ]; then
        echo "  ğŸ¯ ì¢…í•© í’ˆì§ˆ ì ìˆ˜: ${quality_score}/100"
        
        # ì ìˆ˜ì— ë”°ë¥¸ ìƒíƒœ í‘œì‹œ
        if (( $(echo "$quality_score >= 90" | bc -l) )); then
            echo "  âœ… ìƒíƒœ: ìš°ìˆ˜ (í”„ë¡œë•ì…˜ ì‚¬ìš© ê°€ëŠ¥)"
        elif (( $(echo "$quality_score >= 70" | bc -l) )); then
            echo "  ğŸŸ¡ ìƒíƒœ: ì–‘í˜¸ (ì¼ë¶€ ìµœì í™” ê¶Œì¥)"
        elif (( $(echo "$quality_score >= 50" | bc -l) )); then
            echo "  ğŸŸ  ìƒíƒœ: ë³´í†µ (ê°œì„  í•„ìš”)"
        else
            echo "  ğŸ”´ ìƒíƒœ: ë¶ˆëŸ‰ (ì‹¬ê°í•œ ë¬¸ì œ)"
        fi
    fi
fi

echo "  ğŸ“ ê²€ì¦ ê²°ê³¼ ìœ„ì¹˜: $RESULT_DIR"
echo ""

# ì£¼ìš” ê²€ì¦ í•­ëª©ë³„ ê²°ê³¼
echo "ğŸ“‹ ì£¼ìš” ê²€ì¦ í•­ëª©:"
echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"

# íŒŒì¼ ë¬´ê²°ì„±
if [ -f "$RESULT_DIR/integrity_report.json" ]; then
    missing_files=$(python3 -c "import json; data=json.load(open('$RESULT_DIR/integrity_report.json')); print(data['summary']['required_missing'])" 2>/dev/null || echo "N/A")
    corrupted_files=$(python3 -c "import json; data=json.load(open('$RESULT_DIR/integrity_report.json')); print(data['summary']['corrupted_files'])" 2>/dev/null || echo "N/A")
    
    if [ "$missing_files" = "0" ] && [ "$corrupted_files" = "0" ]; then
        echo "  âœ… íŒŒì¼ ë¬´ê²°ì„±: ëª¨ë“  íŒŒì¼ ì •ìƒ"
    else
        echo "  âŒ íŒŒì¼ ë¬´ê²°ì„±: ëˆ„ë½ ${missing_files}ê°œ, ì†ìƒ ${corrupted_files}ê°œ"
    fi
fi

# ëª¨ë¸ ë¡œë”©
if [ -f "$RESULT_DIR/loading_report.json" ]; then
    loadable_count=$(python3 -c "
import json
data = json.load(open('$RESULT_DIR/loading_report.json'))
count = sum(1 for model in data.values() if model.get('loadable', False))
print(count)
" 2>/dev/null || echo "N/A")
    echo "  ğŸ”§ ëª¨ë¸ ë¡œë”©: ${loadable_count}ê°œ ëª¨ë¸ ë¡œë”© ê°€ëŠ¥"
fi

# ì¶”ë¡  ì„±ëŠ¥
if [ -f "$RESULT_DIR/memory_stats.json" ]; then
    max_memory=$(python3 -c "import json; data=json.load(open('$RESULT_DIR/memory_stats.json')); print(f'{data[\"max_memory_mb\"]:.1f}')" 2>/dev/null || echo "N/A")
    echo "  ğŸ’¾ ë©”ëª¨ë¦¬ ì‚¬ìš©: ìµœëŒ€ ${max_memory} MB"
fi

# ì •í™•ë„
if [ -f "$RESULT_DIR/accuracy_report.json" ]; then
    avg_f1=$(python3 -c "
import json
data = json.load(open('$RESULT_DIR/accuracy_report.json'))
f1_scores = [m['f1_score'] for m in data.get('models', {}).values() if 'f1_score' in m]
if f1_scores:
    print(f'{sum(f1_scores)/len(f1_scores):.3f}')
else:
    print('N/A')
" 2>/dev/null || echo "N/A")
    echo "  ğŸ¯ í‰ê·  F1 ì ìˆ˜: ${avg_f1}"
fi

echo ""
echo "ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:"
find "$RESULT_DIR" -name "*.json" -o -name "*.md" -o -name "*.log" | sort | while read file; do
    rel_path=$(echo "$file" | sed "s|^$(pwd)/||")
    size=$(stat -c%s "$file" 2>/dev/null | numfmt --to=iec)
    echo "  ğŸ“ $rel_path ($size)"
done
echo ""

# ë¦¬í¬íŠ¸ ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°
if [ -f "$RESULT_DIR/validation_report.md" ]; then
    echo "ğŸ“‹ ê²€ì¦ ê²°ê³¼ ë¯¸ë¦¬ë³´ê¸°:"
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    
    # ë¦¬í¬íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ë§Œ ì¶”ì¶œí•˜ì—¬ ì¶œë ¥
    grep -E "^## |^- \*\*|^âœ…|^ğŸŸ¡|^ğŸŸ |^ğŸ”´" "$RESULT_DIR/validation_report.md" | head -15
    
    echo "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”"
    echo ""
fi

echo "ğŸ” ìƒì„¸ ë¶„ì„:"
echo "  ğŸ“„ ì¢…í•© ë¦¬í¬íŠ¸: cat $RESULT_DIR/validation_report.md"
echo "  ğŸ“Š JSON ë°ì´í„°: ls $RESULT_DIR/*.json"
echo ""
echo "ğŸ’¡ ëª¨ë¸ ê°œì„  ë°©ë²•:"
echo "  ğŸ”„ ì ì§„ì  í•™ìŠµ: ./train_models_incremental.sh $MODEL_DIR /path/to/new_logs/"
echo "  ğŸ“Š ëª¨ë¸ ë¹„êµ: ./compare_models.sh $MODEL_DIR other_model/"
echo ""
echo "ğŸ‰ ëª¨ë¸ ê²€ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
echo "   - âœ… íŒŒì¼ ë¬´ê²°ì„± ê²€ì‚¬"
echo "   - âœ… ëª¨ë¸ ë¡œë”© ê²€ì¦"
echo "   - âœ… ì¶”ë¡  ì„±ëŠ¥ ì¸¡ì •"
echo "   - âœ… ì´ìƒíƒì§€ ì •í™•ë„ í‰ê°€"
echo "   - âœ… ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"
echo "   - âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§"
echo "   - âœ… ì¢…í•© í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"
