#!/usr/bin/env python3
"""
PyTorch ëª¨ë¸ì„ ONNX í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬
í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸: Python í•™ìŠµ â†’ C ì¶”ë¡  ë¸Œë¦¬ì§€
"""

import os
import json
import torch
import torch.onnx
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelConverter:
    """PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""

    def __init__(self, output_dir: str = "models/onnx"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

    def _convert_vocab_for_c_engine(self, vocab: Dict, vocab_path: str) -> Dict[str, str]:
        """
        vocab.jsonì„ C ì—”ì§„ìš© í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        Python í•™ìŠµìš©: {"template_id": index} í˜•ì‹
        C ì—”ì§„ìš©: {"template_id": "template_string"} í˜•ì‹

        Args:
            vocab: ì›ë³¸ vocab (ì¸ë±ìŠ¤ í˜•ì‹ì¼ ìˆ˜ ìˆìŒ)
            vocab_path: vocab.json íŒŒì¼ ê²½ë¡œ (parsed.parquet ì°¾ê¸° ìœ„í•´ ì‚¬ìš©)

        Returns:
            C ì—”ì§„ìš© vocab (í…œí”Œë¦¿ ë¬¸ìì—´ í¬í•¨)
        """
        # ì´ë¯¸ í…œí”Œë¦¿ ë¬¸ìì—´ í˜•ì‹ì¸ì§€ í™•ì¸
        sample_value = next(iter(vocab.values())) if vocab else None
        if sample_value and isinstance(sample_value, str) and len(sample_value) > 10:
            # ì´ë¯¸ ì˜¬ë°”ë¥¸ í˜•ì‹
            logger.info("ğŸ“‹ vocabì´ ì´ë¯¸ í…œí”Œë¦¿ ë¬¸ìì—´ í˜•ì‹ì…ë‹ˆë‹¤")
            return vocab

        # ì¸ë±ìŠ¤ í˜•ì‹ì´ë¯€ë¡œ ë³€í™˜ í•„ìš”
        logger.info("ğŸ”„ vocabì„ C ì—”ì§„ìš© í…œí”Œë¦¿ ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")

        # vocab.jsonê³¼ ê°™ì€ ë””ë ‰í† ë¦¬ì—ì„œ parsed.parquet ë˜ëŠ” preview.json ì°¾ê¸°
        vocab_dir = Path(vocab_path).parent

        # Option 1: parsed.parquetì—ì„œ ì¶”ì¶œ
        parsed_path = vocab_dir / "parsed.parquet"
        if parsed_path.exists():
            try:
                import pandas as pd
                logger.info(f"ğŸ“‚ parsed.parquetì—ì„œ í…œí”Œë¦¿ ì¶”ì¶œ: {parsed_path}")
                df = pd.read_parquet(parsed_path)

                if 'template_id' in df.columns and 'template' in df.columns:
                    # CRITICAL: Sort by template_id to match training vocab order!
                    # Training vocab is created with: {t: i for i, t in enumerate(sorted(unique_templates))}
                    df_unique = df[['template_id', 'template']].drop_duplicates('template_id').copy()
                    df_unique['template_id_int'] = df_unique['template_id'].astype(str).astype(int)
                    df_unique = df_unique.sort_values('template_id_int')

                    template_map = {}
                    for _, row in df_unique.iterrows():
                        tid = str(row['template_id'])
                        template_str = str(row['template'])
                        if not pd.isna(tid) and not pd.isna(template_str):
                            template_map[tid] = template_str

                    if template_map:
                        logger.info(f"âœ… {len(template_map)}ê°œ í…œí”Œë¦¿ ì¶”ì¶œ ì™„ë£Œ (ì •ë ¬ëœ ìˆœì„œ)")
                        return template_map
            except Exception as e:
                logger.warning(f"parsed.parquet ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        # Option 2: preview.jsonì—ì„œ ì¶”ì¶œ
        preview_path = vocab_dir / "preview.json"
        if preview_path.exists():
            try:
                logger.info(f"ğŸ“‚ preview.jsonì—ì„œ í…œí”Œë¦¿ ì¶”ì¶œ: {preview_path}")
                with open(preview_path, 'r') as f:
                    preview = json.load(f)

                template_map = {}
                for item in preview:
                    tid = str(item.get('template_id', ''))
                    template = item.get('template', '')
                    if tid and template:
                        template_map[tid] = template

                if template_map:
                    logger.info(f"âœ… {len(template_map)}ê°œ í…œí”Œë¦¿ ì¶”ì¶œ ì™„ë£Œ")
                    return template_map
            except Exception as e:
                logger.warning(f"preview.json ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        # ë³€í™˜ ì‹¤íŒ¨ - ì›ë³¸ vocab ë°˜í™˜í•˜ê³  ê²½ê³ 
        logger.warning("âš ï¸  í…œí”Œë¦¿ ë¬¸ìì—´ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        logger.warning(f"âš ï¸  {vocab_dir}ì— parsed.parquet ë˜ëŠ” preview.jsonì´ í•„ìš”í•©ë‹ˆë‹¤.")
        logger.warning("âš ï¸  C ì—”ì§„ ì‚¬ìš©ì„ ìœ„í•´ ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
        logger.warning(f"    python hybrid_system/training/export_vocab_with_templates.py \\")
        logger.warning(f"        {vocab_dir}/parsed.parquet \\")
        logger.warning(f"        {self.output_dir}/vocab.json")
        return vocab
        
    def convert_deeplog_to_onnx(
        self,
        model_path: str,
        vocab_path: str,
        output_name: str = "deeplog.onnx",
        seq_len: int = 50
    ) -> Dict[str, Any]:
        """
        DeepLog ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜

        Args:
            model_path: PyTorch ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            vocab_path: ì–´íœ˜ ì‚¬ì „ íŒŒì¼ ê²½ë¡œ
            output_name: ì¶œë ¥ ONNX íŒŒì¼ëª…
            seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´

        Returns:
            ë³€í™˜ ê²°ê³¼ ì •ë³´
        """
        logger.info(f"ğŸ”„ DeepLog ëª¨ë¸ ë³€í™˜ ì‹œì‘: {model_path}")

        # ì–´íœ˜ ì‚¬ì „ ë¡œë“œ
        with open(vocab_path, 'r') as f:
            vocab = json.load(f)
        vocab_size = len(vocab)

        # PyTorch ëª¨ë¸ ë¡œë“œ
        # DeepLog ëª¨ë¸ í´ë˜ìŠ¤ import
        import sys
        from pathlib import Path
        # anomaly_log_detector íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€
        root_dir = Path(__file__).parent.parent.parent
        if str(root_dir) not in sys.path:
            sys.path.insert(0, str(root_dir))

        from anomaly_log_detector.builders.deeplog import DeepLogLSTM

        # state dict ë¡œë“œ
        state = torch.load(model_path, map_location='cpu')
        model_vocab_size = int(state.get("vocab_size", vocab_size))
        model_seq_len = int(state.get("seq_len", seq_len))

        # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        model = DeepLogLSTM(vocab_size=model_vocab_size)
        model.load_state_dict(state["state_dict"])
        model.eval()

        # seq_len ì—…ë°ì´íŠ¸ (ëª¨ë¸ì— ì €ì¥ëœ ê°’ ì‚¬ìš©)
        seq_len = model_seq_len
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„± (ë°°ì¹˜ í¬ê¸° 1, ì‹œí€€ìŠ¤ ê¸¸ì´)
        dummy_input = torch.randint(0, vocab_size, (1, seq_len), dtype=torch.long)

        # ONNX ë³€í™˜
        onnx_path = self.output_dir / output_name

        # PyTorch ë²„ì „ì— ë”°ë¼ ì ì ˆí•œ ONNX export ë°©ì‹ ì„ íƒ
        import warnings
        pytorch_version = tuple(int(x) for x in torch.__version__.split('+')[0].split('.')[:2])

        # PyTorch 2.9+ ë˜ëŠ” dynamo ì§€ì› ë²„ì „ì—ì„œëŠ” ìƒˆë¡œìš´ ë°©ì‹ ì‚¬ìš© ì‹œë„
        export_success = False

        if pytorch_version >= (2, 4):  # dynamo ê¸°ë°˜ exportëŠ” 2.4+ì—ì„œ ì‚¬ìš© ê°€ëŠ¥
            try:
                logger.info("ğŸ”„ PyTorch dynamo ê¸°ë°˜ ONNX export ì‹œë„...")
                with warnings.catch_warnings():
                    warnings.filterwarnings("ignore", category=UserWarning)

                    torch.onnx.export(
                        model,
                        dummy_input,
                        str(onnx_path),
                        export_params=True,
                        opset_version=11,
                        do_constant_folding=True,
                        input_names=['input_sequence'],
                        output_names=['predictions'],
                        dynamic_axes={
                            'input_sequence': {0: 'batch_size', 1: 'sequence_length'},
                            'predictions': {0: 'batch_size'}
                        },
                        dynamo=True,  # ìƒˆë¡œìš´ export ë°©ì‹
                        verbose=False
                    )
                export_success = True
                logger.info("âœ… dynamo ê¸°ë°˜ export ì„±ê³µ")
            except Exception as e:
                logger.warning(f"âš ï¸  dynamo export ì‹¤íŒ¨: {e}")
                logger.info("ğŸ”„ ë ˆê±°ì‹œ TorchScript ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„...")

        # ë ˆê±°ì‹œ ë°©ì‹ ë˜ëŠ” dynamo ì‹¤íŒ¨ ì‹œ
        if not export_success:
            with warnings.catch_warnings():
                warnings.filterwarnings("ignore", category=UserWarning)
                warnings.filterwarnings("ignore", category=DeprecationWarning)

                torch.onnx.export(
                    model,
                    dummy_input,
                    str(onnx_path),
                    export_params=True,
                    opset_version=11,  # í˜¸í™˜ì„±ì„ ìœ„í•´ ì•ˆì •ì ì¸ ë²„ì „ ì‚¬ìš©
                    do_constant_folding=True,
                    input_names=['input_sequence'],
                    output_names=['predictions'],
                    dynamic_axes={
                        'input_sequence': {0: 'batch_size', 1: 'sequence_length'},
                        'predictions': {0: 'batch_size'}
                    },
                    verbose=False
                )
            logger.info("âœ… ë ˆê±°ì‹œ TorchScript ë°©ì‹ export ì„±ê³µ")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'model_type': 'deeplog',
            'vocab_size': vocab_size,
            'seq_len': seq_len,
            'input_shape': [1, seq_len],
            'output_shape': [1, vocab_size],
            'input_names': ['input_sequence'],
            'output_names': ['predictions'],
            'opset_version': 11
        }
        
        metadata_path = self.output_dir / f"{output_name}.meta.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # ì–´íœ˜ ì‚¬ì „ ì²˜ë¦¬
        vocab_output = self.output_dir / "vocab.json"

        # vocab.json í˜•ì‹ í™•ì¸ ë° ë³€í™˜
        vocab_for_c_engine = self._convert_vocab_for_c_engine(vocab, vocab_path)

        with open(vocab_output, 'w') as f:
            json.dump(vocab_for_c_engine, f, ensure_ascii=False, indent=2)

        logger.info(f"âœ… DeepLog ë³€í™˜ ì™„ë£Œ: {onnx_path}")
        logger.info(f"ğŸ“Š ë©”íƒ€ë°ì´í„°: {metadata_path}")
        logger.info(f"ğŸ“š ì–´íœ˜ ì‚¬ì „: {vocab_output}")

        # vocab í˜•ì‹ í™•ì¸ ë©”ì‹œì§€
        sample_value = next(iter(vocab_for_c_engine.values())) if vocab_for_c_engine else None
        if sample_value and isinstance(sample_value, str) and len(sample_value) > 10:
            logger.info(f"âœ… C ì—”ì§„ìš© vocab í˜•ì‹ (template strings): {len(vocab_for_c_engine)} templates")
        else:
            logger.warning(f"âš ï¸  vocabì´ ì¸ë±ìŠ¤ í˜•ì‹ì…ë‹ˆë‹¤. C ì—”ì§„ ì‚¬ìš© ì‹œ í…œí”Œë¦¿ ë¬¸ìì—´ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return {
            'onnx_path': str(onnx_path),
            'metadata_path': str(metadata_path),
            'vocab_path': str(vocab_output),
            'metadata': metadata
        }
    
    def convert_mscred_to_onnx(
        self,
        model_path: str,
        output_name: str = "mscred.onnx",
        window_size: int = 50,
        feature_dim: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        MS-CRED ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜
        
        Args:
            model_path: PyTorch ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            output_name: ì¶œë ¥ ONNX íŒŒì¼ëª…
            window_size: ìœˆë„ìš° í¬ê¸°
            feature_dim: í”¼ì²˜ ì°¨ì› (ìë™ ê°ì§€ ì‹œë„)
            
        Returns:
            ë³€í™˜ ê²°ê³¼ ì •ë³´
        """
        logger.info(f"ğŸ”„ MS-CRED ëª¨ë¸ ë³€í™˜ ì‹œì‘: {model_path}")

        # MS-CRED ëª¨ë¸ í´ë˜ìŠ¤ import
        import sys
        from pathlib import Path
        # anomaly_log_detector íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€
        root_dir = Path(__file__).parent.parent.parent
        if str(root_dir) not in sys.path:
            sys.path.insert(0, str(root_dir))

        from anomaly_log_detector.mscred_model import MSCREDModel

        # state dict ë¡œë“œ
        state = torch.load(model_path, map_location='cpu')

        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        # MS-CREDëŠ” 'model_state_dict' í‚¤ë¡œ ì €ì¥ë¨
        if isinstance(state, dict):
            if 'model_state_dict' in state:
                # MSCREDTrainer.save_model() í˜•ì‹
                state_dict = state['model_state_dict']
                saved_feature_dim = state.get('feature_dim', feature_dim)
                saved_window_size = state.get('window_size', window_size)
            elif 'state_dict' in state:
                # ë‹¤ë¥¸ ì €ì¥ í˜•ì‹
                state_dict = state['state_dict']
                saved_feature_dim = state.get('feature_dim', feature_dim)
                saved_window_size = state.get('window_size', window_size)
            else:
                # state_dictë§Œ ìˆëŠ” ê²½ìš° (êµ¬ë²„ì „)
                state_dict = state
                saved_feature_dim = feature_dim
                saved_window_size = window_size
        else:
            state_dict = state
            saved_feature_dim = feature_dim
            saved_window_size = window_size

        # ëª¨ë¸ ìƒì„±
        # MSCREDModelì˜ íŒŒë¼ë¯¸í„°: input_channels, base_channels
        # feature_dimì€ ONNX exportì‹œ ì…ë ¥ í¬ê¸° ê²°ì •ì—ë§Œ ì‚¬ìš©
        try:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ëª¨ë¸ ìƒì„± (input_channels=1, base_channels=32)
            model = MSCREDModel(input_channels=1, base_channels=32)
            model.load_state_dict(state_dict)
            model.eval()
        except Exception as e:
            logger.warning(f"MSCREDModel ë¡œë”© ì‹¤íŒ¨, ì¬ì‹œë„: {e}")
            # state_dict í‚¤ë¥¼ í™•ì¸í•˜ì—¬ íŒŒë¼ë¯¸í„° ì¶”ì •
            try:
                # state dictì—ì„œ ì²« Conv ë ˆì´ì–´ì˜ in_channels ì¶”ì¶œ ì‹œë„
                first_conv_key = [k for k in state_dict.keys() if 'encoder' in k and 'conv' in k and 'weight' in k][0]
                in_channels = state_dict[first_conv_key].shape[1]
                model = MSCREDModel(input_channels=in_channels, base_channels=32)
                model.load_state_dict(state_dict)
                model.eval()
            except:
                # ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ state ìì²´ê°€ ëª¨ë¸ì¼ ìˆ˜ ìˆìŒ
                if hasattr(state, 'eval'):
                    model = state
                    model.eval()
                else:
                    raise RuntimeError(f"MSCREDModel ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # í”¼ì²˜ ì°¨ì› ê²°ì •
        # MS-CREDì˜ feature_dimì€ í…œí”Œë¦¿ ê°œìˆ˜ (width)ë¥¼ ì˜ë¯¸
        if feature_dim is None:
            # saved stateì—ì„œ í™•ì¸
            if saved_feature_dim and saved_feature_dim > 1:
                feature_dim = saved_feature_dim
            else:
                # ê¸°ë³¸ê°’: ì¼ë°˜ì ì¸ ë¡œê·¸ í…œí”Œë¦¿ ê°œìˆ˜
                # ë„ˆë¬´ ì‘ìœ¼ë©´ conv ê³„ì‚°ì´ ì‹¤íŒ¨í•˜ë¯€ë¡œ ì¶©ë¶„íˆ í° ê°’ ì‚¬ìš©
                feature_dim = 100
                logger.warning(f"í”¼ì²˜ ì°¨ì›ì„ ëª…ì‹œí•˜ì§€ ì•ŠìŒ. ê¸°ë³¸ê°’ ì‚¬ìš©: {feature_dim}")
                logger.warning(f"ì‹¤ì œ ì‚¬ìš©í•œ í…œí”Œë¦¿ ê°œìˆ˜ì™€ ë§ì§€ ì•Šìœ¼ë©´ --feature-dim ì˜µì…˜ìœ¼ë¡œ ì§€ì •í•˜ì„¸ìš”.")

        # ìµœì†Œ í¬ê¸° ê²€ì¦ (conv ë ˆì´ì–´ë¥¼ í†µê³¼í•˜ê¸° ìœ„í•œ ìµœì†Œ í¬ê¸°)
        # MultiScaleConvBlockì˜ ìµœëŒ€ kernel_size=7, padding=3ì´ë¯€ë¡œ
        # ìµœì†Œ feature_dim >= 7 í•„ìš”
        if feature_dim < 10:
            logger.warning(f"feature_dim({feature_dim})ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ìµœì†Œ 10ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            feature_dim = 10

        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        # MSCREDModelì€ 4D í…ì„œ (batch, channels, height, width) ê¸°ëŒ€
        # - batch: 1
        # - channels: 1 (input_channels)
        # - height: window_size (ì‹œê°„ ì¶•)
        # - width: feature_dim (í…œí”Œë¦¿ ìˆ˜)
        logger.info(f"MSCRED ë”ë¯¸ ì…ë ¥ í¬ê¸°: (1, 1, {window_size}, {feature_dim})")
        dummy_input = torch.randn(1, 1, window_size, feature_dim)
        
        # ONNX ë³€í™˜
        onnx_path = self.output_dir / output_name

        # ë¶ˆí•„ìš”í•œ ê²½ê³  ì–µì œ
        import warnings
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning)

            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input_features'],
                output_names=['reconstructed'],
                dynamic_axes={
                    'input_features': {0: 'batch_size'},
                    'reconstructed': {0: 'batch_size'}
                },
                verbose=False
            )
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'model_type': 'mscred',
            'window_size': window_size,
            'feature_dim': feature_dim,
            'input_shape': [1, 1, window_size, feature_dim],  # (batch, channels, height, width)
            'output_shape': [1, 1, window_size, feature_dim],
            'input_names': ['input_features'],
            'output_names': ['reconstructed'],
            'opset_version': 11
        }
        
        metadata_path = self.output_dir / f"{output_name}.meta.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"âœ… MS-CRED ë³€í™˜ ì™„ë£Œ: {onnx_path}")
        logger.info(f"ğŸ“Š ë©”íƒ€ë°ì´í„°: {metadata_path}")
        
        return {
            'onnx_path': str(onnx_path),
            'metadata_path': str(metadata_path),
            'metadata': metadata
        }
    
    def validate_onnx_model(self, onnx_path: str) -> bool:
        """
        ONNX ëª¨ë¸ì˜ ìœ íš¨ì„± ê²€ì¦
        
        Args:
            onnx_path: ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ìœ íš¨ì„± ê²€ì¦ ê²°ê³¼
        """
        try:
            import onnx
            import onnxruntime as ort
            
            # ONNX ëª¨ë¸ ë¡œë“œ ë° ê²€ì¦
            model = onnx.load(onnx_path)
            onnx.checker.check_model(model)
            
            # ONNX Runtimeìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥ì„± í™•ì¸
            session = ort.InferenceSession(onnx_path)
            
            logger.info(f"âœ… ONNX ëª¨ë¸ ê²€ì¦ ì„±ê³µ: {onnx_path}")
            logger.info(f"ğŸ“Š ì…ë ¥: {[input.name for input in session.get_inputs()]}")
            logger.info(f"ğŸ“Š ì¶œë ¥: {[output.name for output in session.get_outputs()]}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ONNX ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def optimize_onnx_model(self, onnx_path: str, portable: bool = False) -> str:
        """
        ONNX ëª¨ë¸ ìµœì í™” (ê·¸ë˜í”„ ìµœì í™”, ìƒìˆ˜ í´ë”© ë“±)

        Args:
            onnx_path: ì›ë³¸ ONNX ëª¨ë¸ ê²½ë¡œ
            portable: Trueì´ë©´ ë²”ìš© ìµœì í™”ë§Œ ì ìš© (í•˜ë“œì›¨ì–´ íŠ¹í™” ìµœì í™” ì œì™¸)
                     Falseì´ë©´ ìµœëŒ€ ìµœì í™” ì ìš© (í˜„ì¬ í•˜ë“œì›¨ì–´ì— íŠ¹í™”)

        Returns:
            ìµœì í™”ëœ ëª¨ë¸ ê²½ë¡œ
        """
        try:
            import onnxruntime as ort

            # ìµœì í™” ì„¤ì •
            sess_options = ort.SessionOptions()

            if portable:
                # ë²”ìš© ìµœì í™”: ëª¨ë“  í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥
                # ORT_ENABLE_BASIC: ê¸°ë³¸ ê·¸ë˜í”„ ìµœì í™”ë§Œ ì ìš©
                sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC
                suffix = '_portable'
                logger.info("ğŸŒ ë²”ìš© ìµœì í™” ëª¨ë“œ (ëª¨ë“  í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)")
            else:
                # ìµœëŒ€ ìµœì í™”: í˜„ì¬ í•˜ë“œì›¨ì–´ì— íŠ¹í™”
                # ORT_ENABLE_ALL: í•˜ë“œì›¨ì–´ë³„ ìµœì í™” í¬í•¨
                sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
                suffix = '_optimized'
                logger.info("âš¡ ìµœëŒ€ ìµœì í™” ëª¨ë“œ (í˜„ì¬ í™˜ê²½ì— íŠ¹í™”)")

            sess_options.optimized_model_filepath = onnx_path.replace('.onnx', f'{suffix}.onnx')

            # ì„¸ì…˜ ìƒì„± ì‹œ ìë™ìœ¼ë¡œ ìµœì í™”ëœ ëª¨ë¸ ì €ì¥
            session = ort.InferenceSession(onnx_path, sess_options)

            optimized_path = sess_options.optimized_model_filepath

            # ìµœì í™” íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            import os
            if os.path.exists(optimized_path):
                logger.info(f"âœ… ONNX ëª¨ë¸ ìµœì í™” ì™„ë£Œ: {optimized_path}")
                return optimized_path
            else:
                # ìµœì í™”ê°€ ì ìš©ë˜ì—ˆì§€ë§Œ íŒŒì¼ë¡œ ì €ì¥ë˜ì§€ ì•ŠìŒ (ë©”ëª¨ë¦¬ ë‚´ ìµœì í™”)
                logger.info(f"âš¡ ONNX ëª¨ë¸ ìµœì í™” ì ìš©ë¨ (ë©”ëª¨ë¦¬ ë‚´): {onnx_path}")
                return onnx_path

        except Exception as e:
            logger.error(f"âŒ ONNX ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return onnx_path


def convert_all_models(
    deeplog_model: str,
    mscred_model: str,
    vocab_path: str,
    output_dir: str = "models/onnx",
    feature_dim: Optional[int] = None,
    portable: bool = False
) -> Dict[str, Any]:
    """
    ëª¨ë“  ëª¨ë¸ì„ ì¼ê´„ ë³€í™˜

    Args:
        deeplog_model: DeepLog ëª¨ë¸ ê²½ë¡œ
        mscred_model: MS-CRED ëª¨ë¸ ê²½ë¡œ
        vocab_path: ì–´íœ˜ ì‚¬ì „ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        feature_dim: MS-CRED í”¼ì²˜ ì°¨ì› (í…œí”Œë¦¿ ê°œìˆ˜, Noneì´ë©´ ìë™ ê°ì§€)
        portable: Trueì´ë©´ ë²”ìš© ìµœì í™” (ëª¨ë“  í™˜ê²½), Falseì´ë©´ ìµœëŒ€ ìµœì í™” (í˜„ì¬ í•˜ë“œì›¨ì–´)

    Returns:
        ë³€í™˜ ê²°ê³¼ ìš”ì•½
    """
    converter = ModelConverter(output_dir)
    results = {}

    # DeepLog ë³€í™˜
    if os.path.exists(deeplog_model):
        try:
            deeplog_result = converter.convert_deeplog_to_onnx(
                deeplog_model, vocab_path
            )

            # ê²€ì¦ ë° ìµœì í™”
            if converter.validate_onnx_model(deeplog_result['onnx_path']):
                optimized_path = converter.optimize_onnx_model(
                    deeplog_result['onnx_path'],
                    portable=portable
                )
                deeplog_result['optimized_path'] = optimized_path

            results['deeplog'] = deeplog_result

        except Exception as e:
            logger.error(f"âŒ DeepLog ë³€í™˜ ì‹¤íŒ¨: {e}")
            results['deeplog'] = {'error': str(e)}

    # MS-CRED ë³€í™˜
    if os.path.exists(mscred_model):
        try:
            mscred_result = converter.convert_mscred_to_onnx(
                mscred_model,
                feature_dim=feature_dim
            )

            # ê²€ì¦ ë° ìµœì í™”
            if converter.validate_onnx_model(mscred_result['onnx_path']):
                optimized_path = converter.optimize_onnx_model(
                    mscred_result['onnx_path'],
                    portable=portable
                )
                mscred_result['optimized_path'] = optimized_path

            results['mscred'] = mscred_result

        except Exception as e:
            logger.error(f"âŒ MS-CRED ë³€í™˜ ì‹¤íŒ¨: {e}")
            results['mscred'] = {'error': str(e)}
    
    # ë³€í™˜ ìš”ì•½ ì €ì¥
    summary_path = Path(output_dir) / "conversion_summary.json"
    with open(summary_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    logger.info(f"ğŸ“‹ ë³€í™˜ ìš”ì•½ ì €ì¥: {summary_path}")
    
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜")
    parser.add_argument("--deeplog-model", type=str, help="DeepLog ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--mscred-model", type=str, help="MS-CRED ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--vocab", type=str, help="ì–´íœ˜ ì‚¬ì „ ê²½ë¡œ")
    parser.add_argument("--output-dir", type=str, default="models/onnx", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--validate", action="store_true", help="ë³€í™˜ í›„ ê²€ì¦ ì‹¤í–‰")
    
    args = parser.parse_args()
    
    if not args.deeplog_model and not args.mscred_model:
        print("âŒ ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        parser.print_help()
        exit(1)
    
    # ë³€í™˜ ì‹¤í–‰
    results = convert_all_models(
        args.deeplog_model or "",
        args.mscred_model or "",
        args.vocab or "",
        args.output_dir
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ‰ ëª¨ë¸ ë³€í™˜ ì™„ë£Œ!")
    for model_name, result in results.items():
        if 'error' in result:
            print(f"âŒ {model_name}: {result['error']}")
        else:
            print(f"âœ… {model_name}: {result['onnx_path']}")
            if 'optimized_path' in result:
                print(f"âš¡ ìµœì í™”ë¨: {result['optimized_path']}")
