#!/usr/bin/env python3  # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì…”ë±…
"""PyTorch ëª¨ë¸ì„ ONNX í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬ ìš”ì•½

- ëª©ì : Python í•™ìŠµ ëª¨ë¸(PyTorch)ì„ C ì¶”ë¡  ì—”ì§„ì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ONNXë¡œ ë³€í™˜
- ë³€í™˜ ëŒ€ìƒ: DeepLog LSTM, MS-CRED ì»¨ë³¼ë£¨ì…˜ ëª¨ë¸
- í•µì‹¬ ê¸°ëŠ¥:
  * DeepLog/MS-CRED ëª¨ë¸ì„ ONNXë¡œ export (dynamic_axes ì§€ì›)
  * vocab.jsonì„ Python í˜•ì‹ì—ì„œ C ì—”ì§„ìš© í˜•ì‹ìœ¼ë¡œ ë³€í™˜
  * ONNX ëª¨ë¸ ê²€ì¦ ë° ìµœì í™” (ë²”ìš©/í•˜ë“œì›¨ì–´ íŠ¹í™” ì˜µì…˜)
- ì¶œë ¥: ONNX ëª¨ë¸(.onnx), ë©”íƒ€ë°ì´í„°(.meta.json), C ì—”ì§„ìš© vocab.json
"""  # ëª¨ë“ˆ ìš”ì•½ ì„¤ëª…
import os  # í™˜ê²½ ë³€ìˆ˜/ê²½ë¡œ ìœ í‹¸
import json  # JSON ì…ì¶œë ¥
import torch  # PyTorch ê¸°ë³¸ ëª¨ë“ˆ
import torch.onnx  # ONNX export API
import numpy as np  # (ë¯¸ì‚¬ìš©) ìˆ˜ì¹˜ ì—°ì‚°
from pathlib import Path  # ê²½ë¡œ ì²˜ë¦¬
from typing import Dict, Any, Optional, Tuple  # íƒ€ì… íŒíŠ¸
import logging  # ë¡œê¹… í”„ë ˆì„ì›Œí¬

# ë¡œê¹… ì„¤ì •  # INFO ë ˆë²¨ ê¸°ë³¸ ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)  # ëª¨ë“ˆ ë¡œê±° ìƒì„±


class ModelConverter:  # ëª¨ë¸ ë³€í™˜ê¸° í´ë˜ìŠ¤
    """PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""  # í´ë˜ìŠ¤ ì„¤ëª…

    def __init__(self, output_dir: str = "models/onnx"):  # ì´ˆê¸°í™”
        self.output_dir = Path(output_dir)  # ì¶œë ¥ ê²½ë¡œ
        self.output_dir.mkdir(parents=True, exist_ok=True)  # í´ë” ìƒì„±

    def _convert_vocab_for_c_engine(self, vocab: Dict, vocab_path: str) -> Dict[str, str]:  # C ì—”ì§„ìš© vocab ë³€í™˜
        """
        vocab.jsonì„ C ì—”ì§„ìš© í˜•ì‹ìœ¼ë¡œ ë³€í™˜

        Python í•™ìŠµìš©: {"template_string": index} í˜•ì‹ (sorted template string order)
        C ì—”ì§„ìš©: {"index": "template_string"} í˜•ì‹ (same sorted order)

        CRITICAL: Python vocab is created with:
            {t: i for i, t in enumerate(sorted(unique_templates))}
        So vocab indices are in SORTED TEMPLATE STRING order, NOT template_id order!

        Args:
            vocab: ì›ë³¸ vocab ({"template_string": index} í˜•ì‹)  # ì…ë ¥ vocab
            vocab_path: vocab.json íŒŒì¼ ê²½ë¡œ (ì°¸ê³ ìš©)  # ì°¸ê³  ê²½ë¡œ

        Returns:
            C ì—”ì§„ìš© vocab ({"index": "template_string"} í˜•ì‹)  # ì¶œë ¥ vocab
        """  # API ì„¤ëª…
        # vocab í˜•ì‹ í™•ì¸  # ë¹ˆ vocab ì²˜ë¦¬
        if not vocab:
            logger.warning("âš ï¸  ë¹ˆ vocab")  # ê²½ê³  ë¡œê·¸
            return vocab  # ê·¸ëŒ€ë¡œ ë°˜í™˜

        # ì²« ë²ˆì§¸ í•­ëª©ìœ¼ë¡œ í˜•ì‹ íŒë‹¨  # ìƒ˜í”Œ í•­ëª©ìœ¼ë¡œ í˜•ì‹ ì¶”ì •
        first_key = next(iter(vocab.keys()))  # ì²« í‚¤ ì¶”ì¶œ
        first_value = next(iter(vocab.values()))  # ì²« ê°’ ì¶”ì¶œ

        # Case 1: ì´ë¯¸ C ì—”ì§„ìš© í˜•ì‹ {"0": "template_string", ...}  # í˜•ì‹ ê²€ì‚¬
        if isinstance(first_value, str) and not first_key.isdigit():  # ê°’ì€ ë¬¸ìì—´, í‚¤ëŠ” ìˆ«ì ì•„ë‹˜
            # ì˜ëª»ëœ í˜•ì‹ ê²½ê³  (keyê°€ ìˆ«ìê°€ ì•„ë‹Œ ê²½ìš°)
            logger.warning(f"âš ï¸  vocab í˜•ì‹ì´ ì´ìƒí•©ë‹ˆë‹¤: key='{first_key}', value='{first_value}'")
            logger.warning("âš ï¸  ì˜ˆìƒ í˜•ì‹: {{\"template_string\": index}} ë˜ëŠ” {{\"index\": \"template_string\"}}")

        if isinstance(first_value, str) and first_key.isdigit():  # C ì—”ì§„ìš© í˜•ì‹ í™•ì¸
            # ì´ë¯¸ C ì—”ì§„ìš© í˜•ì‹ {"0": "template string"}
            logger.info("ğŸ“‹ vocabì´ ì´ë¯¸ C ì—”ì§„ìš© í…œí”Œë¦¿ ë¬¸ìì—´ í˜•ì‹ì…ë‹ˆë‹¤")
            return vocab  # ë³€í™˜ ë¶ˆí•„ìš”

        # Case 2: Python í•™ìŠµìš© í˜•ì‹ì¸ì§€ í™•ì¸  # Python í˜•ì‹ ì²´í¬
        # ì˜¬ë°”ë¥¸ í˜•ì‹: {"template_string": 0, ...}
        # ì˜ëª»ëœ í˜•ì‹: {"1": 0, "2": 1, ...} (template_idë¥¼ keyë¡œ ì‚¬ìš©)
        if isinstance(first_value, int):  # ê°’ì´ ì •ìˆ˜ë©´ Python í˜•ì‹ ê°€ëŠ¥
            # ì‹¤ì œ í…œí”Œë¦¿ ë¬¸ìì—´ì¸ì§€ í™•ì¸  # template_id ì˜¤ìš© ë°©ì§€
            # template_idëŠ” ë³´í†µ ì§§ì€ ìˆ«ì ë¬¸ìì—´ì´ë¯€ë¡œ ê¸¸ì´ë¡œ êµ¬ë¶„
            if first_key.isdigit() and len(first_key) <= 5:  # template_id ì˜¤ìš© ê°ì§€
                logger.error("âŒ vocabì´ template_idë¥¼ keyë¡œ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤!")
                logger.error(f"   í˜„ì¬ í˜•ì‹: {{\"{first_key}\": {first_value}, ...}}")
                logger.error("   ì˜¬ë°”ë¥¸ í˜•ì‹: {{\"actual template string\": 0, ...}}")
                logger.error("   í•´ê²°: build_deeplog_inputs()ì—ì„œ template_col='template' ì‚¬ìš©")
                raise ValueError(
                    "vocab.jsonì´ template_idë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. "
                    "build_deeplog_inputs(template_col='template')ë¡œ ì¬ìƒì„±í•˜ì„¸ìš”."
                )

        # Python vocab í˜•ì‹ í™•ì¸: {template_string: index}  # ë³€í™˜ ìˆ˜í–‰
        # ë³€í™˜: {index: template_string}
        logger.info("ğŸ”„ vocabì„ C ì—”ì§„ìš© í…œí”Œë¦¿ ë¬¸ìì—´ í˜•ì‹ìœ¼ë¡œ ë³€í™˜ ì¤‘...")

        # CRITICAL: Python vocabì˜ ì¸ë±ìŠ¤ ìˆœì„œë¥¼ ê·¸ëŒ€ë¡œ ìœ ì§€!  # ìˆœì„œ ë³´ì¡´ ì¤‘ìš”
        # Python: {template: idx} where idxëŠ” sorted(template) ìˆœì„œ
        # C: {str(idx): template} ë™ì¼í•œ ìˆœì„œ
        template_map = {}  # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        for template_str, vocab_idx in vocab.items():  # ê° í•­ëª© ë³€í™˜
            template_map[str(vocab_idx)] = template_str  # ì¸ë±ìŠ¤: í…œí”Œë¦¿ ë¬¸ìì—´

        if template_map:  # ë³€í™˜ ì„±ê³µ
            logger.info(f"âœ… {len(template_map)}ê°œ í…œí”Œë¦¿ ë³€í™˜ ì™„ë£Œ")
            logger.info(f"ğŸ“Š Python vocab ìˆœì„œ ìœ ì§€ (sorted template string order)")

            # ê²€ì¦: ì¸ë±ìŠ¤ê°€ ì—°ì†ì ì¸ì§€ í™•ì¸  # ì¸ë±ìŠ¤ ì—°ì†ì„± ì²´í¬
            indices = sorted([int(k) for k in template_map.keys()])
            expected_indices = list(range(len(indices)))  # ê¸°ëŒ€ ì¸ë±ìŠ¤
            if indices != expected_indices:  # ë¶ˆì—°ì† ê°ì§€
                logger.warning(f"âš ï¸  vocab ì¸ë±ìŠ¤ê°€ ì—°ì†ì ì´ì§€ ì•ŠìŠµë‹ˆë‹¤!")
                logger.warning(f"   ê¸°ëŒ€: {expected_indices[:5]}...")
                logger.warning(f"   ì‹¤ì œ: {indices[:5]}...")

            return template_map  # ë³€í™˜ ê²°ê³¼ ë°˜í™˜

        # ë³€í™˜ ì‹¤íŒ¨  # ë³€í™˜ ì‹¤íŒ¨ ì²˜ë¦¬
        logger.warning("âš ï¸  vocab ë³€í™˜ ì‹¤íŒ¨")
        return vocab  # ì›ë³¸ ë°˜í™˜
        
    def convert_deeplog_to_onnx(
        self,
        model_path: str,
        vocab_path: str,
        output_name: str = "deeplog.onnx",
        seq_len: Optional[int] = None
    ) -> Dict[str, Any]:  # DeepLogë¥¼ ONNXë¡œ ë³€í™˜
        """
        DeepLog ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜

        Args:
            model_path: PyTorch ëª¨ë¸ íŒŒì¼ ê²½ë¡œ  # ì…ë ¥ ëª¨ë¸ ê²½ë¡œ
            vocab_path: ì–´íœ˜ ì‚¬ì „ íŒŒì¼ ê²½ë¡œ  # vocab.json ê²½ë¡œ
            output_name: ì¶œë ¥ ONNX íŒŒì¼ëª…  # ì¶œë ¥ íŒŒì¼ëª…
            seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´ (Noneì´ë©´ ëª¨ë¸ì— ì €ì¥ëœ ê°’ ì‚¬ìš©,  # ë™ì  ê¸¸ì´ ì§€ì›
                    ONNXëŠ” dynamic_axesë¡œ ë‹¤ì–‘í•œ ê¸¸ì´ ì§€ì›)

        Returns:
            ë³€í™˜ ê²°ê³¼ ì •ë³´  # ONNX ê²½ë¡œ, ë©”íƒ€ë°ì´í„°, vocab ê²½ë¡œ
        """  # API ì„¤ëª…
        logger.info(f"ğŸ”„ DeepLog ëª¨ë¸ ë³€í™˜ ì‹œì‘: {model_path}")  # ì‹œì‘ ë¡œê·¸

        # ì–´íœ˜ ì‚¬ì „ ë¡œë“œ  # vocab íŒŒì¼ ì½ê¸°
        with open(vocab_path, 'r') as f:
            vocab = json.load(f)
        vocab_size = len(vocab)  # ì–´íœ˜ í¬ê¸°

        # PyTorch ëª¨ë¸ ë¡œë“œ  # DeepLog í´ë˜ìŠ¤ import
        # DeepLog ëª¨ë¸ í´ë˜ìŠ¤ import
        import sys
        from pathlib import Path
        # anomaly_log_detector íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€
        root_dir = Path(__file__).parent.parent.parent  # í”„ë¡œì íŠ¸ ë£¨íŠ¸
        if str(root_dir) not in sys.path:
            sys.path.insert(0, str(root_dir))  # ê²½ë¡œ ì¶”ê°€

        from anomaly_log_detector.builders.deeplog import DeepLogLSTM  # LSTM í´ë˜ìŠ¤ ì„í¬íŠ¸

        # state dict ë¡œë“œ  # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        state = torch.load(model_path, map_location='cpu')  # CPUì—ì„œ ë¡œë“œ
        model_vocab_size = int(state.get("vocab_size", vocab_size))  # ëª¨ë¸ ì–´íœ˜ í¬ê¸°
        model_seq_len = int(state.get("seq_len", 50))  # ê¸°ë³¸ê°’ 50  # í•™ìŠµ ì‹œí€€ìŠ¤ ê¸¸ì´
        model_embed_dim = int(state.get("embed_dim", 64))  # ê¸°ë³¸ê°’ 64  # ì„ë² ë”© ì°¨ì›
        model_hidden_dim = int(state.get("hidden_dim", 128))  # ê¸°ë³¸ê°’ 128  # ì€ë‹‰ ì°¨ì›

        # seq_len ê²°ì •: íŒŒë¼ë¯¸í„°ë¡œ ì§€ì •ë˜ì§€ ì•Šìœ¼ë©´ ëª¨ë¸ì— ì €ì¥ëœ ê°’ ì‚¬ìš©  # ì‹œí€€ìŠ¤ ê¸¸ì´ ê²°ì •
        if seq_len is None:
            seq_len = model_seq_len  # ëª¨ë¸ ì €ì¥ê°’ ì‚¬ìš©

        # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ  # ì¸ìŠ¤í„´ìŠ¤ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë³µì›
        logger.info(f"ğŸ“Š ëª¨ë¸ íŒŒë¼ë¯¸í„°: vocab_size={model_vocab_size}, embed_dim={model_embed_dim}, hidden_dim={model_hidden_dim}, seq_len={seq_len}")
        model = DeepLogLSTM(vocab_size=model_vocab_size, embed_dim=model_embed_dim, hidden_dim=model_hidden_dim)  # ëª¨ë¸ ìƒì„±
        model.load_state_dict(state["state_dict"])  # ê°€ì¤‘ì¹˜ ë¡œë“œ
        model.eval()  # í‰ê°€ ëª¨ë“œ ì„¤ì •
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„± (ë°°ì¹˜ í¬ê¸° 1, ì‹œí€€ìŠ¤ ê¸¸ì´)  # ONNX exportìš© ìƒ˜í”Œ ì…ë ¥
        dummy_input = torch.randint(0, vocab_size, (1, seq_len), dtype=torch.long)  # ëœë¤ ì •ìˆ˜ ì‹œí€€ìŠ¤

        # ONNX ë³€í™˜  # ONNX íŒŒì¼ ê²½ë¡œ ì„¤ì •
        onnx_path = self.output_dir / output_name  # ì¶œë ¥ ê²½ë¡œ

        # ONNX export (ì•ˆì •ì ì¸ ë ˆê±°ì‹œ ë°©ì‹ ì‚¬ìš©)  # TorchScript ë°©ì‹ ì‚¬ìš©
        # Note: dynamo ê¸°ë°˜ exportëŠ” ì¼ë¶€ ëª¨ë¸ì—ì„œ ì˜¤ë¥˜ ë°œìƒí•˜ë¯€ë¡œ ë ˆê±°ì‹œ ë°©ì‹ ì‚¬ìš©
        import warnings

        logger.info("ğŸ”„ ONNX export ì‹œì‘ (TorchScript ë°©ì‹)...")  # export ì‹œì‘ ë¡œê·¸
        with warnings.catch_warnings():  # ê²½ê³  ì–µì œ
            warnings.filterwarnings("ignore", category=UserWarning)  # ì‚¬ìš©ì ê²½ê³  ë¬´ì‹œ
            warnings.filterwarnings("ignore", category=DeprecationWarning)  # íì§€ ê²½ê³  ë¬´ì‹œ
            warnings.filterwarnings("ignore", category=FutureWarning)  # ë¯¸ë˜ ê²½ê³  ë¬´ì‹œ

            # PyTorch 2.0+ í˜¸í™˜ì„±: dynamo ë°©ì‹ ëª…ì‹œì  ë¹„í™œì„±í™”  # export ì˜µì…˜ ì„¤ì •
            export_options = {
                'export_params': True,  # íŒŒë¼ë¯¸í„° í¬í•¨
                'opset_version': 11,  # í˜¸í™˜ì„±ì„ ìœ„í•´ ì•ˆì •ì ì¸ ë²„ì „ ì‚¬ìš©  # ONNX opset ë²„ì „
                'do_constant_folding': True,  # ìƒìˆ˜ í´ë”© í™œì„±í™”
                'input_names': ['input_sequence'],  # ì…ë ¥ ì´ë¦„
                'output_names': ['predictions'],  # ì¶œë ¥ ì´ë¦„
                'dynamic_axes': {
                    'input_sequence': {0: 'batch_size', 1: 'sequence_length'},  # ë™ì  ë°°ì¹˜/ì‹œí€€ìŠ¤
                    'predictions': {0: 'batch_size'}  # ë™ì  ë°°ì¹˜
                },
                'verbose': False  # ìƒì„¸ ì¶œë ¥ ë¹„í™œì„±í™”
            }

            # PyTorch 2.1+ì—ì„œ dynamo ë°©ì‹ ê°•ì œ ë¹„í™œì„±í™”  # ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬
            try:
                # dynamo=Falseë¥¼ ì‹œë„ (PyTorch 2.1+)
                torch.onnx.export(
                    model,  # ë³€í™˜í•  ëª¨ë¸
                    dummy_input,  # ë”ë¯¸ ì…ë ¥
                    str(onnx_path),  # ì¶œë ¥ ê²½ë¡œ
                    dynamo=False,  # ëª…ì‹œì ìœ¼ë¡œ ë ˆê±°ì‹œ TorchScript ë°©ì‹ ì‚¬ìš©  # dynamo ë¹„í™œì„±í™”
                    **export_options
                )
            except TypeError:  # PyTorch 2.0 ì´í•˜
                # PyTorch 2.0 ì´í•˜ëŠ” dynamo íŒŒë¼ë¯¸í„° ì—†ìŒ
                torch.onnx.export(
                    model,
                    dummy_input,
                    str(onnx_path),
                    **export_options
                )
        logger.info("âœ… ONNX export ì„±ê³µ")  # ì„±ê³µ ë¡œê·¸
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥  # ëª¨ë¸ ì •ë³´ ì €ì¥
        metadata = {
            'model_type': 'deeplog',  # ëª¨ë¸ íƒ€ì…
            'vocab_size': vocab_size,  # ì–´íœ˜ í¬ê¸°
            'seq_len': seq_len,  # í•™ìŠµ ì‹œ ì‚¬ìš©í•œ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¶Œì¥ê°’)  # ê¶Œì¥ ì‹œí€€ìŠ¤ ê¸¸ì´
            'input_shape': [1, seq_len],  # ì˜ˆì‹œ ì…ë ¥ í˜•íƒœ  # ìƒ˜í”Œ ì…ë ¥ í˜•íƒœ
            'output_shape': [1, vocab_size],  # ì¶œë ¥ í˜•íƒœ
            'input_names': ['input_sequence'],  # ì…ë ¥ ì´ë¦„
            'output_names': ['predictions'],  # ì¶œë ¥ ì´ë¦„
            'opset_version': 11,  # opset ë²„ì „
            'dynamic_axes': {
                'input_sequence': {
                    '0': 'batch_size',  # ë°°ì¹˜ ë™ì 
                    '1': 'sequence_length'  # ë™ì : ë‹¤ì–‘í•œ ê¸¸ì´ ì§€ì›  # ì‹œí€€ìŠ¤ ë™ì 
                },
                'predictions': {
                    '0': 'batch_size'  # ë°°ì¹˜ ë™ì 
                }
            },
            'notes': 'ONNX model supports dynamic sequence lengths via dynamic_axes. seq_len is recommended value from training.'  # ì£¼ì„
        }
        
        metadata_path = self.output_dir / f"{output_name}.meta.json"  # ë©”íƒ€ë°ì´í„° ê²½ë¡œ
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)  # JSON ì €ì¥
        
        # ì–´íœ˜ ì‚¬ì „ ì²˜ë¦¬  # C ì—”ì§„ìš© vocab ë³€í™˜
        vocab_output = self.output_dir / "vocab.json"  # vocab ì¶œë ¥ ê²½ë¡œ

        # vocab.json í˜•ì‹ í™•ì¸ ë° ë³€í™˜  # ë³€í™˜ ì‹¤í–‰
        vocab_for_c_engine = self._convert_vocab_for_c_engine(vocab, vocab_path)  # ë³€í™˜ ë©”ì„œë“œ í˜¸ì¶œ

        with open(vocab_output, 'w') as f:
            json.dump(vocab_for_c_engine, f, ensure_ascii=False, indent=2)  # C ì—”ì§„ìš© vocab ì €ì¥

        logger.info(f"âœ… DeepLog ë³€í™˜ ì™„ë£Œ: {onnx_path}")  # ì™„ë£Œ ë¡œê·¸
        logger.info(f"ğŸ“Š ë©”íƒ€ë°ì´í„°: {metadata_path}")
        logger.info(f"ğŸ“š ì–´íœ˜ ì‚¬ì „: {vocab_output}")

        # vocab í˜•ì‹ í™•ì¸ ë©”ì‹œì§€  # í…œí”Œë¦¿ ë¬¸ìì—´ í˜•ì‹ ê²€ì¦
        sample_value = next(iter(vocab_for_c_engine.values())) if vocab_for_c_engine else None
        if sample_value and isinstance(sample_value, str) and len(sample_value) > 10:  # í…œí”Œë¦¿ ë¬¸ìì—´ í™•ì¸
            logger.info(f"âœ… C ì—”ì§„ìš© vocab í˜•ì‹ (template strings): {len(vocab_for_c_engine)} templates")
        else:
            logger.warning(f"âš ï¸  vocabì´ ì¸ë±ìŠ¤ í˜•ì‹ì…ë‹ˆë‹¤. C ì—”ì§„ ì‚¬ìš© ì‹œ í…œí”Œë¦¿ ë¬¸ìì—´ì´ í•„ìš”í•©ë‹ˆë‹¤.")
        
        return {
            'onnx_path': str(onnx_path),  # ONNX ê²½ë¡œ
            'metadata_path': str(metadata_path),  # ë©”íƒ€ë°ì´í„° ê²½ë¡œ
            'vocab_path': str(vocab_output),  # vocab ê²½ë¡œ
            'metadata': metadata  # ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        }  # ê²°ê³¼ ë°˜í™˜
    
    def convert_mscred_to_onnx(
        self,
        model_path: str,
        output_name: str = "mscred.onnx",
        window_size: int = 50,
        feature_dim: Optional[int] = None
    ) -> Dict[str, Any]:  # MS-CREDë¥¼ ONNXë¡œ ë³€í™˜
        """
        MS-CRED ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜
        
        Args:
            model_path: PyTorch ëª¨ë¸ íŒŒì¼ ê²½ë¡œ  # ì…ë ¥ ëª¨ë¸ ê²½ë¡œ
            output_name: ì¶œë ¥ ONNX íŒŒì¼ëª…  # ì¶œë ¥ íŒŒì¼ëª…
            window_size: ìœˆë„ìš° í¬ê¸°  # ì‹œê°„ ìœˆë„ìš° í¬ê¸°
            feature_dim: í”¼ì²˜ ì°¨ì› (ìë™ ê°ì§€ ì‹œë„)  # í…œí”Œë¦¿ ê°œìˆ˜
            
        Returns:
            ë³€í™˜ ê²°ê³¼ ì •ë³´  # ONNX ê²½ë¡œ, ë©”íƒ€ë°ì´í„°
        """  # API ì„¤ëª…
        logger.info(f"ğŸ”„ MS-CRED ëª¨ë¸ ë³€í™˜ ì‹œì‘: {model_path}")  # ì‹œì‘ ë¡œê·¸

        # MS-CRED ëª¨ë¸ í´ë˜ìŠ¤ import  # ëª¨ë“ˆ import
        import sys
        from pathlib import Path
        # anomaly_log_detector íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€
        root_dir = Path(__file__).parent.parent.parent  # í”„ë¡œì íŠ¸ ë£¨íŠ¸
        if str(root_dir) not in sys.path:
            sys.path.insert(0, str(root_dir))  # ê²½ë¡œ ì¶”ê°€

        from anomaly_log_detector.mscred_model import MSCREDModel  # MS-CRED í´ë˜ìŠ¤ ì„í¬íŠ¸

        # state dict ë¡œë“œ  # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ
        state = torch.load(model_path, map_location='cpu')  # CPUì—ì„œ ë¡œë“œ

        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ  # ì²´í¬í¬ì¸íŠ¸ì—ì„œ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        # MS-CREDëŠ” 'model_state_dict' í‚¤ë¡œ ì €ì¥ë¨
        if isinstance(state, dict):  # ë”•ì…”ë„ˆë¦¬ í˜•ì‹ ì²´í¬
            if 'model_state_dict' in state:  # MSCREDTrainer í˜•ì‹
                # MSCREDTrainer.save_model() í˜•ì‹
                state_dict = state['model_state_dict']  # ìƒíƒœ ë”•ì…”ë„ˆë¦¬ ì¶”ì¶œ
                saved_feature_dim = state.get('feature_dim', feature_dim)  # í”¼ì²˜ ì°¨ì›
                saved_window_size = state.get('window_size', window_size)  # ìœˆë„ìš° í¬ê¸°
            elif 'state_dict' in state:  # ë‹¤ë¥¸ í˜•ì‹
                # ë‹¤ë¥¸ ì €ì¥ í˜•ì‹
                state_dict = state['state_dict']  # ìƒíƒœ ë”•ì…”ë„ˆë¦¬
                saved_feature_dim = state.get('feature_dim', feature_dim)  # í”¼ì²˜ ì°¨ì›
                saved_window_size = state.get('window_size', window_size)  # ìœˆë„ìš° í¬ê¸°
            else:
                # state_dictë§Œ ìˆëŠ” ê²½ìš° (êµ¬ë²„ì „)  # êµ¬ë²„ì „ ì²˜ë¦¬
                state_dict = state  # ì „ì²´ state ì‚¬ìš©
                saved_feature_dim = feature_dim  # íŒŒë¼ë¯¸í„° ì‚¬ìš©
                saved_window_size = window_size
        else:
            state_dict = state  # state ìì²´ê°€ ë”•ì…”ë„ˆë¦¬
            saved_feature_dim = feature_dim
            saved_window_size = window_size

        # ëª¨ë¸ ìƒì„±  # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        # MSCREDModelì˜ íŒŒë¼ë¯¸í„°: input_channels, base_channels
        # feature_dimì€ ONNX exportì‹œ ì…ë ¥ í¬ê¸° ê²°ì •ì—ë§Œ ì‚¬ìš©
        try:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ëª¨ë¸ ìƒì„± (input_channels=1, base_channels=32)  # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
            model = MSCREDModel(input_channels=1, base_channels=32)  # ëª¨ë¸ ìƒì„±
            model.load_state_dict(state_dict)  # ê°€ì¤‘ì¹˜ ë¡œë“œ
            model.eval()  # í‰ê°€ ëª¨ë“œ
        except Exception as e:
            logger.warning(f"MSCREDModel ë¡œë”© ì‹¤íŒ¨, ì¬ì‹œë„: {e}")  # ì¬ì‹œë„ ë¡œê·¸
            # state_dict í‚¤ë¥¼ í™•ì¸í•˜ì—¬ íŒŒë¼ë¯¸í„° ì¶”ì •  # íŒŒë¼ë¯¸í„° ìë™ ì¶”ì •
            try:
                # state dictì—ì„œ ì²« Conv ë ˆì´ì–´ì˜ in_channels ì¶”ì¶œ ì‹œë„  # ì±„ë„ ìˆ˜ ì¶”ì •
                first_conv_key = [k for k in state_dict.keys() if 'encoder' in k and 'conv' in k and 'weight' in k][0]  # ì²« Conv ë ˆì´ì–´ ì°¾ê¸°
                in_channels = state_dict[first_conv_key].shape[1]  # ì…ë ¥ ì±„ë„ ìˆ˜ ì¶”ì¶œ
                model = MSCREDModel(input_channels=in_channels, base_channels=32)  # ì¶”ì • íŒŒë¼ë¯¸í„°ë¡œ ìƒì„±
                model.load_state_dict(state_dict)  # ê°€ì¤‘ì¹˜ ë¡œë“œ
                model.eval()  # í‰ê°€ ëª¨ë“œ
            except:
                # ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ state ìì²´ê°€ ëª¨ë¸ì¼ ìˆ˜ ìˆìŒ  # ëª¨ë¸ ê°ì²´ì¸ì§€ í™•ì¸
                if hasattr(state, 'eval'):  # eval ë©”ì„œë“œ ì¡´ì¬ í™•ì¸
                    model = state  # stateë¥¼ ëª¨ë¸ë¡œ ì‚¬ìš©
                    model.eval()  # í‰ê°€ ëª¨ë“œ
                else:
                    raise RuntimeError(f"MSCREDModel ë¡œë”© ì‹¤íŒ¨: {e}")  # ì—ëŸ¬ ë°œìƒ
        
        # í”¼ì²˜ ì°¨ì› ê²°ì •  # í…œí”Œë¦¿ ê°œìˆ˜ ê²°ì •
        # MS-CREDì˜ feature_dimì€ í…œí”Œë¦¿ ê°œìˆ˜ (width)ë¥¼ ì˜ë¯¸
        if feature_dim is None:  # íŒŒë¼ë¯¸í„°ë¡œ ì§€ì • ì•ˆë¨
            # saved stateì—ì„œ í™•ì¸  # ì €ì¥ëœ ê°’ í™•ì¸
            if saved_feature_dim and saved_feature_dim > 1:  # ì €ì¥ëœ ê°’ ìœ íš¨
                feature_dim = saved_feature_dim  # ì €ì¥ëœ ê°’ ì‚¬ìš©
            else:
                # ê¸°ë³¸ê°’: ì¼ë°˜ì ì¸ ë¡œê·¸ í…œí”Œë¦¿ ê°œìˆ˜  # ê¸°ë³¸ê°’ ì‚¬ìš©
                # ë„ˆë¬´ ì‘ìœ¼ë©´ conv ê³„ì‚°ì´ ì‹¤íŒ¨í•˜ë¯€ë¡œ ì¶©ë¶„íˆ í° ê°’ ì‚¬ìš©
                feature_dim = 100  # ê¸°ë³¸ í…œí”Œë¦¿ ê°œìˆ˜
                logger.warning(f"í”¼ì²˜ ì°¨ì›ì„ ëª…ì‹œí•˜ì§€ ì•ŠìŒ. ê¸°ë³¸ê°’ ì‚¬ìš©: {feature_dim}")
                logger.warning(f"ì‹¤ì œ ì‚¬ìš©í•œ í…œí”Œë¦¿ ê°œìˆ˜ì™€ ë§ì§€ ì•Šìœ¼ë©´ --feature-dim ì˜µì…˜ìœ¼ë¡œ ì§€ì •í•˜ì„¸ìš”.")

        # ìµœì†Œ í¬ê¸° ê²€ì¦ (conv ë ˆì´ì–´ë¥¼ í†µê³¼í•˜ê¸° ìœ„í•œ ìµœì†Œ í¬ê¸°)  # ìµœì†Œ í¬ê¸° ì²´í¬
        # MultiScaleConvBlockì˜ ìµœëŒ€ kernel_size=7, padding=3ì´ë¯€ë¡œ
        # ìµœì†Œ feature_dim >= 7 í•„ìš”
        if feature_dim < 10:  # ìµœì†Œ í¬ê¸° ë¯¸ë‹¬
            logger.warning(f"feature_dim({feature_dim})ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ìµœì†Œ 10ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            feature_dim = 10  # ìµœì†Œê°’ìœ¼ë¡œ ì„¤ì •

        # ë”ë¯¸ ì…ë ¥ ìƒì„±  # ONNX exportìš© ìƒ˜í”Œ ì…ë ¥
        # MSCREDModelì€ 4D í…ì„œ (batch, channels, height, width) ê¸°ëŒ€
        # - batch: 1
        # - channels: 1 (input_channels)
        # - height: window_size (ì‹œê°„ ì¶•)
        # - width: feature_dim (í…œí”Œë¦¿ ìˆ˜)
        logger.info(f"MSCRED ë”ë¯¸ ì…ë ¥ í¬ê¸°: (1, 1, {window_size}, {feature_dim})")
        dummy_input = torch.randn(1, 1, window_size, feature_dim)  # ëœë¤ 4D í…ì„œ
        
        # ONNX ë³€í™˜  # ONNX íŒŒì¼ ê²½ë¡œ
        onnx_path = self.output_dir / output_name  # ì¶œë ¥ ê²½ë¡œ

        # ë¶ˆí•„ìš”í•œ ê²½ê³  ì–µì œ  # ê²½ê³  í•„í„°ë§
        import warnings
        with warnings.catch_warnings():  # ê²½ê³  ì»¨í…ìŠ¤íŠ¸
            warnings.filterwarnings("ignore", category=UserWarning)  # ì‚¬ìš©ì ê²½ê³  ë¬´ì‹œ
            warnings.filterwarnings("ignore", category=DeprecationWarning)  # íì§€ ê²½ê³  ë¬´ì‹œ
            warnings.filterwarnings("ignore", category=FutureWarning)  # ë¯¸ë˜ ê²½ê³  ë¬´ì‹œ

            # PyTorch 2.0+ í˜¸í™˜ì„±: dynamo ë°©ì‹ ëª…ì‹œì  ë¹„í™œì„±í™”  # export ì˜µì…˜ ì„¤ì •
            export_options = {
                'export_params': True,  # íŒŒë¼ë¯¸í„° í¬í•¨
                'opset_version': 11,  # opset ë²„ì „
                'do_constant_folding': True,  # ìƒìˆ˜ í´ë”©
                'input_names': ['input_features'],  # ì…ë ¥ ì´ë¦„
                'output_names': ['reconstructed'],  # ì¶œë ¥ ì´ë¦„
                'dynamic_axes': {
                    'input_features': {0: 'batch_size'},  # ë™ì  ë°°ì¹˜
                    'reconstructed': {0: 'batch_size'}  # ë™ì  ë°°ì¹˜
                },
                'verbose': False  # ìƒì„¸ ì¶œë ¥ ë¹„í™œì„±í™”
            }

            # PyTorch 2.1+ì—ì„œ dynamo ë°©ì‹ ê°•ì œ ë¹„í™œì„±í™”  # ë²„ì „ í˜¸í™˜ì„± ì²˜ë¦¬
            try:
                torch.onnx.export(
                    model,  # ë³€í™˜í•  ëª¨ë¸
                    dummy_input,  # ë”ë¯¸ ì…ë ¥
                    str(onnx_path),  # ì¶œë ¥ ê²½ë¡œ
                    dynamo=False,  # ëª…ì‹œì ìœ¼ë¡œ ë ˆê±°ì‹œ TorchScript ë°©ì‹ ì‚¬ìš©  # dynamo ë¹„í™œì„±í™”
                    **export_options
                )
            except TypeError:  # PyTorch 2.0 ì´í•˜
                # PyTorch 2.0 ì´í•˜ëŠ” dynamo íŒŒë¼ë¯¸í„° ì—†ìŒ
                torch.onnx.export(
                    model,
                    dummy_input,
                    str(onnx_path),
                    **export_options
                )
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥  # ëª¨ë¸ ì •ë³´ ì €ì¥
        metadata = {
            'model_type': 'mscred',  # ëª¨ë¸ íƒ€ì…
            'window_size': window_size,  # ìœˆë„ìš° í¬ê¸°
            'feature_dim': feature_dim,  # í”¼ì²˜ ì°¨ì›
            'input_shape': [1, 1, window_size, feature_dim],  # (batch, channels, height, width)  # ì…ë ¥ í˜•íƒœ
            'output_shape': [1, 1, window_size, feature_dim],  # ì¶œë ¥ í˜•íƒœ
            'input_names': ['input_features'],  # ì…ë ¥ ì´ë¦„
            'output_names': ['reconstructed'],  # ì¶œë ¥ ì´ë¦„
            'opset_version': 11  # opset ë²„ì „
        }
        
        metadata_path = self.output_dir / f"{output_name}.meta.json"  # ë©”íƒ€ë°ì´í„° ê²½ë¡œ
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)  # JSON ì €ì¥
        
        logger.info(f"âœ… MS-CRED ë³€í™˜ ì™„ë£Œ: {onnx_path}")  # ì™„ë£Œ ë¡œê·¸
        logger.info(f"ğŸ“Š ë©”íƒ€ë°ì´í„°: {metadata_path}")
        
        return {
            'onnx_path': str(onnx_path),  # ONNX ê²½ë¡œ
            'metadata_path': str(metadata_path),  # ë©”íƒ€ë°ì´í„° ê²½ë¡œ
            'metadata': metadata  # ë©”íƒ€ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        }  # ê²°ê³¼ ë°˜í™˜
    
    def validate_onnx_model(self, onnx_path: str) -> bool:  # ONNX ëª¨ë¸ ê²€ì¦
        """
        ONNX ëª¨ë¸ì˜ ìœ íš¨ì„± ê²€ì¦
        
        Args:
            onnx_path: ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ  # ì…ë ¥ ê²½ë¡œ
            
        Returns:
            ìœ íš¨ì„± ê²€ì¦ ê²°ê³¼  # ê²€ì¦ ì„±ê³µ ì—¬ë¶€
        """  # API ì„¤ëª…
        try:
            import onnx  # ONNX ë¼ì´ë¸ŒëŸ¬ë¦¬
            import onnxruntime as ort  # ONNX Runtime
            
            # ONNX ëª¨ë¸ ë¡œë“œ ë° ê²€ì¦  # ëª¨ë¸ ë¡œë“œ
            model = onnx.load(onnx_path)  # ëª¨ë¸ íŒŒì¼ ë¡œë“œ
            onnx.checker.check_model(model)  # ëª¨ë¸ ìœ íš¨ì„± ê²€ì‚¬
            
            # ONNX Runtimeìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥ì„± í™•ì¸  # ì‹¤í–‰ ê°€ëŠ¥ì„± ì²´í¬
            session = ort.InferenceSession(onnx_path)  # ì¶”ë¡  ì„¸ì…˜ ìƒì„±
            
            logger.info(f"âœ… ONNX ëª¨ë¸ ê²€ì¦ ì„±ê³µ: {onnx_path}")  # ì„±ê³µ ë¡œê·¸
            logger.info(f"ğŸ“Š ì…ë ¥: {[input.name for input in session.get_inputs()]}")  # ì…ë ¥ ì •ë³´
            logger.info(f"ğŸ“Š ì¶œë ¥: {[output.name for output in session.get_outputs()]}")  # ì¶œë ¥ ì •ë³´
            
            return True  # ê²€ì¦ ì„±ê³µ
            
        except Exception as e:
            logger.error(f"âŒ ONNX ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")  # ì‹¤íŒ¨ ë¡œê·¸
            return False  # ê²€ì¦ ì‹¤íŒ¨
    
    def optimize_onnx_model(self, onnx_path: str, portable: bool = False) -> str:  # ONNX ëª¨ë¸ ìµœì í™”
        """
        ONNX ëª¨ë¸ ìµœì í™” (ê·¸ë˜í”„ ìµœì í™”, ìƒìˆ˜ í´ë”© ë“±)

        Args:
            onnx_path: ì›ë³¸ ONNX ëª¨ë¸ ê²½ë¡œ  # ì…ë ¥ ê²½ë¡œ
            portable: Trueì´ë©´ ë²”ìš© ìµœì í™”ë§Œ ì ìš© (í•˜ë“œì›¨ì–´ íŠ¹í™” ìµœì í™” ì œì™¸)  # ë²”ìš©/íŠ¹í™” ì„ íƒ
                     Falseì´ë©´ ìµœëŒ€ ìµœì í™” ì ìš© (í˜„ì¬ í•˜ë“œì›¨ì–´ì— íŠ¹í™”)

        Returns:
            ìµœì í™”ëœ ëª¨ë¸ ê²½ë¡œ  # ìµœì í™”ëœ íŒŒì¼ ê²½ë¡œ
        """  # API ì„¤ëª…
        try:
            import onnxruntime as ort  # ONNX Runtime

            # ìµœì í™” ì„¤ì •  # ì„¸ì…˜ ì˜µì…˜ ì„¤ì •
            sess_options = ort.SessionOptions()  # ì„¸ì…˜ ì˜µì…˜ ìƒì„±

            if portable:  # ë²”ìš© ëª¨ë“œ
                # ë²”ìš© ìµœì í™”: ëª¨ë“  í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥  # ë²”ìš© ìµœì í™” ì ìš©
                # ORT_ENABLE_BASIC: ê¸°ë³¸ ê·¸ë˜í”„ ìµœì í™”ë§Œ ì ìš©
                sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC  # ê¸°ë³¸ ìµœì í™”
                suffix = '_portable'  # íŒŒì¼ëª… ì ‘ë¯¸ì‚¬
                logger.info("ğŸŒ ë²”ìš© ìµœì í™” ëª¨ë“œ (ëª¨ë“  í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)")
            else:  # ìµœëŒ€ ìµœì í™” ëª¨ë“œ
                # ìµœëŒ€ ìµœì í™”: í˜„ì¬ í•˜ë“œì›¨ì–´ì— íŠ¹í™”  # í•˜ë“œì›¨ì–´ íŠ¹í™” ìµœì í™”
                # ORT_ENABLE_ALL: í•˜ë“œì›¨ì–´ë³„ ìµœì í™” í¬í•¨
                sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL  # ìµœëŒ€ ìµœì í™”
                suffix = '_optimized'  # íŒŒì¼ëª… ì ‘ë¯¸ì‚¬
                logger.info("âš¡ ìµœëŒ€ ìµœì í™” ëª¨ë“œ (í˜„ì¬ í™˜ê²½ì— íŠ¹í™”)")

            sess_options.optimized_model_filepath = onnx_path.replace('.onnx', f'{suffix}.onnx')  # ìµœì í™” íŒŒì¼ ê²½ë¡œ

            # ì„¸ì…˜ ìƒì„± ì‹œ ìë™ìœ¼ë¡œ ìµœì í™”ëœ ëª¨ë¸ ì €ì¥  # ìµœì í™” ì‹¤í–‰
            session = ort.InferenceSession(onnx_path, sess_options)  # ì„¸ì…˜ ìƒì„± ë° ìµœì í™”

            optimized_path = sess_options.optimized_model_filepath  # ìµœì í™” íŒŒì¼ ê²½ë¡œ

            # ìµœì í™” íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸  # íŒŒì¼ ì¡´ì¬ í™•ì¸
            import os
            if os.path.exists(optimized_path):  # íŒŒì¼ ì¡´ì¬
                logger.info(f"âœ… ONNX ëª¨ë¸ ìµœì í™” ì™„ë£Œ: {optimized_path}")
                return optimized_path  # ìµœì í™” íŒŒì¼ ê²½ë¡œ ë°˜í™˜
            else:
                # ìµœì í™”ê°€ ì ìš©ë˜ì—ˆì§€ë§Œ íŒŒì¼ë¡œ ì €ì¥ë˜ì§€ ì•ŠìŒ (ë©”ëª¨ë¦¬ ë‚´ ìµœì í™”)  # ë©”ëª¨ë¦¬ ìµœì í™”
                logger.info(f"âš¡ ONNX ëª¨ë¸ ìµœì í™” ì ìš©ë¨ (ë©”ëª¨ë¦¬ ë‚´): {onnx_path}")
                return onnx_path  # ì›ë³¸ ê²½ë¡œ ë°˜í™˜

        except Exception as e:
            logger.error(f"âŒ ONNX ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")  # ì‹¤íŒ¨ ë¡œê·¸
            return onnx_path  # ì›ë³¸ ê²½ë¡œ ë°˜í™˜


def convert_all_models(
    deeplog_model: str,
    mscred_model: str,
    vocab_path: str,
    output_dir: str = "models/onnx",
    seq_len: Optional[int] = None,
    feature_dim: Optional[int] = None,
    portable: bool = False
) -> Dict[str, Any]:  # ëª¨ë“  ëª¨ë¸ ì¼ê´„ ë³€í™˜
    """
    ëª¨ë“  ëª¨ë¸ì„ ì¼ê´„ ë³€í™˜

    Args:
        deeplog_model: DeepLog ëª¨ë¸ ê²½ë¡œ  # ì…ë ¥ ëª¨ë¸ ê²½ë¡œ
        mscred_model: MS-CRED ëª¨ë¸ ê²½ë¡œ  # ì…ë ¥ ëª¨ë¸ ê²½ë¡œ
        vocab_path: ì–´íœ˜ ì‚¬ì „ ê²½ë¡œ  # vocab.json ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬  # ì¶œë ¥ í´ë”
        seq_len: DeepLog ì‹œí€€ìŠ¤ ê¸¸ì´ (Noneì´ë©´ ëª¨ë¸ ì €ì¥ê°’ ì‚¬ìš©)  # ì‹œí€€ìŠ¤ ê¸¸ì´
        feature_dim: MS-CRED í”¼ì²˜ ì°¨ì› (í…œí”Œë¦¿ ê°œìˆ˜, Noneì´ë©´ ìë™ ê°ì§€)  # í…œí”Œë¦¿ ê°œìˆ˜
        portable: Trueì´ë©´ ë²”ìš© ìµœì í™” (ëª¨ë“  í™˜ê²½), Falseì´ë©´ ìµœëŒ€ ìµœì í™” (í˜„ì¬ í•˜ë“œì›¨ì–´)  # ìµœì í™” ëª¨ë“œ

    Returns:
        ë³€í™˜ ê²°ê³¼ ìš”ì•½  # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """  # API ì„¤ëª…
    converter = ModelConverter(output_dir)  # ë³€í™˜ê¸° ìƒì„±
    results = {}  # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬

    # DeepLog ë³€í™˜  # DeepLog ëª¨ë¸ ì²˜ë¦¬
    if os.path.exists(deeplog_model):  # íŒŒì¼ ì¡´ì¬ í™•ì¸
        try:
            deeplog_result = converter.convert_deeplog_to_onnx(
                deeplog_model, vocab_path, seq_len=seq_len  # DeepLog ë³€í™˜ í˜¸ì¶œ
            )

            # ê²€ì¦ ë° ìµœì í™”  # ONNX ê²€ì¦ ë° ìµœì í™”
            if converter.validate_onnx_model(deeplog_result['onnx_path']):  # ê²€ì¦ ì„±ê³µ
                optimized_path = converter.optimize_onnx_model(
                    deeplog_result['onnx_path'],  # ìµœì í™” ì‹¤í–‰
                    portable=portable
                )
                deeplog_result['optimized_path'] = optimized_path  # ìµœì í™” ê²½ë¡œ ì¶”ê°€

            results['deeplog'] = deeplog_result  # ê²°ê³¼ ì €ì¥

        except Exception as e:
            logger.error(f"âŒ DeepLog ë³€í™˜ ì‹¤íŒ¨: {e}")  # ì—ëŸ¬ ë¡œê·¸
            results['deeplog'] = {'error': str(e)}  # ì—ëŸ¬ ì €ì¥

    # MS-CRED ë³€í™˜  # MS-CRED ëª¨ë¸ ì²˜ë¦¬
    if os.path.exists(mscred_model):  # íŒŒì¼ ì¡´ì¬ í™•ì¸
        try:
            mscred_result = converter.convert_mscred_to_onnx(
                mscred_model,  # MS-CRED ë³€í™˜ í˜¸ì¶œ
                feature_dim=feature_dim
            )

            # ê²€ì¦ ë° ìµœì í™”  # ONNX ê²€ì¦ ë° ìµœì í™”
            if converter.validate_onnx_model(mscred_result['onnx_path']):  # ê²€ì¦ ì„±ê³µ
                optimized_path = converter.optimize_onnx_model(
                    mscred_result['onnx_path'],  # ìµœì í™” ì‹¤í–‰
                    portable=portable
                )
                mscred_result['optimized_path'] = optimized_path  # ìµœì í™” ê²½ë¡œ ì¶”ê°€

            results['mscred'] = mscred_result  # ê²°ê³¼ ì €ì¥

        except Exception as e:
            logger.error(f"âŒ MS-CRED ë³€í™˜ ì‹¤íŒ¨: {e}")  # ì—ëŸ¬ ë¡œê·¸
            results['mscred'] = {'error': str(e)}  # ì—ëŸ¬ ì €ì¥
    
    # ë³€í™˜ ìš”ì•½ ì €ì¥  # ê²°ê³¼ ìš”ì•½ ì €ì¥
    summary_path = Path(output_dir) / "conversion_summary.json"  # ìš”ì•½ íŒŒì¼ ê²½ë¡œ
    with open(summary_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)  # JSON ì €ì¥
    
    logger.info(f"ğŸ“‹ ë³€í™˜ ìš”ì•½ ì €ì¥: {summary_path}")  # ì €ì¥ ë¡œê·¸
    
    return results  # ê²°ê³¼ ë°˜í™˜


if __name__ == "__main__":  # ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰
    import argparse  # ì¸ì íŒŒì‹±
    
    parser = argparse.ArgumentParser(description="PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜")  # íŒŒì„œ ìƒì„±
    parser.add_argument("--deeplog-model", type=str, help="DeepLog ëª¨ë¸ ê²½ë¡œ")  # DeepLog ì¸ì
    parser.add_argument("--mscred-model", type=str, help="MS-CRED ëª¨ë¸ ê²½ë¡œ")  # MS-CRED ì¸ì
    parser.add_argument("--vocab", type=str, help="ì–´íœ˜ ì‚¬ì „ ê²½ë¡œ")  # vocab ì¸ì
    parser.add_argument("--output-dir", type=str, default="models/onnx", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")  # ì¶œë ¥ ì¸ì
    parser.add_argument("--validate", action="store_true", help="ë³€í™˜ í›„ ê²€ì¦ ì‹¤í–‰")  # ê²€ì¦ ì¸ì
    
    args = parser.parse_args()  # ì¸ì íŒŒì‹±
    
    if not args.deeplog_model and not args.mscred_model:  # ëª¨ë¸ ê²½ë¡œ ì—†ìŒ
        print("âŒ ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")  # ì—ëŸ¬ ë©”ì‹œì§€
        parser.print_help()  # ë„ì›€ë§ ì¶œë ¥
        exit(1)  # ì¢…ë£Œ
    
    # ë³€í™˜ ì‹¤í–‰  # ë³€í™˜ í˜¸ì¶œ
    results = convert_all_models(
        args.deeplog_model or "",  # DeepLog ê²½ë¡œ
        args.mscred_model or "",  # MS-CRED ê²½ë¡œ
        args.vocab or "",  # vocab ê²½ë¡œ
        args.output_dir  # ì¶œë ¥ ë””ë ‰í† ë¦¬
    )
    
    # ê²°ê³¼ ì¶œë ¥  # ê²°ê³¼ í‘œì‹œ
    print("\nğŸ‰ ëª¨ë¸ ë³€í™˜ ì™„ë£Œ!")  # ì™„ë£Œ ë©”ì‹œì§€
    for model_name, result in results.items():  # ê° ëª¨ë¸ ê²°ê³¼
        if 'error' in result:  # ì—ëŸ¬ ìˆìŒ
            print(f"âŒ {model_name}: {result['error']}")  # ì—ëŸ¬ ì¶œë ¥
        else:  # ì„±ê³µ
            print(f"âœ… {model_name}: {result['onnx_path']}")  # ê²½ë¡œ ì¶œë ¥
            if 'optimized_path' in result:  # ìµœì í™”ë¨
                print(f"âš¡ ìµœì í™”ë¨: {result['optimized_path']}")  # ìµœì í™” ê²½ë¡œ ì¶œë ¥
