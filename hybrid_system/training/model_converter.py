#!/usr/bin/env python3
"""
PyTorch ëª¨ë¸ì„ ONNX í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬
í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸: Python í•™ìŠµ â†’ C ì¶”ë¡  ë¸Œë¦¬ì§€
"""

import os
import json
import torch
import torch.onnx
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelConverter:
    """PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = "models/onnx"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def convert_deeplog_to_onnx(
        self,
        model_path: str,
        vocab_path: str,
        output_name: str = "deeplog.onnx",
        seq_len: int = 50
    ) -> Dict[str, Any]:
        """
        DeepLog ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜

        Args:
            model_path: PyTorch ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            vocab_path: ì–´íœ˜ ì‚¬ì „ íŒŒì¼ ê²½ë¡œ
            output_name: ì¶œë ¥ ONNX íŒŒì¼ëª…
            seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´

        Returns:
            ë³€í™˜ ê²°ê³¼ ì •ë³´
        """
        logger.info(f"ğŸ”„ DeepLog ëª¨ë¸ ë³€í™˜ ì‹œì‘: {model_path}")

        # ì–´íœ˜ ì‚¬ì „ ë¡œë“œ
        with open(vocab_path, 'r') as f:
            vocab = json.load(f)
        vocab_size = len(vocab)

        # PyTorch ëª¨ë¸ ë¡œë“œ
        # DeepLog ëª¨ë¸ í´ë˜ìŠ¤ import
        import sys
        from pathlib import Path
        # anomaly_log_detector íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€
        root_dir = Path(__file__).parent.parent.parent
        if str(root_dir) not in sys.path:
            sys.path.insert(0, str(root_dir))

        from anomaly_log_detector.builders.deeplog import DeepLogLSTM

        # state dict ë¡œë“œ
        state = torch.load(model_path, map_location='cpu')
        model_vocab_size = int(state.get("vocab_size", vocab_size))
        model_seq_len = int(state.get("seq_len", seq_len))

        # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        model = DeepLogLSTM(vocab_size=model_vocab_size)
        model.load_state_dict(state["state_dict"])
        model.eval()

        # seq_len ì—…ë°ì´íŠ¸ (ëª¨ë¸ì— ì €ì¥ëœ ê°’ ì‚¬ìš©)
        seq_len = model_seq_len
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„± (ë°°ì¹˜ í¬ê¸° 1, ì‹œí€€ìŠ¤ ê¸¸ì´)
        dummy_input = torch.randint(0, vocab_size, (1, seq_len), dtype=torch.long)

        # ONNX ë³€í™˜
        onnx_path = self.output_dir / output_name

        # LSTM ê²½ê³  ì–µì œ
        import warnings
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning, message=".*LSTM.*")

            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=11,  # í˜¸í™˜ì„±ì„ ìœ„í•´ ì•ˆì •ì ì¸ ë²„ì „ ì‚¬ìš©
                do_constant_folding=True,
                input_names=['input_sequence'],
                output_names=['predictions'],
                dynamic_axes={
                    'input_sequence': {0: 'batch_size', 1: 'sequence_length'},
                    'predictions': {0: 'batch_size'}
                },
                verbose=False
            )
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'model_type': 'deeplog',
            'vocab_size': vocab_size,
            'seq_len': seq_len,
            'input_shape': [1, seq_len],
            'output_shape': [1, vocab_size],
            'input_names': ['input_sequence'],
            'output_names': ['predictions'],
            'opset_version': 11
        }
        
        metadata_path = self.output_dir / f"{output_name}.meta.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # ì–´íœ˜ ì‚¬ì „ë„ í•¨ê»˜ ë³µì‚¬
        vocab_output = self.output_dir / "vocab.json"
        with open(vocab_output, 'w') as f:
            json.dump(vocab, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… DeepLog ë³€í™˜ ì™„ë£Œ: {onnx_path}")
        logger.info(f"ğŸ“Š ë©”íƒ€ë°ì´í„°: {metadata_path}")
        logger.info(f"ğŸ“š ì–´íœ˜ ì‚¬ì „: {vocab_output}")
        
        return {
            'onnx_path': str(onnx_path),
            'metadata_path': str(metadata_path),
            'vocab_path': str(vocab_output),
            'metadata': metadata
        }
    
    def convert_mscred_to_onnx(
        self,
        model_path: str,
        output_name: str = "mscred.onnx",
        window_size: int = 50,
        feature_dim: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        MS-CRED ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜
        
        Args:
            model_path: PyTorch ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            output_name: ì¶œë ¥ ONNX íŒŒì¼ëª…
            window_size: ìœˆë„ìš° í¬ê¸°
            feature_dim: í”¼ì²˜ ì°¨ì› (ìë™ ê°ì§€ ì‹œë„)
            
        Returns:
            ë³€í™˜ ê²°ê³¼ ì •ë³´
        """
        logger.info(f"ğŸ”„ MS-CRED ëª¨ë¸ ë³€í™˜ ì‹œì‘: {model_path}")

        # MS-CRED ëª¨ë¸ í´ë˜ìŠ¤ import
        import sys
        from pathlib import Path
        # anomaly_log_detector íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€
        root_dir = Path(__file__).parent.parent.parent
        if str(root_dir) not in sys.path:
            sys.path.insert(0, str(root_dir))

        from anomaly_log_detector.mscred_model import MSCREDModel

        # state dict ë¡œë“œ
        state = torch.load(model_path, map_location='cpu')

        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        # MS-CREDëŠ” 'model_state_dict' í‚¤ë¡œ ì €ì¥ë¨
        if isinstance(state, dict):
            if 'model_state_dict' in state:
                # MSCREDTrainer.save_model() í˜•ì‹
                state_dict = state['model_state_dict']
                saved_feature_dim = state.get('feature_dim', feature_dim)
                saved_window_size = state.get('window_size', window_size)
            elif 'state_dict' in state:
                # ë‹¤ë¥¸ ì €ì¥ í˜•ì‹
                state_dict = state['state_dict']
                saved_feature_dim = state.get('feature_dim', feature_dim)
                saved_window_size = state.get('window_size', window_size)
            else:
                # state_dictë§Œ ìˆëŠ” ê²½ìš° (êµ¬ë²„ì „)
                state_dict = state
                saved_feature_dim = feature_dim
                saved_window_size = window_size
        else:
            state_dict = state
            saved_feature_dim = feature_dim
            saved_window_size = window_size

        # ëª¨ë¸ ìƒì„±
        # MSCREDModelì˜ íŒŒë¼ë¯¸í„°: input_channels, base_channels
        # feature_dimì€ ONNX exportì‹œ ì…ë ¥ í¬ê¸° ê²°ì •ì—ë§Œ ì‚¬ìš©
        try:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ëª¨ë¸ ìƒì„± (input_channels=1, base_channels=32)
            model = MSCREDModel(input_channels=1, base_channels=32)
            model.load_state_dict(state_dict)
            model.eval()
        except Exception as e:
            logger.warning(f"MSCREDModel ë¡œë”© ì‹¤íŒ¨, ì¬ì‹œë„: {e}")
            # state_dict í‚¤ë¥¼ í™•ì¸í•˜ì—¬ íŒŒë¼ë¯¸í„° ì¶”ì •
            try:
                # state dictì—ì„œ ì²« Conv ë ˆì´ì–´ì˜ in_channels ì¶”ì¶œ ì‹œë„
                first_conv_key = [k for k in state_dict.keys() if 'encoder' in k and 'conv' in k and 'weight' in k][0]
                in_channels = state_dict[first_conv_key].shape[1]
                model = MSCREDModel(input_channels=in_channels, base_channels=32)
                model.load_state_dict(state_dict)
                model.eval()
            except:
                # ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ state ìì²´ê°€ ëª¨ë¸ì¼ ìˆ˜ ìˆìŒ
                if hasattr(state, 'eval'):
                    model = state
                    model.eval()
                else:
                    raise RuntimeError(f"MSCREDModel ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # í”¼ì²˜ ì°¨ì› ê²°ì •
        # MS-CREDì˜ feature_dimì€ í…œí”Œë¦¿ ê°œìˆ˜ (width)ë¥¼ ì˜ë¯¸
        if feature_dim is None:
            # saved stateì—ì„œ í™•ì¸
            if saved_feature_dim and saved_feature_dim > 1:
                feature_dim = saved_feature_dim
            else:
                # ê¸°ë³¸ê°’: ì¼ë°˜ì ì¸ ë¡œê·¸ í…œí”Œë¦¿ ê°œìˆ˜
                # ë„ˆë¬´ ì‘ìœ¼ë©´ conv ê³„ì‚°ì´ ì‹¤íŒ¨í•˜ë¯€ë¡œ ì¶©ë¶„íˆ í° ê°’ ì‚¬ìš©
                feature_dim = 100
                logger.warning(f"í”¼ì²˜ ì°¨ì›ì„ ëª…ì‹œí•˜ì§€ ì•ŠìŒ. ê¸°ë³¸ê°’ ì‚¬ìš©: {feature_dim}")
                logger.warning(f"ì‹¤ì œ ì‚¬ìš©í•œ í…œí”Œë¦¿ ê°œìˆ˜ì™€ ë§ì§€ ì•Šìœ¼ë©´ --feature-dim ì˜µì…˜ìœ¼ë¡œ ì§€ì •í•˜ì„¸ìš”.")

        # ìµœì†Œ í¬ê¸° ê²€ì¦ (conv ë ˆì´ì–´ë¥¼ í†µê³¼í•˜ê¸° ìœ„í•œ ìµœì†Œ í¬ê¸°)
        # MultiScaleConvBlockì˜ ìµœëŒ€ kernel_size=7, padding=3ì´ë¯€ë¡œ
        # ìµœì†Œ feature_dim >= 7 í•„ìš”
        if feature_dim < 10:
            logger.warning(f"feature_dim({feature_dim})ì´ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤. ìµœì†Œ 10ìœ¼ë¡œ ì„¤ì •í•©ë‹ˆë‹¤.")
            feature_dim = 10

        # ë”ë¯¸ ì…ë ¥ ìƒì„±
        # MSCREDModelì€ 4D í…ì„œ (batch, channels, height, width) ê¸°ëŒ€
        # - batch: 1
        # - channels: 1 (input_channels)
        # - height: window_size (ì‹œê°„ ì¶•)
        # - width: feature_dim (í…œí”Œë¦¿ ìˆ˜)
        logger.info(f"MSCRED ë”ë¯¸ ì…ë ¥ í¬ê¸°: (1, 1, {window_size}, {feature_dim})")
        dummy_input = torch.randn(1, 1, window_size, feature_dim)
        
        # ONNX ë³€í™˜
        onnx_path = self.output_dir / output_name

        # ë¶ˆí•„ìš”í•œ ê²½ê³  ì–µì œ
        import warnings
        with warnings.catch_warnings():
            warnings.filterwarnings("ignore", category=UserWarning)

            torch.onnx.export(
                model,
                dummy_input,
                str(onnx_path),
                export_params=True,
                opset_version=11,
                do_constant_folding=True,
                input_names=['input_features'],
                output_names=['reconstructed'],
                dynamic_axes={
                    'input_features': {0: 'batch_size'},
                    'reconstructed': {0: 'batch_size'}
                },
                verbose=False
            )
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'model_type': 'mscred',
            'window_size': window_size,
            'feature_dim': feature_dim,
            'input_shape': [1, 1, window_size, feature_dim],  # (batch, channels, height, width)
            'output_shape': [1, 1, window_size, feature_dim],
            'input_names': ['input_features'],
            'output_names': ['reconstructed'],
            'opset_version': 11
        }
        
        metadata_path = self.output_dir / f"{output_name}.meta.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"âœ… MS-CRED ë³€í™˜ ì™„ë£Œ: {onnx_path}")
        logger.info(f"ğŸ“Š ë©”íƒ€ë°ì´í„°: {metadata_path}")
        
        return {
            'onnx_path': str(onnx_path),
            'metadata_path': str(metadata_path),
            'metadata': metadata
        }
    
    def validate_onnx_model(self, onnx_path: str) -> bool:
        """
        ONNX ëª¨ë¸ì˜ ìœ íš¨ì„± ê²€ì¦
        
        Args:
            onnx_path: ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ìœ íš¨ì„± ê²€ì¦ ê²°ê³¼
        """
        try:
            import onnx
            import onnxruntime as ort
            
            # ONNX ëª¨ë¸ ë¡œë“œ ë° ê²€ì¦
            model = onnx.load(onnx_path)
            onnx.checker.check_model(model)
            
            # ONNX Runtimeìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥ì„± í™•ì¸
            session = ort.InferenceSession(onnx_path)
            
            logger.info(f"âœ… ONNX ëª¨ë¸ ê²€ì¦ ì„±ê³µ: {onnx_path}")
            logger.info(f"ğŸ“Š ì…ë ¥: {[input.name for input in session.get_inputs()]}")
            logger.info(f"ğŸ“Š ì¶œë ¥: {[output.name for output in session.get_outputs()]}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ONNX ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def optimize_onnx_model(self, onnx_path: str, portable: bool = False) -> str:
        """
        ONNX ëª¨ë¸ ìµœì í™” (ê·¸ë˜í”„ ìµœì í™”, ìƒìˆ˜ í´ë”© ë“±)

        Args:
            onnx_path: ì›ë³¸ ONNX ëª¨ë¸ ê²½ë¡œ
            portable: Trueì´ë©´ ë²”ìš© ìµœì í™”ë§Œ ì ìš© (í•˜ë“œì›¨ì–´ íŠ¹í™” ìµœì í™” ì œì™¸)
                     Falseì´ë©´ ìµœëŒ€ ìµœì í™” ì ìš© (í˜„ì¬ í•˜ë“œì›¨ì–´ì— íŠ¹í™”)

        Returns:
            ìµœì í™”ëœ ëª¨ë¸ ê²½ë¡œ
        """
        try:
            import onnxruntime as ort

            # ìµœì í™” ì„¤ì •
            sess_options = ort.SessionOptions()

            if portable:
                # ë²”ìš© ìµœì í™”: ëª¨ë“  í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥
                # ORT_ENABLE_BASIC: ê¸°ë³¸ ê·¸ë˜í”„ ìµœì í™”ë§Œ ì ìš©
                sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_BASIC
                suffix = '_portable'
                logger.info("ğŸŒ ë²”ìš© ìµœì í™” ëª¨ë“œ (ëª¨ë“  í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥)")
            else:
                # ìµœëŒ€ ìµœì í™”: í˜„ì¬ í•˜ë“œì›¨ì–´ì— íŠ¹í™”
                # ORT_ENABLE_ALL: í•˜ë“œì›¨ì–´ë³„ ìµœì í™” í¬í•¨
                sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
                suffix = '_optimized'
                logger.info("âš¡ ìµœëŒ€ ìµœì í™” ëª¨ë“œ (í˜„ì¬ í™˜ê²½ì— íŠ¹í™”)")

            sess_options.optimized_model_filepath = onnx_path.replace('.onnx', f'{suffix}.onnx')

            # ì„¸ì…˜ ìƒì„± ì‹œ ìë™ìœ¼ë¡œ ìµœì í™”ëœ ëª¨ë¸ ì €ì¥
            session = ort.InferenceSession(onnx_path, sess_options)

            optimized_path = sess_options.optimized_model_filepath

            # ìµœì í™” íŒŒì¼ì´ ìƒì„±ë˜ì—ˆëŠ”ì§€ í™•ì¸
            import os
            if os.path.exists(optimized_path):
                logger.info(f"âœ… ONNX ëª¨ë¸ ìµœì í™” ì™„ë£Œ: {optimized_path}")
                return optimized_path
            else:
                # ìµœì í™”ê°€ ì ìš©ë˜ì—ˆì§€ë§Œ íŒŒì¼ë¡œ ì €ì¥ë˜ì§€ ì•ŠìŒ (ë©”ëª¨ë¦¬ ë‚´ ìµœì í™”)
                logger.info(f"âš¡ ONNX ëª¨ë¸ ìµœì í™” ì ìš©ë¨ (ë©”ëª¨ë¦¬ ë‚´): {onnx_path}")
                return onnx_path

        except Exception as e:
            logger.error(f"âŒ ONNX ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return onnx_path


def convert_all_models(
    deeplog_model: str,
    mscred_model: str,
    vocab_path: str,
    output_dir: str = "models/onnx",
    feature_dim: Optional[int] = None,
    portable: bool = False
) -> Dict[str, Any]:
    """
    ëª¨ë“  ëª¨ë¸ì„ ì¼ê´„ ë³€í™˜

    Args:
        deeplog_model: DeepLog ëª¨ë¸ ê²½ë¡œ
        mscred_model: MS-CRED ëª¨ë¸ ê²½ë¡œ
        vocab_path: ì–´íœ˜ ì‚¬ì „ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        feature_dim: MS-CRED í”¼ì²˜ ì°¨ì› (í…œí”Œë¦¿ ê°œìˆ˜, Noneì´ë©´ ìë™ ê°ì§€)
        portable: Trueì´ë©´ ë²”ìš© ìµœì í™” (ëª¨ë“  í™˜ê²½), Falseì´ë©´ ìµœëŒ€ ìµœì í™” (í˜„ì¬ í•˜ë“œì›¨ì–´)

    Returns:
        ë³€í™˜ ê²°ê³¼ ìš”ì•½
    """
    converter = ModelConverter(output_dir)
    results = {}

    # DeepLog ë³€í™˜
    if os.path.exists(deeplog_model):
        try:
            deeplog_result = converter.convert_deeplog_to_onnx(
                deeplog_model, vocab_path
            )

            # ê²€ì¦ ë° ìµœì í™”
            if converter.validate_onnx_model(deeplog_result['onnx_path']):
                optimized_path = converter.optimize_onnx_model(
                    deeplog_result['onnx_path'],
                    portable=portable
                )
                deeplog_result['optimized_path'] = optimized_path

            results['deeplog'] = deeplog_result

        except Exception as e:
            logger.error(f"âŒ DeepLog ë³€í™˜ ì‹¤íŒ¨: {e}")
            results['deeplog'] = {'error': str(e)}

    # MS-CRED ë³€í™˜
    if os.path.exists(mscred_model):
        try:
            mscred_result = converter.convert_mscred_to_onnx(
                mscred_model,
                feature_dim=feature_dim
            )

            # ê²€ì¦ ë° ìµœì í™”
            if converter.validate_onnx_model(mscred_result['onnx_path']):
                optimized_path = converter.optimize_onnx_model(
                    mscred_result['onnx_path'],
                    portable=portable
                )
                mscred_result['optimized_path'] = optimized_path

            results['mscred'] = mscred_result

        except Exception as e:
            logger.error(f"âŒ MS-CRED ë³€í™˜ ì‹¤íŒ¨: {e}")
            results['mscred'] = {'error': str(e)}
    
    # ë³€í™˜ ìš”ì•½ ì €ì¥
    summary_path = Path(output_dir) / "conversion_summary.json"
    with open(summary_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    logger.info(f"ğŸ“‹ ë³€í™˜ ìš”ì•½ ì €ì¥: {summary_path}")
    
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜")
    parser.add_argument("--deeplog-model", type=str, help="DeepLog ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--mscred-model", type=str, help="MS-CRED ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--vocab", type=str, help="ì–´íœ˜ ì‚¬ì „ ê²½ë¡œ")
    parser.add_argument("--output-dir", type=str, default="models/onnx", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--validate", action="store_true", help="ë³€í™˜ í›„ ê²€ì¦ ì‹¤í–‰")
    
    args = parser.parse_args()
    
    if not args.deeplog_model and not args.mscred_model:
        print("âŒ ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        parser.print_help()
        exit(1)
    
    # ë³€í™˜ ì‹¤í–‰
    results = convert_all_models(
        args.deeplog_model or "",
        args.mscred_model or "",
        args.vocab or "",
        args.output_dir
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ‰ ëª¨ë¸ ë³€í™˜ ì™„ë£Œ!")
    for model_name, result in results.items():
        if 'error' in result:
            print(f"âŒ {model_name}: {result['error']}")
        else:
            print(f"âœ… {model_name}: {result['onnx_path']}")
            if 'optimized_path' in result:
                print(f"âš¡ ìµœì í™”ë¨: {result['optimized_path']}")
