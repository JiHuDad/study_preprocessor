#!/usr/bin/env python3
"""
PyTorch ëª¨ë¸ì„ ONNX í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ë„êµ¬
í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œì˜ í•µì‹¬ ì»´í¬ë„ŒíŠ¸: Python í•™ìŠµ â†’ C ì¶”ë¡  ë¸Œë¦¬ì§€
"""

import os
import json
import torch
import torch.onnx
import numpy as np
from pathlib import Path
from typing import Dict, Any, Optional, Tuple
import logging

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class ModelConverter:
    """PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, output_dir: str = "models/onnx"):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
    def convert_deeplog_to_onnx(
        self,
        model_path: str,
        vocab_path: str,
        output_name: str = "deeplog.onnx",
        seq_len: int = 50
    ) -> Dict[str, Any]:
        """
        DeepLog ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜

        Args:
            model_path: PyTorch ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            vocab_path: ì–´íœ˜ ì‚¬ì „ íŒŒì¼ ê²½ë¡œ
            output_name: ì¶œë ¥ ONNX íŒŒì¼ëª…
            seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´

        Returns:
            ë³€í™˜ ê²°ê³¼ ì •ë³´
        """
        logger.info(f"ğŸ”„ DeepLog ëª¨ë¸ ë³€í™˜ ì‹œì‘: {model_path}")

        # ì–´íœ˜ ì‚¬ì „ ë¡œë“œ
        with open(vocab_path, 'r') as f:
            vocab = json.load(f)
        vocab_size = len(vocab)

        # PyTorch ëª¨ë¸ ë¡œë“œ
        # DeepLog ëª¨ë¸ í´ë˜ìŠ¤ import
        import sys
        from pathlib import Path
        # study_preprocessor íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€
        root_dir = Path(__file__).parent.parent.parent
        if str(root_dir) not in sys.path:
            sys.path.insert(0, str(root_dir))

        from study_preprocessor.builders.deeplog import DeepLogLSTM

        # state dict ë¡œë“œ
        state = torch.load(model_path, map_location='cpu')
        model_vocab_size = int(state.get("vocab_size", vocab_size))
        model_seq_len = int(state.get("seq_len", seq_len))

        # ëª¨ë¸ ìƒì„± ë° ê°€ì¤‘ì¹˜ ë¡œë“œ
        model = DeepLogLSTM(vocab_size=model_vocab_size)
        model.load_state_dict(state["state_dict"])
        model.eval()

        # seq_len ì—…ë°ì´íŠ¸ (ëª¨ë¸ì— ì €ì¥ëœ ê°’ ì‚¬ìš©)
        seq_len = model_seq_len
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„± (ë°°ì¹˜ í¬ê¸° 1, ì‹œí€€ìŠ¤ ê¸¸ì´)
        dummy_input = torch.randint(0, vocab_size, (1, seq_len), dtype=torch.long)
        
        # ONNX ë³€í™˜
        onnx_path = self.output_dir / output_name
        
        torch.onnx.export(
            model,
            dummy_input,
            str(onnx_path),
            export_params=True,
            opset_version=11,  # í˜¸í™˜ì„±ì„ ìœ„í•´ ì•ˆì •ì ì¸ ë²„ì „ ì‚¬ìš©
            do_constant_folding=True,
            input_names=['input_sequence'],
            output_names=['predictions'],
            dynamic_axes={
                'input_sequence': {0: 'batch_size'},
                'predictions': {0: 'batch_size'}
            }
        )
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'model_type': 'deeplog',
            'vocab_size': vocab_size,
            'seq_len': seq_len,
            'input_shape': [1, seq_len],
            'output_shape': [1, vocab_size],
            'input_names': ['input_sequence'],
            'output_names': ['predictions'],
            'opset_version': 11
        }
        
        metadata_path = self.output_dir / f"{output_name}.meta.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # ì–´íœ˜ ì‚¬ì „ë„ í•¨ê»˜ ë³µì‚¬
        vocab_output = self.output_dir / "vocab.json"
        with open(vocab_output, 'w') as f:
            json.dump(vocab, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… DeepLog ë³€í™˜ ì™„ë£Œ: {onnx_path}")
        logger.info(f"ğŸ“Š ë©”íƒ€ë°ì´í„°: {metadata_path}")
        logger.info(f"ğŸ“š ì–´íœ˜ ì‚¬ì „: {vocab_output}")
        
        return {
            'onnx_path': str(onnx_path),
            'metadata_path': str(metadata_path),
            'vocab_path': str(vocab_output),
            'metadata': metadata
        }
    
    def convert_mscred_to_onnx(
        self,
        model_path: str,
        output_name: str = "mscred.onnx",
        window_size: int = 50,
        feature_dim: Optional[int] = None
    ) -> Dict[str, Any]:
        """
        MS-CRED ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜
        
        Args:
            model_path: PyTorch ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            output_name: ì¶œë ¥ ONNX íŒŒì¼ëª…
            window_size: ìœˆë„ìš° í¬ê¸°
            feature_dim: í”¼ì²˜ ì°¨ì› (ìë™ ê°ì§€ ì‹œë„)
            
        Returns:
            ë³€í™˜ ê²°ê³¼ ì •ë³´
        """
        logger.info(f"ğŸ”„ MS-CRED ëª¨ë¸ ë³€í™˜ ì‹œì‘: {model_path}")

        # MS-CRED ëª¨ë¸ í´ë˜ìŠ¤ import
        import sys
        from pathlib import Path
        # study_preprocessor íŒ¨í‚¤ì§€ ê²½ë¡œ ì¶”ê°€
        root_dir = Path(__file__).parent.parent.parent
        if str(root_dir) not in sys.path:
            sys.path.insert(0, str(root_dir))

        from study_preprocessor.mscred_model import MSCREDModel

        # state dict ë¡œë“œ
        state = torch.load(model_path, map_location='cpu')

        # ëª¨ë¸ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        # MS-CREDëŠ” 'model_state_dict' í‚¤ë¡œ ì €ì¥ë¨
        if isinstance(state, dict):
            if 'model_state_dict' in state:
                # MSCREDTrainer.save_model() í˜•ì‹
                state_dict = state['model_state_dict']
                saved_feature_dim = state.get('feature_dim', feature_dim)
                saved_window_size = state.get('window_size', window_size)
            elif 'state_dict' in state:
                # ë‹¤ë¥¸ ì €ì¥ í˜•ì‹
                state_dict = state['state_dict']
                saved_feature_dim = state.get('feature_dim', feature_dim)
                saved_window_size = state.get('window_size', window_size)
            else:
                # state_dictë§Œ ìˆëŠ” ê²½ìš° (êµ¬ë²„ì „)
                state_dict = state
                saved_feature_dim = feature_dim
                saved_window_size = window_size
        else:
            state_dict = state
            saved_feature_dim = feature_dim
            saved_window_size = window_size

        # ëª¨ë¸ ìƒì„±
        # MSCREDModelì˜ íŒŒë¼ë¯¸í„°: input_channels, base_channels
        # feature_dimì€ ONNX exportì‹œ ì…ë ¥ í¬ê¸° ê²°ì •ì—ë§Œ ì‚¬ìš©
        try:
            # ê¸°ë³¸ê°’ìœ¼ë¡œ ëª¨ë¸ ìƒì„± (input_channels=1, base_channels=32)
            model = MSCREDModel(input_channels=1, base_channels=32)
            model.load_state_dict(state_dict)
            model.eval()
        except Exception as e:
            logger.warning(f"MSCREDModel ë¡œë”© ì‹¤íŒ¨, ì¬ì‹œë„: {e}")
            # state_dict í‚¤ë¥¼ í™•ì¸í•˜ì—¬ íŒŒë¼ë¯¸í„° ì¶”ì •
            try:
                # state dictì—ì„œ ì²« Conv ë ˆì´ì–´ì˜ in_channels ì¶”ì¶œ ì‹œë„
                first_conv_key = [k for k in state_dict.keys() if 'encoder' in k and 'conv' in k and 'weight' in k][0]
                in_channels = state_dict[first_conv_key].shape[1]
                model = MSCREDModel(input_channels=in_channels, base_channels=32)
                model.load_state_dict(state_dict)
                model.eval()
            except:
                # ê·¸ë˜ë„ ì‹¤íŒ¨í•˜ë©´ state ìì²´ê°€ ëª¨ë¸ì¼ ìˆ˜ ìˆìŒ
                if hasattr(state, 'eval'):
                    model = state
                    model.eval()
                else:
                    raise RuntimeError(f"MSCREDModel ë¡œë”© ì‹¤íŒ¨: {e}")
        
        # í”¼ì²˜ ì°¨ì› ìë™ ê°ì§€ (ëª¨ë¸ì˜ ì²« ë²ˆì§¸ ë ˆì´ì–´ì—ì„œ)
        if feature_dim is None:
            try:
                # ì¼ë°˜ì ì¸ MS-CRED êµ¬ì¡°ì—ì„œ ì²« ë²ˆì§¸ Conv ë ˆì´ì–´ ì°¾ê¸°
                for module in model.modules():
                    if hasattr(module, 'in_channels'):
                        feature_dim = module.in_channels
                        break
                
                if feature_dim is None:
                    feature_dim = 100  # ê¸°ë³¸ê°’
                    logger.warning(f"í”¼ì²˜ ì°¨ì› ìë™ ê°ì§€ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {feature_dim}")
            except Exception as e:
                feature_dim = 100
                logger.warning(f"í”¼ì²˜ ì°¨ì› ê°ì§€ ì˜¤ë¥˜: {e}, ê¸°ë³¸ê°’ ì‚¬ìš©: {feature_dim}")
        
        # ë”ë¯¸ ì…ë ¥ ìƒì„± (ë°°ì¹˜ í¬ê¸° 1, ìœˆë„ìš° í¬ê¸°, í”¼ì²˜ ì°¨ì›)
        dummy_input = torch.randn(1, window_size, feature_dim)
        
        # ONNX ë³€í™˜
        onnx_path = self.output_dir / output_name
        
        torch.onnx.export(
            model,
            dummy_input,
            str(onnx_path),
            export_params=True,
            opset_version=11,
            do_constant_folding=True,
            input_names=['input_features'],
            output_names=['reconstructed'],
            dynamic_axes={
                'input_features': {0: 'batch_size'},
                'reconstructed': {0: 'batch_size'}
            }
        )
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'model_type': 'mscred',
            'window_size': window_size,
            'feature_dim': feature_dim,
            'input_shape': [1, window_size, feature_dim],
            'output_shape': [1, window_size, feature_dim],
            'input_names': ['input_features'],
            'output_names': ['reconstructed'],
            'opset_version': 11
        }
        
        metadata_path = self.output_dir / f"{output_name}.meta.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info(f"âœ… MS-CRED ë³€í™˜ ì™„ë£Œ: {onnx_path}")
        logger.info(f"ğŸ“Š ë©”íƒ€ë°ì´í„°: {metadata_path}")
        
        return {
            'onnx_path': str(onnx_path),
            'metadata_path': str(metadata_path),
            'metadata': metadata
        }
    
    def validate_onnx_model(self, onnx_path: str) -> bool:
        """
        ONNX ëª¨ë¸ì˜ ìœ íš¨ì„± ê²€ì¦
        
        Args:
            onnx_path: ONNX ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
            
        Returns:
            ìœ íš¨ì„± ê²€ì¦ ê²°ê³¼
        """
        try:
            import onnx
            import onnxruntime as ort
            
            # ONNX ëª¨ë¸ ë¡œë“œ ë° ê²€ì¦
            model = onnx.load(onnx_path)
            onnx.checker.check_model(model)
            
            # ONNX Runtimeìœ¼ë¡œ ì‹¤í–‰ ê°€ëŠ¥ì„± í™•ì¸
            session = ort.InferenceSession(onnx_path)
            
            logger.info(f"âœ… ONNX ëª¨ë¸ ê²€ì¦ ì„±ê³µ: {onnx_path}")
            logger.info(f"ğŸ“Š ì…ë ¥: {[input.name for input in session.get_inputs()]}")
            logger.info(f"ğŸ“Š ì¶œë ¥: {[output.name for output in session.get_outputs()]}")
            
            return True
            
        except Exception as e:
            logger.error(f"âŒ ONNX ëª¨ë¸ ê²€ì¦ ì‹¤íŒ¨: {e}")
            return False
    
    def optimize_onnx_model(self, onnx_path: str) -> str:
        """
        ONNX ëª¨ë¸ ìµœì í™” (ê·¸ë˜í”„ ìµœì í™”, ìƒìˆ˜ í´ë”© ë“±)
        
        Args:
            onnx_path: ì›ë³¸ ONNX ëª¨ë¸ ê²½ë¡œ
            
        Returns:
            ìµœì í™”ëœ ëª¨ë¸ ê²½ë¡œ
        """
        try:
            import onnxruntime as ort
            
            # ìµœì í™” ì„¤ì •
            sess_options = ort.SessionOptions()
            sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
            
            # ìµœì í™”ëœ ëª¨ë¸ ê²½ë¡œ
            optimized_path = onnx_path.replace('.onnx', '_optimized.onnx')
            
            # ì„¸ì…˜ ìƒì„± (ìµœì í™” ì ìš©)
            session = ort.InferenceSession(onnx_path, sess_options)
            
            # ìµœì í™”ëœ ê·¸ë˜í”„ ì €ì¥ (ONNX Runtime 1.9+)
            try:
                session.save(optimized_path)
                logger.info(f"âœ… ONNX ëª¨ë¸ ìµœì í™” ì™„ë£Œ: {optimized_path}")
                return optimized_path
            except AttributeError:
                logger.warning("ONNX Runtime ë²„ì „ì´ ë‚®ì•„ ìµœì í™” ì €ì¥ ë¶ˆê°€, ì›ë³¸ ë°˜í™˜")
                return onnx_path
                
        except Exception as e:
            logger.error(f"âŒ ONNX ëª¨ë¸ ìµœì í™” ì‹¤íŒ¨: {e}")
            return onnx_path


def convert_all_models(
    deeplog_model: str,
    mscred_model: str,
    vocab_path: str,
    output_dir: str = "models/onnx"
) -> Dict[str, Any]:
    """
    ëª¨ë“  ëª¨ë¸ì„ ì¼ê´„ ë³€í™˜
    
    Args:
        deeplog_model: DeepLog ëª¨ë¸ ê²½ë¡œ
        mscred_model: MS-CRED ëª¨ë¸ ê²½ë¡œ
        vocab_path: ì–´íœ˜ ì‚¬ì „ ê²½ë¡œ
        output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
        
    Returns:
        ë³€í™˜ ê²°ê³¼ ìš”ì•½
    """
    converter = ModelConverter(output_dir)
    results = {}
    
    # DeepLog ë³€í™˜
    if os.path.exists(deeplog_model):
        try:
            deeplog_result = converter.convert_deeplog_to_onnx(
                deeplog_model, vocab_path
            )
            
            # ê²€ì¦ ë° ìµœì í™”
            if converter.validate_onnx_model(deeplog_result['onnx_path']):
                optimized_path = converter.optimize_onnx_model(deeplog_result['onnx_path'])
                deeplog_result['optimized_path'] = optimized_path
            
            results['deeplog'] = deeplog_result
            
        except Exception as e:
            logger.error(f"âŒ DeepLog ë³€í™˜ ì‹¤íŒ¨: {e}")
            results['deeplog'] = {'error': str(e)}
    
    # MS-CRED ë³€í™˜
    if os.path.exists(mscred_model):
        try:
            mscred_result = converter.convert_mscred_to_onnx(mscred_model)
            
            # ê²€ì¦ ë° ìµœì í™”
            if converter.validate_onnx_model(mscred_result['onnx_path']):
                optimized_path = converter.optimize_onnx_model(mscred_result['onnx_path'])
                mscred_result['optimized_path'] = optimized_path
            
            results['mscred'] = mscred_result
            
        except Exception as e:
            logger.error(f"âŒ MS-CRED ë³€í™˜ ì‹¤íŒ¨: {e}")
            results['mscred'] = {'error': str(e)}
    
    # ë³€í™˜ ìš”ì•½ ì €ì¥
    summary_path = Path(output_dir) / "conversion_summary.json"
    with open(summary_path, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    
    logger.info(f"ğŸ“‹ ë³€í™˜ ìš”ì•½ ì €ì¥: {summary_path}")
    
    return results


if __name__ == "__main__":
    import argparse
    
    parser = argparse.ArgumentParser(description="PyTorch ëª¨ë¸ì„ ONNXë¡œ ë³€í™˜")
    parser.add_argument("--deeplog-model", type=str, help="DeepLog ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--mscred-model", type=str, help="MS-CRED ëª¨ë¸ ê²½ë¡œ")
    parser.add_argument("--vocab", type=str, help="ì–´íœ˜ ì‚¬ì „ ê²½ë¡œ")
    parser.add_argument("--output-dir", type=str, default="models/onnx", help="ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--validate", action="store_true", help="ë³€í™˜ í›„ ê²€ì¦ ì‹¤í–‰")
    
    args = parser.parse_args()
    
    if not args.deeplog_model and not args.mscred_model:
        print("âŒ ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        parser.print_help()
        exit(1)
    
    # ë³€í™˜ ì‹¤í–‰
    results = convert_all_models(
        args.deeplog_model or "",
        args.mscred_model or "",
        args.vocab or "",
        args.output_dir
    )
    
    # ê²°ê³¼ ì¶œë ¥
    print("\nğŸ‰ ëª¨ë¸ ë³€í™˜ ì™„ë£Œ!")
    for model_name, result in results.items():
        if 'error' in result:
            print(f"âŒ {model_name}: {result['error']}")
        else:
            print(f"âœ… {model_name}: {result['onnx_path']}")
            if 'optimized_path' in result:
                print(f"âš¡ ìµœì í™”ë¨: {result['optimized_path']}")
