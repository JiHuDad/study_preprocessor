#!/usr/bin/env python3  # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì…”ë±…
"""ìë™ ëª¨ë¸ ë³€í™˜ ë° ë°°í¬ ìŠ¤í¬ë¦½íŠ¸ ìš”ì•½

- ëª©ì : PyTorch ëª¨ë¸ í•™ìŠµ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ONNX ë³€í™˜ ë° C ì—”ì§„ ë°°í¬ íŒ¨í‚¤ì§€ ì¤€ë¹„
- ì£¼ìš” ê¸°ëŠ¥:
  * íŒŒì¼ ì‹œìŠ¤í…œ ê°ì‹œ(watchdog)ë¥¼ í†µí•œ ëª¨ë¸ íŒŒì¼ ìë™ ê°ì§€ ë° ë³€í™˜
  * ê¸°ì¡´ ëª¨ë¸ ì¼ê´„ ë³€í™˜ (DeepLog, MS-CRED)
  * ONNX ëª¨ë¸ì„ ë°°í¬ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬ ë° ë°°í¬ íŒ¨í‚¤ì§€ ìƒì„±
  * ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í•™ìŠµ â†’ ë³€í™˜ â†’ ë°°í¬) ìë™í™”
- ì‹¤í–‰ ëª¨ë“œ:
  * watch: ëª¨ë¸ ë””ë ‰í† ë¦¬ ê°ì‹œ ëª¨ë“œ (ìƒˆ ëª¨ë¸ ìƒì„± ì‹œ ìë™ ë³€í™˜)
  * convert: ê¸°ì¡´ ëª¨ë¸ ì¼ê´„ ë³€í™˜ ëª¨ë“œ
  * pipeline: ì „ì²´ íŒŒì´í”„ë¼ì¸ ëª¨ë“œ (í•™ìŠµë¶€í„° ë°°í¬ê¹Œì§€)
- ì¶œë ¥: ONNX ëª¨ë¸(.onnx), ë©”íƒ€ë°ì´í„°(.meta.json), vocab.json, ë°°í¬ ì •ë³´(JSON)
"""  # ëª¨ë“ˆ ìš”ì•½ ì„¤ëª…
import os  # í™˜ê²½ ë³€ìˆ˜/ê²½ë¡œ ìœ í‹¸
import json  # JSON ì…ì¶œë ¥
import time  # ì‹œê°„ ì²˜ë¦¬
import shutil  # íŒŒì¼ ë³µì‚¬/ì´ë™
from pathlib import Path  # ê²½ë¡œ ì²˜ë¦¬
from typing import Dict, Any, Optional  # íƒ€ì… íŒíŠ¸
import logging  # ë¡œê¹… í”„ë ˆì„ì›Œí¬
from datetime import datetime  # ë‚ ì§œ/ì‹œê°„ ì²˜ë¦¬
from watchdog.observers import Observer  # íŒŒì¼ ì‹œìŠ¤í…œ ê°ì‹œì
from watchdog.events import FileSystemEventHandler  # íŒŒì¼ ì‹œìŠ¤í…œ ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬

from .model_converter import ModelConverter  # ëª¨ë¸ ë³€í™˜ê¸°
from .batch_trainer import BatchTrainer  # ë°°ì¹˜ í•™ìŠµê¸°

# ë¡œê¹… ì„¤ì •  # INFO ë ˆë²¨ ê¸°ë³¸ ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)  # ëª¨ë“ˆ ë¡œê±° ìƒì„±


class ModelWatcher(FileSystemEventHandler):  # ëª¨ë¸ íŒŒì¼ ê°ì‹œì í´ë˜ìŠ¤
    """ëª¨ë¸ íŒŒì¼ ë³€ê²½ ê°ì§€ ë° ìë™ ë³€í™˜"""  # í´ë˜ìŠ¤ ì„¤ëª…
    
    def __init__(self, converter: ModelConverter, watch_patterns: Dict[str, str]):  # ì´ˆê¸°í™”
        self.converter = converter  # ëª¨ë¸ ë³€í™˜ê¸°
        self.watch_patterns = watch_patterns  # {íŒŒì¼íŒ¨í„´: ëª¨ë¸íƒ€ì…}  # ê°ì‹œ íŒ¨í„´ ë”•ì…”ë„ˆë¦¬
        self.last_processed = {}  # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€  # ì²˜ë¦¬ ì´ë ¥ ë”•ì…”ë„ˆë¦¬
    
    def on_created(self, event):  # íŒŒì¼ ìƒì„± ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        """ìƒˆ íŒŒì¼ ìƒì„± ì‹œ ì²˜ë¦¬"""  # ë©”ì„œë“œ ì„¤ëª…
        if not event.is_directory:  # ë””ë ‰í† ë¦¬ê°€ ì•„ë‹˜
            self._process_file(event.src_path)  # íŒŒì¼ ì²˜ë¦¬
    
    def on_modified(self, event):  # íŒŒì¼ ìˆ˜ì • ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬
        """íŒŒì¼ ìˆ˜ì • ì‹œ ì²˜ë¦¬"""  # ë©”ì„œë“œ ì„¤ëª…
        if not event.is_directory:  # ë””ë ‰í† ë¦¬ê°€ ì•„ë‹˜
            self._process_file(event.src_path)  # íŒŒì¼ ì²˜ë¦¬
    
    def _process_file(self, file_path: str):  # íŒŒì¼ ì²˜ë¦¬
        """íŒŒì¼ ì²˜ë¦¬ (ëª¨ë¸ ë³€í™˜)"""  # ë©”ì„œë“œ ì„¤ëª…
        file_path = Path(file_path)  # ê²½ë¡œ ê°ì²´ë¡œ ë³€í™˜
        
        # ì¤‘ë³µ ì²˜ë¦¬ ë°©ì§€ (1ë¶„ ë‚´ ê°™ì€ íŒŒì¼)  # ì¤‘ë³µ ë³€í™˜ ë°©ì§€
        now = time.time()  # í˜„ì¬ ì‹œê°„
        if file_path.name in self.last_processed:  # ì´ì „ì— ì²˜ë¦¬í•œ íŒŒì¼
            if now - self.last_processed[file_path.name] < 60:  # 1ë¶„ ì´ë‚´
                return  # ì²˜ë¦¬ ê±´ë„ˆë›°ê¸°
        
        self.last_processed[file_path.name] = now  # ì²˜ë¦¬ ì‹œê°„ ê¸°ë¡
        
        # íŒ¨í„´ ë§¤ì¹­  # ëª¨ë¸ íƒ€ì… ê°ì§€
        for pattern, model_type in self.watch_patterns.items():  # ê° íŒ¨í„´ í™•ì¸
            if pattern in file_path.name and file_path.suffix == '.pth':  # ëª¨ë¸ íŒŒì¼ í™•ì¸
                logger.info(f"ğŸ” ìƒˆ {model_type} ëª¨ë¸ ê°ì§€: {file_path}")  # ê°ì§€ ë¡œê·¸
                self._convert_model(file_path, model_type)  # ëª¨ë¸ ë³€í™˜
                break  # ì²˜ë¦¬ ì™„ë£Œ
    
    def _convert_model(self, model_path: Path, model_type: str):  # ëª¨ë¸ ë³€í™˜ ì‹¤í–‰
        """ëª¨ë¸ ë³€í™˜ ì‹¤í–‰"""  # ë©”ì„œë“œ ì„¤ëª…
        try:
            if model_type == 'deeplog':  # DeepLog ëª¨ë¸
                # vocab.json ì°¾ê¸°  # ì–´íœ˜ ì‚¬ì „ íŒŒì¼ ì°¾ê¸°
                vocab_path = self._find_vocab_file(model_path)  # vocab íŒŒì¼ ê²€ìƒ‰
                if vocab_path:  # vocab íŒŒì¼ ë°œê²¬
                    result = self.converter.convert_deeplog_to_onnx(
                        str(model_path), str(vocab_path)  # DeepLog ë³€í™˜ í˜¸ì¶œ
                    )
                    logger.info(f"âœ… DeepLog ìë™ ë³€í™˜ ì™„ë£Œ: {result['onnx_path']}")  # ì™„ë£Œ ë¡œê·¸
                else:  # vocab íŒŒì¼ ì—†ìŒ
                    logger.warning("vocab.jsonì„ ì°¾ì„ ìˆ˜ ì—†ì–´ DeepLog ë³€í™˜ ê±´ë„ˆëœ€")  # ê²½ê³  ë¡œê·¸
            
            elif model_type == 'mscred':  # MS-CRED ëª¨ë¸
                result = self.converter.convert_mscred_to_onnx(str(model_path))  # MS-CRED ë³€í™˜ í˜¸ì¶œ
                logger.info(f"âœ… MS-CRED ìë™ ë³€í™˜ ì™„ë£Œ: {result['onnx_path']}")  # ì™„ë£Œ ë¡œê·¸
            
        except Exception as e:
            logger.error(f"âŒ {model_type} ìë™ ë³€í™˜ ì‹¤íŒ¨: {e}")  # ì—ëŸ¬ ë¡œê·¸
    
    def _find_vocab_file(self, model_path: Path) -> Optional[Path]:  # vocab íŒŒì¼ ì°¾ê¸°
        """DeepLogìš© vocab.json íŒŒì¼ ì°¾ê¸°"""  # ë©”ì„œë“œ ì„¤ëª…
        # ê°™ì€ ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°  # ëª¨ë¸ê³¼ ê°™ì€ í´ë” ê²€ìƒ‰
        vocab_path = model_path.parent / "vocab.json"  # vocab íŒŒì¼ ê²½ë¡œ
        if vocab_path.exists():  # íŒŒì¼ ì¡´ì¬
            return vocab_path  # ê²½ë¡œ ë°˜í™˜
        
        # ìƒìœ„ ë””ë ‰í† ë¦¬ë“¤ì—ì„œ ì°¾ê¸°  # ìƒìœ„ í´ë”ë“¤ ê²€ìƒ‰
        for parent in model_path.parents:  # ê° ìƒìœ„ í´ë” í™•ì¸
            vocab_path = parent / "vocab.json"  # vocab íŒŒì¼ ê²½ë¡œ
            if vocab_path.exists():  # íŒŒì¼ ì¡´ì¬
                return vocab_path  # ê²½ë¡œ ë°˜í™˜
        
        # data/processed ë””ë ‰í† ë¦¬ì—ì„œ ì°¾ê¸°  # ì¼ë°˜ì ì¸ ìœ„ì¹˜ ê²€ìƒ‰
        data_dirs = ["data/processed", "../data/processed", "../../data/processed"]  # ê²€ìƒ‰ ê²½ë¡œ ëª©ë¡
        for data_dir in data_dirs:  # ê° ê²½ë¡œ í™•ì¸
            vocab_path = Path(data_dir) / "vocab.json"  # vocab íŒŒì¼ ê²½ë¡œ
            if vocab_path.exists():  # íŒŒì¼ ì¡´ì¬
                return vocab_path  # ê²½ë¡œ ë°˜í™˜
        
        return None  # ì°¾ì§€ ëª»í•¨


class AutoConverter:  # ìë™ ë³€í™˜ ë° ë°°í¬ ê´€ë¦¬ì í´ë˜ìŠ¤
    """ìë™ ë³€í™˜ ë° ë°°í¬ ê´€ë¦¬ì"""  # í´ë˜ìŠ¤ ì„¤ëª…
    
    def __init__(self, 
                 models_dir: str = "models",  # ëª¨ë¸ ë””ë ‰í† ë¦¬
                 onnx_dir: str = "models/onnx",  # ONNX ì¶œë ¥ ë””ë ‰í† ë¦¬
                 deployment_dir: str = "models/deployment"):  # ë°°í¬ ë””ë ‰í† ë¦¬
        self.models_dir = Path(models_dir)  # ëª¨ë¸ ê²½ë¡œ
        self.onnx_dir = Path(onnx_dir)  # ONNX ê²½ë¡œ
        self.deployment_dir = Path(deployment_dir)  # ë°°í¬ ê²½ë¡œ
        
        # ë””ë ‰í† ë¦¬ ìƒì„±  # í•„ìš”í•œ í´ë” ìƒì„±
        self.models_dir.mkdir(parents=True, exist_ok=True)  # ëª¨ë¸ í´ë” ìƒì„±
        self.onnx_dir.mkdir(parents=True, exist_ok=True)  # ONNX í´ë” ìƒì„±
        self.deployment_dir.mkdir(parents=True, exist_ok=True)  # ë°°í¬ í´ë” ìƒì„±
        
        self.converter = ModelConverter(str(self.onnx_dir))  # ëª¨ë¸ ë³€í™˜ê¸° ìƒì„±
        self.observer = None  # íŒŒì¼ ê°ì‹œì ì´ˆê¸°í™”
    
    def start_watching(self):  # ëª¨ë¸ ë””ë ‰í† ë¦¬ ê°ì‹œ ì‹œì‘
        """ëª¨ë¸ ë””ë ‰í† ë¦¬ ê°ì‹œ ì‹œì‘"""  # ë©”ì„œë“œ ì„¤ëª…
        watch_patterns = {
            'deeplog': 'deeplog',  # DeepLog íŒ¨í„´
            'mscred': 'mscred'  # MS-CRED íŒ¨í„´
        }
        
        event_handler = ModelWatcher(self.converter, watch_patterns)  # ì´ë²¤íŠ¸ í•¸ë“¤ëŸ¬ ìƒì„±
        self.observer = Observer()  # ê°ì‹œì ìƒì„±
        self.observer.schedule(event_handler, str(self.models_dir), recursive=True)  # ê°ì‹œ ë“±ë¡
        self.observer.start()  # ê°ì‹œ ì‹œì‘
        
        logger.info(f"ğŸ” ëª¨ë¸ ë””ë ‰í† ë¦¬ ê°ì‹œ ì‹œì‘: {self.models_dir}")  # ì‹œì‘ ë¡œê·¸
        logger.info("ìƒˆë¡œìš´ ëª¨ë¸ì´ ìƒì„±ë˜ë©´ ìë™ìœ¼ë¡œ ONNX ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")  # ì•ˆë‚´ ë©”ì‹œì§€
    
    def stop_watching(self):  # ëª¨ë¸ ë””ë ‰í† ë¦¬ ê°ì‹œ ì¤‘ë‹¨
        """ëª¨ë¸ ë””ë ‰í† ë¦¬ ê°ì‹œ ì¤‘ë‹¨"""  # ë©”ì„œë“œ ì„¤ëª…
        if self.observer:  # ê°ì‹œì ì¡´ì¬
            self.observer.stop()  # ê°ì‹œ ì¤‘ë‹¨
            self.observer.join()  # ìŠ¤ë ˆë“œ ì¢…ë£Œ ëŒ€ê¸°
            logger.info("ğŸ›‘ ëª¨ë¸ ê°ì‹œ ì¤‘ë‹¨")  # ì¤‘ë‹¨ ë¡œê·¸
    
    def convert_existing_models(self) -> Dict[str, Any]:  # ê¸°ì¡´ ëª¨ë¸ ì¼ê´„ ë³€í™˜
        """ê¸°ì¡´ ëª¨ë¸ë“¤ì„ ì¼ê´„ ë³€í™˜"""  # ë©”ì„œë“œ ì„¤ëª…
        logger.info("ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ì¼ê´„ ë³€í™˜ ì‹œì‘")  # ì‹œì‘ ë¡œê·¸
        
        results = {}  # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        
        # DeepLog ëª¨ë¸ ì°¾ê¸°  # DeepLog ëª¨ë¸ ê²€ìƒ‰
        deeplog_models = list(self.models_dir.glob("*deeplog*.pth"))  # DeepLog ëª¨ë¸ ëª©ë¡
        for model_path in deeplog_models:  # ê° ëª¨ë¸ ì²˜ë¦¬
            try:
                vocab_path = self._find_vocab_for_model(model_path)  # vocab íŒŒì¼ ì°¾ê¸°
                if vocab_path:  # vocab íŒŒì¼ ë°œê²¬
                    result = self.converter.convert_deeplog_to_onnx(
                        str(model_path), str(vocab_path)  # DeepLog ë³€í™˜ í˜¸ì¶œ
                    )
                    results[f"deeplog_{model_path.stem}"] = result  # ê²°ê³¼ ì €ì¥
                    logger.info(f"âœ… DeepLog ë³€í™˜: {model_path.name}")  # ì™„ë£Œ ë¡œê·¸
                else:  # vocab íŒŒì¼ ì—†ìŒ
                    logger.warning(f"âš ï¸ vocab.json ì—†ìŒ: {model_path.name}")  # ê²½ê³  ë¡œê·¸
            except Exception as e:
                logger.error(f"âŒ DeepLog ë³€í™˜ ì‹¤íŒ¨ {model_path.name}: {e}")  # ì—ëŸ¬ ë¡œê·¸
        
        # MS-CRED ëª¨ë¸ ì°¾ê¸°  # MS-CRED ëª¨ë¸ ê²€ìƒ‰
        mscred_models = list(self.models_dir.glob("*mscred*.pth"))  # MS-CRED ëª¨ë¸ ëª©ë¡
        for model_path in mscred_models:  # ê° ëª¨ë¸ ì²˜ë¦¬
            try:
                result = self.converter.convert_mscred_to_onnx(str(model_path))  # MS-CRED ë³€í™˜ í˜¸ì¶œ
                results[f"mscred_{model_path.stem}"] = result  # ê²°ê³¼ ì €ì¥
                logger.info(f"âœ… MS-CRED ë³€í™˜: {model_path.name}")  # ì™„ë£Œ ë¡œê·¸
            except Exception as e:
                logger.error(f"âŒ MS-CRED ë³€í™˜ ì‹¤íŒ¨ {model_path.name}: {e}")  # ì—ëŸ¬ ë¡œê·¸
        
        logger.info(f"ğŸ‰ ì¼ê´„ ë³€í™˜ ì™„ë£Œ: {len(results)}ê°œ ëª¨ë¸")  # ì™„ë£Œ ë¡œê·¸
        return results  # ê²°ê³¼ ë°˜í™˜
    
    def _find_vocab_for_model(self, model_path: Path) -> Optional[Path]:  # vocab íŒŒì¼ ì°¾ê¸°
        """ëª¨ë¸ì— í•´ë‹¹í•˜ëŠ” vocab.json ì°¾ê¸°"""  # ë©”ì„œë“œ ì„¤ëª…
        # ëª¨ë¸ê³¼ ê°™ì€ ë””ë ‰í† ë¦¬  # ê°™ì€ í´ë” ê²€ìƒ‰
        vocab_path = model_path.parent / "vocab.json"  # vocab íŒŒì¼ ê²½ë¡œ
        if vocab_path.exists():  # íŒŒì¼ ì¡´ì¬
            return vocab_path  # ê²½ë¡œ ë°˜í™˜
        
        # ì¼ë°˜ì ì¸ ìœ„ì¹˜ë“¤ ê²€ìƒ‰  # ì¼ë°˜ì ì¸ ìœ„ì¹˜ ê²€ìƒ‰
        search_paths = [
            "data/processed/vocab.json",  # ì¼ë°˜ì ì¸ ê²½ë¡œ 1
            "../data/processed/vocab.json",  # ì¼ë°˜ì ì¸ ê²½ë¡œ 2
            "../../data/processed/vocab.json",  # ì¼ë°˜ì ì¸ ê²½ë¡œ 3
            self.models_dir / "vocab.json"  # ëª¨ë¸ ë””ë ‰í† ë¦¬
        ]
        
        for search_path in search_paths:  # ê° ê²½ë¡œ í™•ì¸
            vocab_path = Path(search_path)  # ê²½ë¡œ ê°ì²´ ìƒì„±
            if vocab_path.exists():  # íŒŒì¼ ì¡´ì¬
                return vocab_path  # ê²½ë¡œ ë°˜í™˜
        
        return None  # ì°¾ì§€ ëª»í•¨
    
    def prepare_deployment_package(self, model_name: Optional[str] = None) -> Dict[str, Any]:  # ë°°í¬ íŒ¨í‚¤ì§€ ì¤€ë¹„
        """C ì—”ì§„ ë°°í¬ìš© íŒ¨í‚¤ì§€ ì¤€ë¹„"""  # ë©”ì„œë“œ ì„¤ëª…
        logger.info("ğŸ“¦ ë°°í¬ íŒ¨í‚¤ì§€ ì¤€ë¹„ ì¤‘...")  # ì‹œì‘ ë¡œê·¸
        
        deployment_info = {
            'timestamp': datetime.now().isoformat(),  # íƒ€ì„ìŠ¤íƒ¬í”„
            'models': {},  # ëª¨ë¸ ì •ë³´
            'files': []  # íŒŒì¼ ëª©ë¡
        }
        
        # ONNX ëª¨ë¸ë“¤ ë³µì‚¬  # ONNX ëª¨ë¸ ë³µì‚¬
        onnx_files = list(self.onnx_dir.glob("*.onnx"))  # ONNX íŒŒì¼ ëª©ë¡
        for onnx_file in onnx_files:  # ê° íŒŒì¼ ì²˜ë¦¬
            if model_name and model_name not in onnx_file.name:  # í•„í„°ë§
                continue  # ê±´ë„ˆë›°ê¸°
            
            # ë°°í¬ ë””ë ‰í† ë¦¬ë¡œ ë³µì‚¬  # ONNX íŒŒì¼ ë³µì‚¬
            dest_path = self.deployment_dir / onnx_file.name  # ëŒ€ìƒ ê²½ë¡œ
            shutil.copy2(onnx_file, dest_path)  # íŒŒì¼ ë³µì‚¬
            
            # ë©”íƒ€ë°ì´í„°ë„ ë³µì‚¬  # ë©”íƒ€ë°ì´í„° íŒŒì¼ ë³µì‚¬
            meta_file = onnx_file.with_suffix('.onnx.meta.json')  # ë©”íƒ€ë°ì´í„° íŒŒì¼ ê²½ë¡œ
            if meta_file.exists():  # íŒŒì¼ ì¡´ì¬
                meta_dest = self.deployment_dir / meta_file.name  # ëŒ€ìƒ ê²½ë¡œ
                shutil.copy2(meta_file, meta_dest)  # íŒŒì¼ ë³µì‚¬
                deployment_info['files'].append(str(meta_dest))  # íŒŒì¼ ëª©ë¡ ì¶”ê°€
            
            deployment_info['models'][onnx_file.stem] = {
                'onnx_path': str(dest_path),  # ONNX ê²½ë¡œ
                'size_mb': dest_path.stat().st_size / (1024 * 1024)  # íŒŒì¼ í¬ê¸° (MB)
            }
            deployment_info['files'].append(str(dest_path))  # íŒŒì¼ ëª©ë¡ ì¶”ê°€
        
        # vocab.json ë³µì‚¬  # ì–´íœ˜ ì‚¬ì „ íŒŒì¼ ë³µì‚¬
        vocab_file = self.onnx_dir / "vocab.json"  # vocab íŒŒì¼ ê²½ë¡œ
        if vocab_file.exists():  # íŒŒì¼ ì¡´ì¬
            vocab_dest = self.deployment_dir / "vocab.json"  # ëŒ€ìƒ ê²½ë¡œ
            shutil.copy2(vocab_file, vocab_dest)  # íŒŒì¼ ë³µì‚¬
            deployment_info['files'].append(str(vocab_dest))  # íŒŒì¼ ëª©ë¡ ì¶”ê°€
        
        # ë°°í¬ ì •ë³´ ì €ì¥  # ë°°í¬ ì •ë³´ JSON ì €ì¥
        info_file = self.deployment_dir / "deployment_info.json"  # ì •ë³´ íŒŒì¼ ê²½ë¡œ
        with open(info_file, 'w') as f:
            json.dump(deployment_info, f, indent=2, default=str)  # JSON ì €ì¥
        
        logger.info(f"âœ… ë°°í¬ íŒ¨í‚¤ì§€ ì¤€ë¹„ ì™„ë£Œ: {self.deployment_dir}")  # ì™„ë£Œ ë¡œê·¸
        logger.info(f"ğŸ“Š í¬í•¨ëœ ëª¨ë¸: {list(deployment_info['models'].keys())}")  # ëª¨ë¸ ëª©ë¡ ë¡œê·¸
        
        return deployment_info  # ë°°í¬ ì •ë³´ ë°˜í™˜
    
    def run_full_pipeline(self, log_file: str, auto_deploy: bool = True) -> Dict[str, Any]:  # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        """ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í•™ìŠµ â†’ ë³€í™˜ â†’ ë°°í¬)"""  # ë©”ì„œë“œ ì„¤ëª…
        logger.info("ğŸš€ ì „ì²´ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ ì‹œì‘")  # ì‹œì‘ ë¡œê·¸
        
        pipeline_results = {
            'start_time': datetime.now().isoformat(),  # ì‹œì‘ ì‹œê°„
            'log_file': log_file,  # ë¡œê·¸ íŒŒì¼
            'stages': {}  # ë‹¨ê³„ë³„ ê²°ê³¼
        }
        
        try:
            # 1ë‹¨ê³„: ë°°ì¹˜ í•™ìŠµ  # ë°°ì¹˜ í•™ìŠµ ì‹¤í–‰
            logger.info("1ï¸âƒ£ ë°°ì¹˜ í•™ìŠµ ë‹¨ê³„")
            trainer = BatchTrainer(models_dir=str(self.models_dir))  # í•™ìŠµê¸° ìƒì„±
            training_results = trainer.train_full_pipeline(log_file)  # í•™ìŠµ ì‹¤í–‰
            pipeline_results['stages']['training'] = training_results  # ê²°ê³¼ ì €ì¥
            
            # 2ë‹¨ê³„: ONNX ë³€í™˜  # ONNX ë³€í™˜ ì‹¤í–‰
            logger.info("2ï¸âƒ£ ONNX ë³€í™˜ ë‹¨ê³„")
            conversion_results = {}  # ë³€í™˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
            
            for model_name, model_info in training_results.get('models', {}).items():  # ê° ëª¨ë¸ ë³€í™˜
                try:
                    model_path = model_info['path']  # ëª¨ë¸ ê²½ë¡œ
                    
                    if model_name == 'deeplog':  # DeepLog ëª¨ë¸
                        vocab_path = training_results['files']['vocab']  # vocab íŒŒì¼ ê²½ë¡œ
                        result = self.converter.convert_deeplog_to_onnx(
                            model_path, vocab_path  # DeepLog ë³€í™˜ í˜¸ì¶œ
                        )
                    elif model_name == 'mscred':  # MS-CRED ëª¨ë¸
                        result = self.converter.convert_mscred_to_onnx(model_path)  # MS-CRED ë³€í™˜ í˜¸ì¶œ
                    
                    conversion_results[model_name] = result  # ê²°ê³¼ ì €ì¥
                    logger.info(f"âœ… {model_name} ë³€í™˜ ì™„ë£Œ")  # ì™„ë£Œ ë¡œê·¸
                    
                except Exception as e:
                    logger.error(f"âŒ {model_name} ë³€í™˜ ì‹¤íŒ¨: {e}")  # ì—ëŸ¬ ë¡œê·¸
                    conversion_results[model_name] = {'error': str(e)}  # ì—ëŸ¬ ì €ì¥
            
            pipeline_results['stages']['conversion'] = conversion_results  # ë³€í™˜ ê²°ê³¼ ì €ì¥
            
            # 3ë‹¨ê³„: ë°°í¬ ì¤€ë¹„  # ë°°í¬ íŒ¨í‚¤ì§€ ì¤€ë¹„
            if auto_deploy and conversion_results:  # ìë™ ë°°í¬ í™œì„±í™” ë° ë³€í™˜ ì„±ê³µ
                logger.info("3ï¸âƒ£ ë°°í¬ ì¤€ë¹„ ë‹¨ê³„")
                deployment_info = self.prepare_deployment_package()  # ë°°í¬ íŒ¨í‚¤ì§€ ì¤€ë¹„
                pipeline_results['stages']['deployment'] = deployment_info  # ë°°í¬ ì •ë³´ ì €ì¥
            
            pipeline_results['status'] = 'success'  # ì„±ê³µ ìƒíƒœ
            pipeline_results['end_time'] = datetime.now().isoformat()  # ì¢…ë£Œ ì‹œê°„
            
            logger.info("ğŸ‰ ì „ì²´ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")  # ì™„ë£Œ ë¡œê·¸
            
        except Exception as e:
            pipeline_results['status'] = 'failed'  # ì‹¤íŒ¨ ìƒíƒœ
            pipeline_results['error'] = str(e)  # ì—ëŸ¬ ë©”ì‹œì§€
            pipeline_results['end_time'] = datetime.now().isoformat()  # ì¢…ë£Œ ì‹œê°„
            logger.error(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")  # ì—ëŸ¬ ë¡œê·¸
            raise  # ì˜ˆì™¸ ì¬ë°œìƒ
        
        # ê²°ê³¼ ì €ì¥  # íŒŒì´í”„ë¼ì¸ ê²°ê³¼ JSON ì €ì¥
        results_file = self.deployment_dir / "pipeline_results.json"  # ê²°ê³¼ íŒŒì¼ ê²½ë¡œ
        with open(results_file, 'w') as f:
            json.dump(pipeline_results, f, indent=2, default=str)  # JSON ì €ì¥
        
        return pipeline_results  # íŒŒì´í”„ë¼ì¸ ê²°ê³¼ ë°˜í™˜


def main():  # CLI ì¸í„°í˜ì´ìŠ¤ ë©”ì¸ í•¨ìˆ˜
    """CLI ì¸í„°í˜ì´ìŠ¤"""  # í•¨ìˆ˜ ì„¤ëª…
    import argparse  # ì¸ì íŒŒì‹±
    
    parser = argparse.ArgumentParser(description="ìë™ ëª¨ë¸ ë³€í™˜ ë° ë°°í¬")  # íŒŒì„œ ìƒì„±
    parser.add_argument("--mode", choices=['watch', 'convert', 'pipeline'], 
                       default='convert', help="ì‹¤í–‰ ëª¨ë“œ")  # ì‹¤í–‰ ëª¨ë“œ ì¸ì
    parser.add_argument("--log-file", help="í•™ìŠµìš© ë¡œê·¸ íŒŒì¼ (pipeline ëª¨ë“œ)")  # ë¡œê·¸ íŒŒì¼ ì¸ì
    parser.add_argument("--models-dir", default="models", help="ëª¨ë¸ ë””ë ‰í† ë¦¬")  # ëª¨ë¸ ë””ë ‰í† ë¦¬ ì¸ì
    parser.add_argument("--onnx-dir", default="models/onnx", help="ONNX ì¶œë ¥ ë””ë ‰í† ë¦¬")  # ONNX ë””ë ‰í† ë¦¬ ì¸ì
    parser.add_argument("--deployment-dir", default="models/deployment", help="ë°°í¬ ë””ë ‰í† ë¦¬")  # ë°°í¬ ë””ë ‰í† ë¦¬ ì¸ì
    
    args = parser.parse_args()  # ì¸ì íŒŒì‹±
    
    converter = AutoConverter(args.models_dir, args.onnx_dir, args.deployment_dir)  # ë³€í™˜ê¸° ìƒì„±
    
    if args.mode == 'watch':  # ê°ì‹œ ëª¨ë“œ
        # ê°ì‹œ ëª¨ë“œ  # íŒŒì¼ ì‹œìŠ¤í…œ ê°ì‹œ ì‹œì‘
        converter.start_watching()  # ê°ì‹œ ì‹œì‘
        try:
            while True:  # ë¬´í•œ ë£¨í”„
                time.sleep(1)  # 1ì´ˆ ëŒ€ê¸°
        except KeyboardInterrupt:  # Ctrl+C ì…ë ¥
            converter.stop_watching()  # ê°ì‹œ ì¤‘ë‹¨
            logger.info("ğŸ‘‹ ê°ì‹œ ëª¨ë“œ ì¢…ë£Œ")  # ì¢…ë£Œ ë¡œê·¸
    
    elif args.mode == 'convert':  # ë³€í™˜ ëª¨ë“œ
        # ë³€í™˜ ëª¨ë“œ  # ê¸°ì¡´ ëª¨ë¸ ë³€í™˜
        results = converter.convert_existing_models()  # ëª¨ë¸ ë³€í™˜ ì‹¤í–‰
        if results:  # ë³€í™˜ ì„±ê³µ
            converter.prepare_deployment_package()  # ë°°í¬ íŒ¨í‚¤ì§€ ì¤€ë¹„
        print(f"âœ… ë³€í™˜ ì™„ë£Œ: {len(results)}ê°œ ëª¨ë¸")  # ì™„ë£Œ ë©”ì‹œì§€
    
    elif args.mode == 'pipeline':  # íŒŒì´í”„ë¼ì¸ ëª¨ë“œ
        # ì „ì²´ íŒŒì´í”„ë¼ì¸ ëª¨ë“œ  # ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        if not args.log_file:  # ë¡œê·¸ íŒŒì¼ ì—†ìŒ
            print("âŒ pipeline ëª¨ë“œëŠ” --log-fileì´ í•„ìš”í•©ë‹ˆë‹¤")  # ì—ëŸ¬ ë©”ì‹œì§€
            return  # ì¢…ë£Œ
        
        results = converter.run_full_pipeline(args.log_file)  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        print(f"âœ… íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {results['status']}")  # ì™„ë£Œ ë©”ì‹œì§€


if __name__ == "__main__":  # ìŠ¤í¬ë¦½íŠ¸ ì§ì ‘ ì‹¤í–‰
    main()  # ë©”ì¸ í•¨ìˆ˜ í˜¸ì¶œ
