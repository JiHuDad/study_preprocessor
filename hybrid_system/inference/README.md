# C ì¶”ë¡  ì—”ì§„

í•˜ì´ë¸Œë¦¬ë“œ ë¡œê·¸ ì´ìƒíƒì§€ ì‹œìŠ¤í…œì˜ ê³ ì„±ëŠ¥ C ì¶”ë¡  ì—”ì§„ì…ë‹ˆë‹¤.

## ğŸ¯ íŠ¹ì§•

- **ê³ ì„±ëŠ¥**: Cë¡œ êµ¬í˜„ëœ ìµœì í™”ëœ ì¶”ë¡  ì—”ì§„
- **ONNX ì§€ì›**: PyTorchì—ì„œ ë³€í™˜ëœ ONNX ëª¨ë¸ ì‹¤í–‰
- **ì‹¤ì‹œê°„ ì²˜ë¦¬**: ìŠ¤íŠ¸ë¦¼ ê¸°ë°˜ ë¡œê·¸ ì²˜ë¦¬
- **ë©”ëª¨ë¦¬ íš¨ìœ¨**: ìµœì†Œí•œì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
- **í¬ë¡œìŠ¤ í”Œë«í¼**: Linux/macOS/Windows ì§€ì›

## ğŸ“¦ ì˜ì¡´ì„±

### í•„ìˆ˜ ì˜ì¡´ì„±
- **GCC 4.9+** ë˜ëŠ” **Clang 3.9+**
- **ONNX Runtime C API 1.16.0+**
- **pthread** (POSIX ìŠ¤ë ˆë“œ)
- **libm** (ìˆ˜í•™ ë¼ì´ë¸ŒëŸ¬ë¦¬)

### ì„ íƒì  ì˜ì¡´ì„±
- **valgrind** (ë©”ëª¨ë¦¬ ê²€ì‚¬ìš©)
- **cppcheck** (ì •ì  ë¶„ì„ìš©)
- **clang-format** (ì½”ë“œ í¬ë§·íŒ…ìš©)

## ğŸ› ï¸ ë¹Œë“œ

### 1. ONNX Runtime ì„¤ì¹˜

```bash
# ONNX Runtime ë‹¤ìš´ë¡œë“œ
wget https://github.com/microsoft/onnxruntime/releases/download/v1.16.0/onnxruntime-linux-x64-1.16.0.tgz
tar -xzf onnxruntime-linux-x64-1.16.0.tgz

# ì‹œìŠ¤í…œì— ì„¤ì¹˜
sudo cp onnxruntime-linux-x64-1.16.0/lib/* /usr/local/lib/
sudo cp -r onnxruntime-linux-x64-1.16.0/include/* /usr/local/include/
sudo ldconfig
```

### 2. ì˜ì¡´ì„± í™•ì¸

```bash
make check-deps
```

### 3. ë¹Œë“œ

```bash
# ê¸°ë³¸ ë¹Œë“œ
make

# ë””ë²„ê·¸ ë¹Œë“œ
make debug

# ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¹Œë“œ
make static shared

# ëª¨ë“  íƒ€ê²Ÿ ë¹Œë“œ
make all static shared
```

## ğŸš€ ì‚¬ìš©ë²•

### ê¸°ë³¸ ì‚¬ìš©ë²•

```bash
# ê¸°ë³¸ ì‹¤í–‰
./bin/inference_engine -d deeplog.onnx -v vocab.json -i /var/log/syslog

# í…ŒìŠ¤íŠ¸ ëª¨ë“œ
./bin/inference_engine -d deeplog.onnx -v vocab.json -t

# ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¼ ì²˜ë¦¬
tail -f /var/log/syslog | ./bin/inference_engine -d deeplog.onnx -v vocab.json
```

### ëª…ë ¹í–‰ ì˜µì…˜

```
Usage: inference_engine [OPTIONS]

Options:
  -d, --deeplog PATH     DeepLog ONNX model path
  -m, --mscred PATH      MS-CRED ONNX model path (optional)
  -v, --vocab PATH       Vocabulary JSON file path
  -s, --seq-len N        Sequence length (default: 50)
  -k, --top-k N          Top-K value (default: 3)
  -i, --input PATH       Input log file (default: stdin)
  -o, --output PATH      Output results file (default: stdout)
  -t, --test             Run test mode with sample data
  -h, --help             Show help message
```

### ì˜ˆì‹œ

```bash
# íŒŒì¼ì—ì„œ ë¡œê·¸ ì²˜ë¦¬
./bin/inference_engine \
  -d models/deeplog.onnx \
  -v models/vocab.json \
  -i /var/log/application.log \
  -o results.json

# MS-CRED ëª¨ë¸ í¬í•¨
./bin/inference_engine \
  -d models/deeplog.onnx \
  -m models/mscred.onnx \
  -v models/vocab.json \
  -s 100 -k 5

# ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
tail -f /var/log/syslog | \
./bin/inference_engine \
  -d models/deeplog.onnx \
  -v models/vocab.json
```

## ğŸ“Š ì¶œë ¥ í˜•ì‹

### ì½˜ì†” ì¶œë ¥
```
[ANOMALY] DeepLog anomaly (0.900) - 2024-01-01 10:00:03 ERROR Authentication failed
[NORMAL ] DeepLog normal (0.100) - 2024-01-01 10:00:04 INFO Database query completed
```

### JSON ì¶œë ¥ (-o ì˜µì…˜ ì‚¬ìš©ì‹œ)
```json
[
  {
    "line_number": 1,
    "timestamp": 1704067203,
    "is_anomaly": true,
    "confidence": 0.900000,
    "score": 0.900000,
    "reason": "DeepLog anomaly",
    "predicted_template": 15,
    "log_line": "2024-01-01 10:00:03 ERROR Authentication failed for user bob"
  }
]
```

## ğŸ§ª í…ŒìŠ¤íŠ¸

### ê¸°ë³¸ í…ŒìŠ¤íŠ¸
```bash
make test
```

### ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬
```bash
make benchmark
```

### ë©”ëª¨ë¦¬ ê²€ì‚¬
```bash
make memcheck
```

### ì •ì  ë¶„ì„
```bash
make analyze
```

## ğŸ“š API ì‚¬ìš©ë²•

### C ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ ì‚¬ìš©

```c
#include "inference_engine.h"

int main() {
    // ì¶”ë¡  ì—”ì§„ ìƒì„±
    InferenceEngine* engine = inference_engine_create(50, 3);
    
    // ëª¨ë¸ ë¡œë“œ
    inference_engine_load_models(engine, "deeplog.onnx", NULL);
    inference_engine_load_vocab(engine, "vocab.json");
    
    // ë¡œê·¸ ì²˜ë¦¬
    AnomalyResult result;
    InferenceResult status = inference_engine_process_log(
        engine, 
        "2024-01-01 10:00:01 INFO User logged in", 
        &result
    );
    
    if (status == IE_SUCCESS) {
        printf("Anomaly: %s (%.3f)\n", 
               result.is_anomaly ? "YES" : "NO", 
               result.confidence);
    }
    
    // ì •ë¦¬
    inference_engine_destroy(engine);
    return 0;
}
```

### ì»´íŒŒì¼
```bash
gcc -o my_app my_app.c -linference_engine -lonnxruntime -lm
```

## ğŸ”§ ì„¤ì •

### í™˜ê²½ ë³€ìˆ˜
- `OMP_NUM_THREADS`: OpenMP ìŠ¤ë ˆë“œ ìˆ˜ (ê¸°ë³¸: 1)
- `ORT_LOGGING_LEVEL`: ONNX Runtime ë¡œê·¸ ë ˆë²¨ (0-4)

### ì„±ëŠ¥ íŠœë‹
- **ì‹œí€€ìŠ¤ ê¸¸ì´**: ë” ê¸´ ì‹œí€€ìŠ¤ëŠ” ì •í™•ë„ í–¥ìƒ, ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¦ê°€
- **Top-K ê°’**: ë” í° KëŠ” ë¯¼ê°ë„ ê°ì†Œ, ì •í™•ë„ í–¥ìƒ
- **ë°°ì¹˜ í¬ê¸°**: ë” í° ë°°ì¹˜ëŠ” ì²˜ë¦¬ëŸ‰ í–¥ìƒ, ì§€ì—°ì‹œê°„ ì¦ê°€

## ğŸ“ˆ ì„±ëŠ¥ íŠ¹ì„±

### ë²¤ì¹˜ë§ˆí¬ ê²°ê³¼ (ì˜ˆìƒ)
- **ì²˜ë¦¬ëŸ‰**: 10,000-50,000 logs/sec
- **ì§€ì—°ì‹œê°„**: < 10ms per log
- **ë©”ëª¨ë¦¬**: 50-100MB
- **CPU**: ë‹¨ì¼ ì½”ì–´ 20-30%

### í™•ì¥ì„±
- **ìˆ˜ì§ í™•ì¥**: ë©€í‹°ì½”ì–´ CPU í™œìš©
- **ìˆ˜í‰ í™•ì¥**: ì—¬ëŸ¬ ì¸ìŠ¤í„´ìŠ¤ ë³‘ë ¬ ì‹¤í–‰
- **ìŠ¤íŠ¸ë¦¼ ë¶„í• **: ë¡œê·¸ ì†ŒìŠ¤ë³„ ë¶„ì‚° ì²˜ë¦¬

## ğŸ› ë¬¸ì œ í•´ê²°

### ì¼ë°˜ì ì¸ ë¬¸ì œ

**1. ONNX Runtime ë¡œë“œ ì‹¤íŒ¨**
```bash
# ë¼ì´ë¸ŒëŸ¬ë¦¬ ê²½ë¡œ í™•ì¸
ldd bin/inference_engine
export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH
```

**2. ëª¨ë¸ íŒŒì¼ ì˜¤ë¥˜**
```bash
# ONNX ëª¨ë¸ ê²€ì¦
python -c "import onnx; onnx.checker.check_model('model.onnx')"
```

**3. ë©”ëª¨ë¦¬ ë¶€ì¡±**
```bash
# ì‹œí€€ìŠ¤ ê¸¸ì´ ì¤„ì´ê¸°
./bin/inference_engine -s 25 -d model.onnx -v vocab.json
```

### ë””ë²„ê·¸ ëª¨ë“œ
```bash
# ë””ë²„ê·¸ ë¹Œë“œ
make debug

# GDBë¡œ ë””ë²„ê¹…
gdb ./bin/inference_engine
(gdb) run -d model.onnx -v vocab.json -t
```

## ğŸ¤ ê¸°ì—¬

1. ì½”ë“œ í¬ë§·íŒ…: `make format`
2. ì •ì  ë¶„ì„: `make analyze`
3. í…ŒìŠ¤íŠ¸ ì‹¤í–‰: `make test memcheck`
4. ë¬¸ì„œ ì—…ë°ì´íŠ¸

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” MIT ë¼ì´ì„ ìŠ¤ í•˜ì— ë°°í¬ë©ë‹ˆë‹¤.

## ğŸ”— ê´€ë ¨ ë§í¬

- [ONNX Runtime](https://onnxruntime.ai/)
- [í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ë¬¸ì„œ](../README.md)
- [Python í•™ìŠµ ì»´í¬ë„ŒíŠ¸](../training/README.md)
