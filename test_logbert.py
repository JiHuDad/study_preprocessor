"""LogBERT êµ¬í˜„ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸"""

import sys
import tempfile
from pathlib import Path
import pandas as pd
import numpy as np

print("ğŸ§ª LogBERT êµ¬í˜„ í…ŒìŠ¤íŠ¸ ì‹œì‘...\n")

# 1. í•©ì„± ë°ì´í„° ìƒì„±
print("1ï¸âƒ£ í•©ì„± ë¡œê·¸ ë°ì´í„° ìƒì„±...")
synthetic_data = {
    "line_no": list(range(1, 101)),
    "timestamp": pd.date_range("2025-01-01", periods=100, freq="1min"),
    "host": ["server1"] * 100,
    "template": ["User logged in"] * 50 +
                ["Database connection established"] * 30 +
                ["Cache hit"] * 15 +
                ["CRITICAL: Disk full"] * 5  # ì´ìƒ íŒ¨í„´
}
df = pd.DataFrame(synthetic_data)

with tempfile.TemporaryDirectory() as tmpdir:
    tmpdir = Path(tmpdir)

    # 2. íŒŒì‹±ëœ ë°ì´í„° ì €ì¥
    parsed_file = tmpdir / "parsed.parquet"
    df.to_parquet(parsed_file, index=False)
    print(f"âœ… í•©ì„± ë°ì´í„° ìƒì„±: {len(df)} ë¡œê·¸\n")

    # 3. LogBERT ì…ë ¥ ìƒì„±
    print("2ï¸âƒ£ LogBERT ì…ë ¥ ë°ì´í„° ìƒì„±...")
    from anomaly_log_detector.builders.logbert import build_logbert_inputs

    build_logbert_inputs(
        parsed_parquet=parsed_file,
        out_dir=tmpdir
    )
    print("âœ… vocab.json, sequences.parquet, special_tokens.json ìƒì„±\n")

    # 4. Vocab í™•ì¸
    import json
    vocab_file = tmpdir / "vocab.json"
    with open(vocab_file, 'r') as f:
        vocab = json.load(f)
    print(f"ğŸ“š Vocab í¬ê¸°: {len(vocab)}")
    print(f"   - íŠ¹ìˆ˜ í† í°: {list(vocab.keys())[:5]}")
    print(f"   - í…œí”Œë¦¿ ì˜ˆì‹œ: {list(vocab.keys())[5:8]}\n")

    # 5. LogBERT ëª¨ë¸ í•™ìŠµ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš© ì‘ì€ íŒŒë¼ë¯¸í„°)
    print("3ï¸âƒ£ LogBERT ëª¨ë¸ í•™ìŠµ (ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ ëª¨ë“œ)...")
    from anomaly_log_detector.builders.logbert import train_logbert

    model_path = tmpdir / "logbert_test.pth"
    train_logbert(
        sequences_parquet=tmpdir / "sequences.parquet",
        vocab_json=vocab_file,
        out_path=model_path,
        seq_len=16,  # ì‘ì€ ì‹œí€€ìŠ¤ ê¸¸ì´
        epochs=2,    # ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©
        batch_size=8,
        hidden_size=64,  # ì‘ì€ ëª¨ë¸
        num_layers=2,
        num_heads=4
    )
    print(f"âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {model_path}\n")

    # 6. LogBERT ì¶”ë¡ 
    print("4ï¸âƒ£ LogBERT ì´ìƒ íƒì§€ ì¶”ë¡ ...")
    from anomaly_log_detector.builders.logbert import infer_logbert

    results_df = infer_logbert(
        sequences_parquet=tmpdir / "sequences.parquet",
        model_path=model_path,
        vocab_json=vocab_file,
        threshold_percentile=90.0,  # ë‚®ì€ ì„ê³„ê°’ìœ¼ë¡œ í…ŒìŠ¤íŠ¸
        seq_len=16
    )

    print(f"âœ… ì¶”ë¡  ì™„ë£Œ")
    print(f"   - ì´ ì‹œí€€ìŠ¤: {len(results_df)}")
    print(f"   - ì´ìƒ ì‹œí€€ìŠ¤: {results_df['is_anomaly'].sum()}")
    print(f"   - ì´ìƒë¥ : {results_df['is_anomaly'].mean():.2%}\n")

    # 7. ê²°ê³¼ ìƒì„¸ ì •ë³´
    print("5ï¸âƒ£ ì¶”ë¡  ê²°ê³¼ ìƒì„¸:")
    print(results_df.head(10).to_string(index=False))

    print("\n\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!")
    print("=" * 60)
    print("LogBERT êµ¬í˜„ì´ ì„±ê³µì ìœ¼ë¡œ ì‘ë™í•©ë‹ˆë‹¤!")
    print("=" * 60)
