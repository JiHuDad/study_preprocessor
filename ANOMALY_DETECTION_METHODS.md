# ë¡œê·¸ ì´ìƒ íƒì§€ ë°©ë²•ë¡  ê°€ì´ë“œ

## ğŸ¯ í˜„ì¬ êµ¬í˜„ëœ ë°©ë²•

### 1. **ì‹œê°„ ìœˆë„ìš° ê¸°ë°˜ ì´ìƒ íƒì§€** (í˜„ì¬ ë°©ë²•)
**ì›ë¦¬**: ì—°ì†ëœ ë¡œê·¸ ë¼ì¸ë“¤ì„ ê³ ì • í¬ê¸° ìœˆë„ìš°ë¡œ ë‚˜ëˆ„ì–´ ë¶„ì„
- **ìœˆë„ìš° í¬ê¸°**: 50ê°œ ë¡œê·¸ ë¼ì¸ (ê¸°ë³¸ê°’)
- **ìŠ¬ë¼ì´ë”© ìŠ¤íŠ¸ë¼ì´ë“œ**: 25ê°œ ë¼ì¸ì”© ì´ë™
- **ë¶„ì„ ëŒ€ìƒ**: ê° ìœˆë„ìš° ë‚´ì˜ í…œí”Œë¦¿ íŒ¨í„´ ë³€í™”

**ì¥ì **:
- âœ… ì‹¤ì‹œê°„ íƒì§€ ê°€ëŠ¥
- âœ… ìˆœì„œì— ë¯¼ê°í•œ ì´ìƒ íŒ¨í„´ ê°ì§€
- âœ… ë©”ëª¨ë¦¬ íš¨ìœ¨ì 

**ë‹¨ì **:
- âŒ ê¸´ ì£¼ê¸°ì˜ íŒ¨í„´ ë†“ì¹  ìˆ˜ ìˆìŒ
- âŒ ìœˆë„ìš° ê²½ê³„ì—ì„œ ì´ìƒì´ ë¶„í• ë  ìˆ˜ ìˆìŒ

### 2. **í…œí”Œë¦¿ ë¹ˆë„ ê¸°ë°˜ íƒì§€** (í˜„ì¬ ë°©ë²•)
**ì›ë¦¬**: ê° ë¡œê·¸ í…œí”Œë¦¿ì˜ ì¶œí˜„ ë¹ˆë„ê°€ ê¸‰ê²©íˆ ë³€í™”í•˜ëŠ” êµ¬ê°„ ê°ì§€
- **EWM (Exponentially Weighted Moving)** í†µê³„ ì‚¬ìš©
- **Z-score** ê¸°ë°˜ ì´ìƒì¹˜ íƒì§€
- **ë¯¸ì§€ í…œí”Œë¦¿ìœ¨** ê³„ì‚°

```python
# í˜„ì¬ êµ¬í˜„
freq_z = (counts - ewm_mean) / ewm_std  # ë¹ˆë„ Z-score
unseen_rate = new_templates / total_templates  # ë¯¸ì§€ í…œí”Œë¦¿ ë¹„ìœ¨
score = 0.6 * freq_norm + 0.4 * unseen_norm  # ì¢…í•© ì ìˆ˜
```

## ğŸ†• ìƒˆë¡œìš´ íƒì§€ ë°©ë²•ë“¤

### 3. **ë‚ ì§œë³„ í”„ë¡œíŒŒì¼ ê¸°ë°˜ íƒì§€**

**ì›ë¦¬**: ê³¼ê±° ë™ì¼ ì‹œê°„ëŒ€/ìš”ì¼ì˜ íŒ¨í„´ê³¼ ë¹„êµí•˜ì—¬ ì´ìƒ ê°ì§€

**êµ¬í˜„ ì•„ì´ë””ì–´**:
```python
# ì‹œê°„ë³„ ì •ìƒ í”„ë¡œíŒŒì¼ í•™ìŠµ
hourly_profiles = {
    "hour_0": {"template_dist": {...}, "volume": 1000},
    "hour_1": {"template_dist": {...}, "volume": 800},
    # ...
}

# í˜„ì¬ ì‹œê°„ëŒ€ì™€ ë¹„êµ
current_hour = datetime.now().hour
expected_profile = hourly_profiles[f"hour_{current_hour}"]
deviation = compare_distributions(current_logs, expected_profile)
```

**ì¥ì **:
- âœ… ì‹œê°„/ìš”ì¼ë³„ ìì—°ìŠ¤ëŸ¬ìš´ íŒ¨í„´ ë³€í™” ê³ ë ¤
- âœ… ê³„ì ˆì„±/ì£¼ê¸°ì„± ë°˜ì˜ ê°€ëŠ¥
- âœ… ì •í™•í•œ baseline ì œê³µ

### 4. **íŒŒì¼ë³„ ë¹„êµ íƒì§€**

**ì›ë¦¬**: ì—¬ëŸ¬ ë¡œê·¸ íŒŒì¼ ê°„ì˜ íŒ¨í„´ ì°¨ì´ë¥¼ ë¶„ì„í•˜ì—¬ ì´ìƒ ê°ì§€

**êµ¬í˜„ ì‹œë‚˜ë¦¬ì˜¤**:
```python
# ì„œë²„ë³„/ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ íŒŒì¼ ë¹„êµ
baseline_files = ["server1.log", "server2.log", "server3.log"]
target_file = "server4.log"  # ë¶„ì„í•  íŒŒì¼

# ê° íŒŒì¼ì˜ í…œí”Œë¦¿ ë¶„í¬ ì¶”ì¶œ
baseline_patterns = extract_template_distributions(baseline_files)
target_patterns = extract_template_distributions([target_file])

# ì°¨ì´ì  ë¶„ì„
anomalies = find_distribution_anomalies(target_patterns, baseline_patterns)
```

**ì¥ì **:
- âœ… ê°™ì€ í™˜ê²½ì˜ ë‹¤ë¥¸ ì‹œìŠ¤í…œê³¼ ë¹„êµ
- âœ… ì‹œìŠ¤í…œë³„ íŠ¹ì´ì‚¬í•­ ë°œê²¬
- âœ… ì¼ê´„ ë¶„ì„ìœ¼ë¡œ ë¹ ë¥¸ íŒë‹¨

### 5. **ì‹œê³„ì—´ ë¶„í•´ ê¸°ë°˜ íƒì§€**

**ì›ë¦¬**: ë¡œê·¸ ë³¼ë¥¨/íŒ¨í„´ì„ íŠ¸ë Œë“œ, ê³„ì ˆì„±, ì”ì°¨ë¡œ ë¶„í•´í•˜ì—¬ ë¶„ì„

```python
# STL Decomposition í™œìš©
from statsmodels.tsa.seasonal import STL

# ì‹œê°„ë³„ ë¡œê·¸ ë³¼ë¥¨ ì‹œê³„ì—´
hourly_volumes = df.groupby(df['timestamp'].dt.hour).size()

# ë¶„í•´: íŠ¸ë Œë“œ + ê³„ì ˆì„± + ì”ì°¨
decomposition = STL(hourly_volumes).fit()
residuals = decomposition.resid

# ì”ì°¨ì—ì„œ ì´ìƒì¹˜ íƒì§€
anomalies = detect_outliers_in_residuals(residuals)
```

### 6. **í´ëŸ¬ìŠ¤í„°ë§ ê¸°ë°˜ íƒì§€**

**ì›ë¦¬**: ë¡œê·¸ ìœˆë„ìš°ë“¤ì„ í´ëŸ¬ìŠ¤í„°ë§í•˜ì—¬ outlier ìœˆë„ìš° íƒì§€

```python
# ê° ìœˆë„ìš°ë¥¼ íŠ¹ì„± ë²¡í„°ë¡œ ë³€í™˜
window_features = [
    template_distribution,  # í…œí”Œë¦¿ ë¶„í¬
    timestamp_features,     # ì‹œê°„ íŠ¹ì„±
    volume_metrics,         # ë³¼ë¥¨ ì§€í‘œ
    sequence_patterns       # ìˆœì„œ íŒ¨í„´
]

# í´ëŸ¬ìŠ¤í„°ë§
from sklearn.cluster import DBSCAN
clusters = DBSCAN(eps=0.3).fit(window_features)

# ë…¸ì´ì¦ˆ(-1) í´ëŸ¬ìŠ¤í„°ê°€ ì´ìƒ
anomaly_windows = clusters.labels_ == -1
```

## âœ… í˜„ì¬ êµ¬í˜„ ìƒíƒœ

### Phase 1: ì‹œê°„ ê¸°ë°˜ í™•ì¥ (êµ¬í˜„ ì™„ë£Œ âœ…)
1. **âœ… ì‹œê°„ëŒ€ë³„ í”„ë¡œíŒŒì¼ íƒì§€** - `temporal_anomaly_detector.py`
2. **âœ… ìš”ì¼/ì›”ë³„ íŒ¨í„´ í•™ìŠµ** - ì‹œê°„ ê¸°ë°˜ ë¶„ì„ í¬í•¨
3. **âœ… Rolling window vs Fixed window ë¹„êµ** - ë‹¤ì–‘í•œ ìœˆë„ìš° í¬ê¸° ì§€ì›

### Phase 2: ë‹¤ì¤‘ íŒŒì¼ ë¶„ì„ (êµ¬í˜„ ì™„ë£Œ âœ…)
1. **âœ… íŒŒì¼ ê°„ íŒ¨í„´ ë¹„êµ** - `comparative_anomaly_detector.py`
2. **âœ… ì„œë²„ ê·¸ë£¹ë³„ baseline êµ¬ì¶•** - `enhanced_batch_analyzer.py`
3. **âœ… Cross-validation ë°©ì‹ ì´ìƒ íƒì§€** - ë‹¤ì¤‘ baseline ë¹„êµ

### Phase 3: ê³ ê¸‰ ë¶„ì„ (ë¶€ë¶„ êµ¬í˜„ ğŸš§)
1. **ğŸš§ ì‹œê³„ì—´ ë¶„í•´ íƒì§€** - ê¸°ë³¸ ì‹œê°„ ë¶„ì„ êµ¬í˜„ë¨
2. **âœ… ë”¥ëŸ¬ë‹ ê¸°ë°˜ sequence anomaly** - DeepLog LSTM ëª¨ë¸
3. **ğŸš§ Graph-based íŒ¨í„´ ë¶„ì„** - í–¥í›„ êµ¬í˜„ ì˜ˆì •

## ğŸ†• ìµœì‹  ì¶”ê°€ ê¸°ëŠ¥ (2025-09-20)

### 7. **ë² ì´ìŠ¤ë¼ì¸ í’ˆì§ˆ ê²€ì¦**
**ì›ë¦¬**: ë² ì´ìŠ¤ë¼ì¸ ë¡œê·¸ ìì²´ì˜ í’ˆì§ˆì„ í‰ê°€í•˜ì—¬ ë¬¸ì œ ìˆëŠ” íŒŒì¼ í•„í„°ë§

**êµ¬í˜„**: `baseline_validator.py`
```python
# í’ˆì§ˆ í‰ê°€ ì§€í‘œ
quality_metrics = {
    'error_rate': error_logs / total_logs,
    'warning_rate': warning_logs / total_logs,
    'template_diversity': unique_templates,
    'log_volume': total_logs,
    'rare_template_ratio': rare_templates / unique_templates
}

# í’ˆì§ˆ ì„ê³„ê°’
QUALITY_THRESHOLDS = {
    'max_error_rate': 0.02,      # 2% ì´í•˜
    'max_warning_rate': 0.05,    # 5% ì´í•˜
    'min_templates': 10,         # ìµœì†Œ 10ê°œ í…œí”Œë¦¿
    'min_logs': 100,            # ìµœì†Œ 100ê°œ ë¡œê·¸
    'max_rare_ratio': 0.3       # í¬ê·€ í…œí”Œë¦¿ 30% ì´í•˜
}
```

**ì¥ì **:
- âœ… ì´ìƒíƒì§€ ì •í™•ë„ í–¥ìƒ
- âœ… ìë™ í’ˆì§ˆ í•„í„°ë§
- âœ… ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” baseline ë³´ì¥

### 8. **ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„**
**ì›ë¦¬**: ì´ìƒíƒì§€ ê²°ê³¼ì—ì„œ ì‹¤ì œ ë¬¸ì œ ë¡œê·¸ë¥¼ ì¶”ì¶œí•˜ì—¬ ì‚¬ëŒì´ ì´í•´í•˜ê¸° ì‰½ê²Œ ì œê³µ

**êµ¬í˜„**: `log_sample_analyzer.py`
```python
# ìƒ˜í”Œ ì¶”ì¶œ ë° ë¶„ì„
def extract_anomaly_samples(anomaly_results, parsed_logs):
    samples = []
    for anomaly in anomaly_results:
        # ì´ìƒ êµ¬ê°„ì˜ ì‹¤ì œ ë¡œê·¸ ì¶”ì¶œ
        log_sample = get_log_context(parsed_logs, anomaly.line_range)
        
        # ì „í›„ ë§¥ë½ ì¶”ê°€
        context = get_surrounding_context(parsed_logs, anomaly.line_range, 3)
        
        # ì´ìƒ ì›ì¸ ë¶„ì„
        explanation = analyze_anomaly_pattern(log_sample, anomaly.type)
        
        samples.append({
            'sample': log_sample,
            'context': context,
            'explanation': explanation,
            'severity': classify_severity(anomaly)
        })
    
    return samples
```

**ì¥ì **:
- âœ… ì‹¤ì œ ë¬¸ì œ ë¡œê·¸ í™•ì¸ ê°€ëŠ¥
- âœ… ì „í›„ ë§¥ë½ìœ¼ë¡œ ìƒí™© íŒŒì•…
- âœ… ì‚¬ëŒ ì¹œí™”ì ì¸ ì„¤ëª… ì œê³µ

## ğŸ“Š ê° ë°©ë²•ì˜ ì ìš© ì‹œë‚˜ë¦¬ì˜¤

| ë°©ë²• | ì ìš© ì‹œë‚˜ë¦¬ì˜¤ | ì¥ì  | ë‹¨ì  |
|------|---------------|------|------|
| **í˜„ì¬ ìœˆë„ìš° ë°©ì‹** | ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ | ë¹ ë¥¸ íƒì§€, ë©”ëª¨ë¦¬ íš¨ìœ¨ | ì¥ê¸° íŒ¨í„´ ë†“ì¹¨ |
| **ë‚ ì§œë³„ í”„ë¡œíŒŒì¼** | ì •ê¸° ì ê²€, ìŠ¤ì¼€ì¤„ ë¶„ì„ | ì •í™•í•œ baseline | ë°ì´í„° ì¶•ì  í•„ìš” |
| **íŒŒì¼ë³„ ë¹„êµ** | ì‹œìŠ¤í…œ ê°„ ë¹„êµ, ë°°ì¹˜ ë¶„ì„ | ìƒëŒ€ì  ì´ìƒ íƒì§€ | ì‹œê°„ ì •ë³´ ì†ì‹¤ |
| **ì‹œê³„ì—´ ë¶„í•´** | ì¥ê¸° íŠ¸ë Œë“œ ë¶„ì„ | ê³„ì ˆì„± ê³ ë ¤ | ë³µì¡ì„± ì¦ê°€ |
| **í´ëŸ¬ìŠ¤í„°ë§** | íŒ¨í„´ ë°œê²¬, ê·¸ë£¹í™” | ìƒˆë¡œìš´ íŒ¨í„´ ë°œê²¬ | í•´ì„ ì–´ë ¤ì›€ |
| **ë² ì´ìŠ¤ë¼ì¸ í’ˆì§ˆê²€ì¦** | ë°°ì¹˜ ë¶„ì„, í’ˆì§ˆ ê´€ë¦¬ | ì •í™•ë„ í–¥ìƒ | ì´ˆê¸° ì„¤ì • í•„ìš” |
| **ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„** | ë¬¸ì œ ì§„ë‹¨, ì‚¬í›„ ë¶„ì„ | ì§ê´€ì  ì´í•´ | ì¶”ê°€ ì²˜ë¦¬ ì‹œê°„ |

## ğŸ¯ í†µí•© ì†”ë£¨ì…˜

í˜„ì¬ ì‹œìŠ¤í…œì€ **ëª¨ë“  ì£¼ìš” ë°©ë²•ë“¤ì´ í†µí•©**ë˜ì–´ `run_enhanced_batch_analysis.sh` í•˜ë‚˜ë¡œ ì‹¤í–‰ ê°€ëŠ¥í•©ë‹ˆë‹¤:

1. **âœ… ìë™ ë² ì´ìŠ¤ë¼ì¸ í’ˆì§ˆ ê²€ì¦**
2. **âœ… ì‹œê°„ ê¸°ë°˜ ì´ìƒíƒì§€**  
3. **âœ… íŒŒì¼ë³„ ë¹„êµ ì´ìƒíƒì§€**
4. **âœ… ë”¥ëŸ¬ë‹ ê¸°ë°˜ ì‹œí€€ìŠ¤ ë¶„ì„**
5. **âœ… ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ ì¶”ì¶œ ë° ë¶„ì„**

## ğŸš€ ì‚¬ìš© ë°©ë²•

```bash
# ëª¨ë“  ê¸°ëŠ¥ì„ í•œë²ˆì— ì‹¤í–‰
./run_enhanced_batch_analysis.sh /path/to/logs/

# ê²°ê³¼ í™•ì¸ (ì¤‘ìš”ë„ ìˆœ)
cat enhanced_analysis_*/ENHANCED_ANALYSIS_SUMMARY.md
cat enhanced_analysis_*/processed_*/log_samples_analysis/anomaly_analysis_report.md
```
