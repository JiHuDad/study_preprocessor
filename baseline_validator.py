#!/usr/bin/env python3
"""
Baseline íŒŒì¼ í’ˆì§ˆ ê²€ì¦ ë„êµ¬
- ì´ìƒí•œ baseline íŒŒì¼ë“¤ì„ ì‚¬ì „ì— í•„í„°ë§
- ë‹¤ì–‘í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ baseline í’ˆì§ˆ í‰ê°€
"""
import pandas as pd
import numpy as np
from pathlib import Path
from typing import List, Dict, Tuple, Optional
import argparse
import json

class BaselineValidator:
    def __init__(self):
        # ì •ìƒ baselineì˜ ê¸°ì¤€ê°’ë“¤ (ê²½í—˜ì  ì„ê³„ê°’)
        self.thresholds = {
            'max_error_rate': 0.02,      # 2% ì´í•˜ ì—ëŸ¬ìœ¨
            'max_warning_rate': 0.05,    # 5% ì´í•˜ ê²½ê³ ìœ¨  
            'min_template_count': 10,    # ìµœì†Œ 10ê°œ í…œí”Œë¦¿
            'max_rare_template_ratio': 0.3,  # í¬ê·€ í…œí”Œë¦¿ 30% ì´í•˜
            'min_log_count': 100,        # ìµœì†Œ 100ê°œ ë¡œê·¸
            'max_template_entropy': 10,  # í…œí”Œë¦¿ ì—”íŠ¸ë¡œí”¼ ìƒí•œ
            'min_template_entropy': 2,   # í…œí”Œë¦¿ ì—”íŠ¸ë¡œí”¼ í•˜í•œ
        }
    
    def validate_single_baseline(self, parsed_file: str) -> Dict:
        """ë‹¨ì¼ baseline íŒŒì¼ì„ ê²€ì¦í•©ë‹ˆë‹¤."""
        try:
            df = pd.read_parquet(parsed_file)
            file_name = Path(parsed_file).name
            
            # ê¸°ë³¸ í†µê³„ ê³„ì‚°
            total_logs = len(df)
            if total_logs == 0:
                return {
                    'file': file_name,
                    'valid': False,
                    'score': 0.0,
                    'issues': ['Empty dataset'],
                    'stats': {}
                }
            
            # í…œí”Œë¦¿ ë¶„ì„
            template_counts = df['template_id'].value_counts()
            unique_templates = len(template_counts)
            rare_templates = len([t for t, count in template_counts.items() if count == 1])
            rare_template_ratio = rare_templates / max(unique_templates, 1)
            
            # í…œí”Œë¦¿ ë¶„í¬ ì—”íŠ¸ë¡œí”¼
            template_probs = template_counts / total_logs
            template_entropy = -sum(p * np.log2(p) for p in template_probs if p > 0)
            
            # ì—ëŸ¬/ê²½ê³  ë¡œê·¸ ë¶„ì„
            error_keywords = ['error', 'ERROR', 'fail', 'FAIL', 'exception', 'Exception', 'panic', 'fatal', 'crash']
            warning_keywords = ['warn', 'WARN', 'warning', 'WARNING', 'deprecated']
            
            error_logs = df[df['raw'].str.contains('|'.join(error_keywords), case=False, na=False)]
            warning_logs = df[df['raw'].str.contains('|'.join(warning_keywords), case=False, na=False)]
            
            error_rate = len(error_logs) / total_logs
            warning_rate = len(warning_logs) / total_logs
            
            # ì‹œê°„ ì¼ê´€ì„± ê²€ì‚¬
            time_consistency = True
            time_gaps = []
            if 'timestamp' in df.columns:
                df_sorted = df.sort_values('timestamp').copy()
                df_sorted['timestamp'] = pd.to_datetime(df_sorted['timestamp'], errors='coerce')
                valid_timestamps = df_sorted.dropna(subset=['timestamp'])
                
                if len(valid_timestamps) > 1:
                    time_diffs = valid_timestamps['timestamp'].diff().dt.total_seconds()
                    # 1ì‹œê°„ ì´ìƒ ê°­ì´ ìˆìœ¼ë©´ ì˜ì‹¬
                    large_gaps = time_diffs[time_diffs > 3600]
                    time_gaps = large_gaps.tolist()
                    time_consistency = len(large_gaps) < total_logs * 0.1  # 10% ì´í•˜
            
            # ë¡œê·¸ ë°€ë„ ë¶„ì„ (ì‹œê°„ë‹¹ ë¡œê·¸ ìˆ˜)
            logs_per_hour = None
            if 'timestamp' in df.columns and len(df) > 1:
                try:
                    start_time = pd.to_datetime(df['timestamp'].min())
                    end_time = pd.to_datetime(df['timestamp'].max())
                    duration_hours = (end_time - start_time).total_seconds() / 3600
                    if duration_hours > 0:
                        logs_per_hour = total_logs / duration_hours
                except:
                    pass
            
            # í†µê³„ ì •ë¦¬
            stats = {
                'total_logs': total_logs,
                'unique_templates': unique_templates,
                'rare_templates': rare_templates,
                'rare_template_ratio': rare_template_ratio,
                'template_entropy': template_entropy,
                'error_rate': error_rate,
                'warning_rate': warning_rate,
                'time_consistency': time_consistency,
                'large_time_gaps': len(time_gaps),
                'logs_per_hour': logs_per_hour
            }
            
            # ê²€ì¦ ê²°ê³¼ ê³„ì‚°
            issues = []
            score = 1.0  # ì™„ë²½í•œ ì ìˆ˜ì—ì„œ ì‹œì‘í•˜ì—¬ ë¬¸ì œë§ˆë‹¤ ê°ì 
            
            # ê° ê¸°ì¤€ë³„ ê²€ì‚¬
            if error_rate > self.thresholds['max_error_rate']:
                issues.append(f"ë†’ì€ ì—ëŸ¬ìœ¨: {error_rate:.2%} (ê¸°ì¤€: {self.thresholds['max_error_rate']:.2%})")
                score -= 0.3
            
            if warning_rate > self.thresholds['max_warning_rate']:
                issues.append(f"ë†’ì€ ê²½ê³ ìœ¨: {warning_rate:.2%} (ê¸°ì¤€: {self.thresholds['max_warning_rate']:.2%})")
                score -= 0.2
            
            if unique_templates < self.thresholds['min_template_count']:
                issues.append(f"í…œí”Œë¦¿ ë¶€ì¡±: {unique_templates}ê°œ (ê¸°ì¤€: {self.thresholds['min_template_count']}ê°œ)")
                score -= 0.2
            
            if rare_template_ratio > self.thresholds['max_rare_template_ratio']:
                issues.append(f"í¬ê·€ í…œí”Œë¦¿ ê³¼ë‹¤: {rare_template_ratio:.2%} (ê¸°ì¤€: {self.thresholds['max_rare_template_ratio']:.2%})")
                score -= 0.1
            
            if total_logs < self.thresholds['min_log_count']:
                issues.append(f"ë¡œê·¸ ìˆ˜ ë¶€ì¡±: {total_logs}ê°œ (ê¸°ì¤€: {self.thresholds['min_log_count']}ê°œ)")
                score -= 0.3
            
            if template_entropy > self.thresholds['max_template_entropy']:
                issues.append(f"í…œí”Œë¦¿ ì—”íŠ¸ë¡œí”¼ ê³¼ë‹¤: {template_entropy:.2f} (ê¸°ì¤€: {self.thresholds['max_template_entropy']})")
                score -= 0.2
            
            if template_entropy < self.thresholds['min_template_entropy']:
                issues.append(f"í…œí”Œë¦¿ ë‹¤ì–‘ì„± ë¶€ì¡±: {template_entropy:.2f} (ê¸°ì¤€: {self.thresholds['min_template_entropy']})")
                score -= 0.1
            
            if not time_consistency:
                issues.append("ì‹œê°„ ì¼ê´€ì„± ë¬¸ì œ: í° ì‹œê°„ ê°­ì´ ë‹¤ìˆ˜ ì¡´ì¬")
                score -= 0.1
            
            # ìµœì¢… ì ìˆ˜ ì¡°ì •
            score = max(0.0, score)
            is_valid = score >= 0.7 and len(issues) <= 2  # 70% ì´ìƒ ì ìˆ˜, ë¬¸ì œ 2ê°œ ì´í•˜
            
            return {
                'file': file_name,
                'valid': is_valid,
                'score': score,
                'issues': issues,
                'stats': stats
            }
            
        except Exception as e:
            return {
                'file': Path(parsed_file).name,
                'valid': False,
                'score': 0.0,
                'issues': [f'ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}'],
                'stats': {}
            }
    
    def validate_multiple_baselines(self, baseline_files: List[str]) -> Dict:
        """ì—¬ëŸ¬ baseline íŒŒì¼ë“¤ì„ ê²€ì¦í•˜ê³  í•„í„°ë§í•©ë‹ˆë‹¤."""
        
        print(f"ğŸ” {len(baseline_files)}ê°œ baseline íŒŒì¼ ê²€ì¦ ì¤‘...")
        
        results = []
        for baseline_file in baseline_files:
            print(f"   ê²€ì¦: {Path(baseline_file).name}")
            result = self.validate_single_baseline(baseline_file)
            results.append(result)
        
        # ê²°ê³¼ ë¶„ì„
        valid_baselines = [r for r in results if r['valid']]
        invalid_baselines = [r for r in results if not r['valid']]
        
        # ìƒí˜¸ ì¼ê´€ì„± ê²€ì‚¬ (validí•œ ê²ƒë“¤ë¼ë¦¬)
        if len(valid_baselines) > 1:
            consistency_issues = self._check_mutual_consistency(valid_baselines, baseline_files)
        else:
            consistency_issues = []
        
        print(f"\nğŸ“Š ê²€ì¦ ê²°ê³¼:")
        print(f"   âœ… ìœ íš¨: {len(valid_baselines)}ê°œ")
        print(f"   âŒ ë¬´íš¨: {len(invalid_baselines)}ê°œ")
        print(f"   âš ï¸  ì¼ê´€ì„± ë¬¸ì œ: {len(consistency_issues)}ê°œ")
        
        return {
            'total_files': len(baseline_files),
            'valid_count': len(valid_baselines),
            'invalid_count': len(invalid_baselines),
            'valid_baselines': valid_baselines,
            'invalid_baselines': invalid_baselines,
            'consistency_issues': consistency_issues,
            'recommended_baselines': [r['file'] for r in valid_baselines if r['score'] >= 0.8]
        }
    
    def _check_mutual_consistency(self, valid_results: List[Dict], baseline_files: List[str]) -> List[Dict]:
        """ìœ íš¨í•œ baselineë“¤ ê°„ì˜ ìƒí˜¸ ì¼ê´€ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤."""
        
        consistency_issues = []
        
        # ì£¼ìš” ì§€í‘œë“¤ì˜ ë¶„í¬ ë¶„ì„
        metrics = ['error_rate', 'warning_rate', 'template_entropy', 'logs_per_hour']
        
        for metric in metrics:
            values = []
            for result in valid_results:
                val = result['stats'].get(metric)
                if val is not None:
                    values.append(val)
            
            if len(values) < 2:
                continue
                
            mean_val = np.mean(values)
            std_val = np.std(values)
            
            # Outlier íƒì§€ (2Ïƒ ë°–ì˜ ê°’ë“¤)
            for i, (result, val) in enumerate(zip(valid_results, values)):
                if abs(val - mean_val) > 2 * std_val:
                    consistency_issues.append({
                        'file': result['file'],
                        'metric': metric,
                        'value': val,
                        'mean': mean_val,
                        'std': std_val,
                        'description': f'{metric}ì´ ë‹¤ë¥¸ baselineë“¤ê³¼ {abs(val - mean_val)/max(mean_val, 1e-6)*100:.1f}% ì°¨ì´'
                    })
        
        return consistency_issues
    
    def generate_validation_report(self, validation_result: Dict, output_file: str = None) -> str:
        """ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        report = f"""# Baseline íŒŒì¼ ê²€ì¦ ë¦¬í¬íŠ¸

## ğŸ“Š ì „ì²´ ìš”ì•½
- **ì´ íŒŒì¼ ìˆ˜**: {validation_result['total_files']}ê°œ
- **ìœ íš¨í•œ íŒŒì¼**: {validation_result['valid_count']}ê°œ
- **ë¬´íš¨í•œ íŒŒì¼**: {validation_result['invalid_count']}ê°œ
- **ì¶”ì²œ íŒŒì¼**: {len(validation_result['recommended_baselines'])}ê°œ

## âœ… ìœ íš¨í•œ Baseline íŒŒì¼ë“¤

"""
        
        for result in validation_result['valid_baselines']:
            stats = result['stats']
            report += f"""### {result['file']}
- **í’ˆì§ˆ ì ìˆ˜**: {result['score']:.2f}/1.0
- **ë¡œê·¸ ìˆ˜**: {stats.get('total_logs', 0):,}ê°œ
- **í…œí”Œë¦¿ ìˆ˜**: {stats.get('unique_templates', 0)}ê°œ
- **ì—ëŸ¬ìœ¨**: {stats.get('error_rate', 0):.2%}
- **ê²½ê³ ìœ¨**: {stats.get('warning_rate', 0):.2%}
- **í…œí”Œë¦¿ ì—”íŠ¸ë¡œí”¼**: {stats.get('template_entropy', 0):.2f}
"""
            if result['issues']:
                report += f"- **ê²½ë¯¸í•œ ì´ìŠˆ**: {', '.join(result['issues'])}\n"
            report += "\n"
        
        if validation_result['invalid_baselines']:
            report += "## âŒ ë¬´íš¨í•œ Baseline íŒŒì¼ë“¤\n\n"
            for result in validation_result['invalid_baselines']:
                report += f"""### {result['file']}
- **í’ˆì§ˆ ì ìˆ˜**: {result['score']:.2f}/1.0
- **ì£¼ìš” ë¬¸ì œ**: {', '.join(result['issues'])}

"""
        
        if validation_result['consistency_issues']:
            report += "## âš ï¸  ì¼ê´€ì„± ë¬¸ì œ\n\n"
            for issue in validation_result['consistency_issues']:
                report += f"- **{issue['file']}**: {issue['description']}\n"
            report += "\n"
        
        report += f"""## ğŸ’¡ ê¶Œì¥ì‚¬í•­

### ì‚¬ìš© ê¶Œì¥ íŒŒì¼ë“¤
{chr(10).join('- ' + f for f in validation_result['recommended_baselines'])}

### í’ˆì§ˆ ê°œì„  ë°©ë²•
1. **ì—ëŸ¬ìœ¨ì´ ë†’ì€ íŒŒì¼ë“¤**: í•´ë‹¹ ì‹œê¸°ì˜ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
2. **ë¡œê·¸ ìˆ˜ê°€ ì ì€ íŒŒì¼ë“¤**: ë” ê¸´ ê¸°ê°„ì˜ ë¡œê·¸ ìˆ˜ì§‘
3. **í…œí”Œë¦¿ ë‹¤ì–‘ì„± ë¬¸ì œ**: ë‹¤ì–‘í•œ ì‹œìŠ¤í…œ í™œë™ì´ í¬í•¨ëœ ê¸°ê°„ ì„ íƒ
4. **ì‹œê°„ ì¼ê´€ì„± ë¬¸ì œ**: ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ë‹¨ êµ¬ê°„ í™•ì¸

### ì´ìƒíƒì§€ ì •í™•ë„ í–¥ìƒì„ ìœ„í•œ ì¡°ì¹˜
- í’ˆì§ˆ ì ìˆ˜ 0.8 ì´ìƒ íŒŒì¼ë“¤ë§Œ baselineìœ¼ë¡œ ì‚¬ìš©
- ìµœì†Œ 3ê°œ ì´ìƒì˜ ìœ íš¨í•œ baseline í™•ë³´
- ì¼ê´€ì„± ë¬¸ì œê°€ ìˆëŠ” íŒŒì¼ë“¤ì€ ì œì™¸ ê³ ë ¤
"""
        
        if output_file:
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ“„ ê²€ì¦ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")
        
        return report

def validate_baseline_files(baseline_files: List[str], output_dir: str = None):
    """Baseline íŒŒì¼ë“¤ì„ ê²€ì¦í•˜ê³  ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    
    if output_dir:
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
    
    validator = BaselineValidator()
    
    # ê²€ì¦ ì‹¤í–‰
    result = validator.validate_multiple_baselines(baseline_files)
    
    # ê²°ê³¼ ì €ì¥
    if output_dir:
        # JSON ê²°ê³¼ ì €ì¥
        with open(output_path / "validation_result.json", 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False, default=str)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = validator.generate_validation_report(result, output_path / "baseline_validation_report.md")
        
        print(f"ğŸ“‚ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_path}")
    else:
        validator.generate_validation_report(result)
    
    return result

def main():
    parser = argparse.ArgumentParser(description="Baseline íŒŒì¼ í’ˆì§ˆ ê²€ì¦ ë„êµ¬")
    parser.add_argument("baseline_files", nargs='+', help="ê²€ì¦í•  baseline íŒŒì¼ë“¤ (parsed.parquet)")
    parser.add_argument("--output-dir", help="ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬")
    parser.add_argument("--score-threshold", type=float, default=0.7, help="ìœ íš¨ baseline ìµœì†Œ ì ìˆ˜ (ê¸°ë³¸: 0.7)")
    
    args = parser.parse_args()
    
    # ì„ê³„ê°’ ì—…ë°ì´íŠ¸
    validator = BaselineValidator()
    
    result = validate_baseline_files(args.baseline_files, args.output_dir)
    
    print(f"\nğŸ¯ ê¶Œì¥ baseline íŒŒì¼ë“¤:")
    for file in result['recommended_baselines']:
        print(f"   âœ… {file}")

if __name__ == "__main__":
    main()
