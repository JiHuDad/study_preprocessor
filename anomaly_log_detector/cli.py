"""í†µí•© CLI ëª¨ë“ˆ

- ëª©ì : íŒŒì´í”„ë¼ì¸ ì „ ê³¼ì •ì„ CLI ëª…ë ¹ìœ¼ë¡œ ì œê³µ (íŒŒì‹± â†’ ë² ì´ìŠ¤ë¼ì¸/DeepLog/MS-CRED â†’ ë¶„ì„/ë¦¬í¬íŠ¸ â†’ í‰ê°€/ë³€í™˜)
- ì£¼ìš” ëª…ë ¹:
  * parse: ì›ì‹œ ë¡œê·¸ íŒŒì‹±/ë§ˆìŠ¤í‚¹ â†’ parsed.parquet
  * detect: ë² ì´ìŠ¤ë¼ì¸ ì´ìƒíƒì§€ â†’ baseline_scores.parquet
  * build-deeplog / deeplog-train / deeplog-infer(-enhanced): DeepLog ì…ë ¥/í•™ìŠµ/ì¶”ë¡ 
  * build-mscred / mscred-train / mscred-infer: MS-CRED ì…ë ¥/í•™ìŠµ/ì¶”ë¡ 
  * report: ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„±
  * analyze-*: ë‹¤ì–‘í•œ ë¶„ì„ ë„êµ¬ ë˜í•‘ (temporal/comparative/mscred/log samples)
  * eval: ë² ì´ìŠ¤ë¼ì¸/DeepLogì— ëŒ€í•œ PRF1 í‰ê°€
  * convert-onnx / hybrid-pipeline: ëª¨ë¸ ë³€í™˜ ë° í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
"""

import os  # í™˜ê²½/ê²½ë¡œ ìœ í‹¸
from pathlib import Path  # ê²½ë¡œ íƒ€ì…
from typing import Optional  # ì„ íƒì  íƒ€ì…
import json  # JSON ì…ì¶œë ¥
import click  # CLI í”„ë ˆì„ì›Œí¬
import pandas as pd  # ë°ì´í„° ì²˜ë¦¬

from .preprocess import LogPreprocessor, PreprocessConfig  # ì „ì²˜ë¦¬ ìœ í‹¸
from .detect import baseline_detect, BaselineParams  # ë² ì´ìŠ¤ë¼ì¸ íƒì§€
from .builders.deeplog import (  # DeepLog íŒŒì´í”„ë¼ì¸ í•¨ìˆ˜ë“¤
    build_deeplog_inputs, train_deeplog, infer_deeplog_topk,
    infer_deeplog_enhanced, EnhancedInferenceConfig
)
from .builders.mscred import build_mscred_window_counts  # MS-CRED ì…ë ¥ ìƒì„±
from .synth import (  # í•©ì„± ë¡œê·¸ ìƒì„±ê¸°ë“¤
    generate_synthetic_log,
    generate_training_data,
    generate_inference_normal,
    generate_inference_anomaly,
)
from .eval import evaluate_baseline, evaluate_deeplog  # í‰ê°€ ìœ í‹¸


@click.group()  # ë£¨íŠ¸ ì»¤ë§¨ë“œ ê·¸ë£¹
def main() -> None:  # ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
    """Anomaly Log Detector: Comprehensive log anomaly detection framework with DeepLog, MS-CRED, and baseline methods"""  # CLI ì„¤ëª…


@main.command()  # parse ëª…ë ¹ ë“±ë¡
@click.option("--input", "input_path", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True)  # ì…ë ¥ ë¡œê·¸ íŒŒì¼
@click.option("--out-dir", type=click.Path(file_okay=False, path_type=Path), required=True)  # ì¶œë ¥ ë””ë ‰í† ë¦¬
@click.option("--drain-state", type=click.Path(dir_okay=False, path_type=Path), default=None, help="Drain3 ìƒíƒœ íŒŒì¼ ê²½ë¡œ")  # Drain3 ìƒíƒœ íŒŒì¼
# Masking toggles  # ë§ˆìŠ¤í‚¹ ì˜µì…˜ ìŠ¤ìœ„ì¹˜(ë¹„í™œì„±í™” í† ê¸€)
@click.option("--no-mask-paths", is_flag=True, default=False)  # ê²½ë¡œ ë§ˆìŠ¤í‚¹ í•´ì œ
@click.option("--no-mask-hex", is_flag=True, default=False)  # 16ì§„ìˆ˜ ë§ˆìŠ¤í‚¹ í•´ì œ
@click.option("--no-mask-ips", is_flag=True, default=False)  # IP ë§ˆìŠ¤í‚¹ í•´ì œ
@click.option("--no-mask-mac", is_flag=True, default=False)  # MAC ë§ˆìŠ¤í‚¹ í•´ì œ
@click.option("--no-mask-uuid", is_flag=True, default=False)  # UUID ë§ˆìŠ¤í‚¹ í•´ì œ
@click.option("--no-mask-pid", is_flag=True, default=False)  # PID ë§ˆìŠ¤í‚¹ í•´ì œ
@click.option("--no-mask-device", is_flag=True, default=False)  # ë””ë°”ì´ìŠ¤ë²ˆí˜¸ ë§ˆìŠ¤í‚¹ í•´ì œ
@click.option("--no-mask-num", is_flag=True, default=False)  # ì¼ë°˜ ìˆ«ì ë§ˆìŠ¤í‚¹ í•´ì œ
def parse(input_path: Path, out_dir: Path, drain_state: Path | None,
          no_mask_paths: bool, no_mask_hex: bool, no_mask_ips: bool, no_mask_mac: bool,
          no_mask_uuid: bool, no_mask_pid: bool, no_mask_device: bool, no_mask_num: bool) -> None:  # ë¡œê·¸ íŒŒì‹± ì—”íŠ¸ë¦¬
    """ì›ì‹œ ë¡œê·¸ íŒŒì¼ì„ íŒŒì‹±/ë§ˆìŠ¤í‚¹í•˜ê³  Parquetìœ¼ë¡œ ì €ì¥."""  # ëª…ë ¹ ì„¤ëª…
    out_dir.mkdir(parents=True, exist_ok=True)  # ì¶œë ¥ í´ë” ìƒì„±
    cfg = PreprocessConfig(  # ì „ì²˜ë¦¬ ì„¤ì • êµ¬ì„±
        drain_state_path=str(drain_state) if drain_state else None,  # Drain ìƒíƒœ ê²½ë¡œ
        mask_paths=not no_mask_paths,  # ê²½ë¡œ ë§ˆìŠ¤í‚¹ ì—¬ë¶€
        mask_hex=not no_mask_hex,  # 16ì§„ìˆ˜ ë§ˆìŠ¤í‚¹ ì—¬ë¶€
        mask_ips=not no_mask_ips,  # IP ë§ˆìŠ¤í‚¹ ì—¬ë¶€
        mask_mac=not no_mask_mac,  # MAC ë§ˆìŠ¤í‚¹ ì—¬ë¶€
        mask_uuid=not no_mask_uuid,  # UUID ë§ˆìŠ¤í‚¹ ì—¬ë¶€
        mask_pid_fields=not no_mask_pid,  # PID í•„ë“œ ë§ˆìŠ¤í‚¹ ì—¬ë¶€
        mask_device_numbers=not no_mask_device,  # ë””ë°”ì´ìŠ¤ ë²ˆí˜¸ ë§ˆìŠ¤í‚¹ ì—¬ë¶€
        mask_numbers=not no_mask_num,  # ì¼ë°˜ ìˆ«ì ë§ˆìŠ¤í‚¹ ì—¬ë¶€
    )
    pre = LogPreprocessor(cfg)  # ì „ì²˜ë¦¬ê¸° ìƒì„±
    df = pre.process_file(str(input_path))  # íŒŒì¼ íŒŒì‹±/ë§ˆìŠ¤í‚¹ ìˆ˜í–‰
    parquet_path = out_dir / "parsed.parquet"  # ì¶œë ¥ íŒŒì¼ ê²½ë¡œ
    df.to_parquet(parquet_path, index=False)  # Parquet ì €ì¥
    # ë¯¸ë¦¬ë³´ê¸°ìš© ì¼ë¶€ ìƒ˜í”Œë„ JSON ì €ì¥
    preview = df.head(10).to_dict(orient="records")  # ìƒìœ„ ìƒ˜í”Œ ì¶”ì¶œ
    (out_dir / "preview.json").write_text(json.dumps(preview, ensure_ascii=False, default=str, indent=2))  # JSON ì €ì¥
    click.echo(f"Saved: {parquet_path}")  # ê²°ê³¼ ê²½ë¡œ ì¶œë ¥
    click.echo(f"Preview: {out_dir / 'preview.json'}")  # ë¯¸ë¦¬ë³´ê¸° ê²½ë¡œ ì¶œë ¥


@main.command()  # detect ëª…ë ¹ ë“±ë¡
@click.option("--parsed", "parsed_parquet", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True)  # parsed.parquet ê²½ë¡œ
@click.option("--out-dir", type=click.Path(file_okay=False, path_type=Path), required=True)  # ì¶œë ¥ ë””ë ‰í† ë¦¬
@click.option("--window-size", type=int, default=50)  # ìœˆë„ìš° í¬ê¸°
@click.option("--stride", type=int, default=25)  # ìŠ¤íŠ¸ë¼ì´ë“œ
@click.option("--ewm-alpha", type=float, default=0.3)  # EWM ì•ŒíŒŒ
@click.option("--q", "anomaly_q", type=float, default=0.95, help="Anomaly quantile threshold")  # ì´ìƒ ì„ê³„ ë°±ë¶„ìœ„
def detect(parsed_parquet: Path, out_dir: Path, window_size: int, stride: int, ewm_alpha: float, anomaly_q: float) -> None:  # ë² ì´ìŠ¤ë¼ì¸ ì‹¤í–‰
    """ë² ì´ìŠ¤ë¼ì¸ ì´ìƒíƒì§€(ìƒˆ í…œí”Œë¦¿ ë¹„ìœ¨ + ë¹ˆë„ ê¸‰ë³€) ì‹¤í–‰."""  # ì„¤ëª…
    params = BaselineParams(window_size=window_size, stride=stride, ewm_alpha=ewm_alpha, anomaly_quantile=anomaly_q)  # íŒŒë¼ë¯¸í„° êµ¬ì„±
    out_path = baseline_detect(str(parsed_parquet), str(out_dir), params)  # íƒì§€ ì‹¤í–‰
    click.echo(f"Saved baseline scores: {out_path}")  # ê²°ê³¼ ì¶œë ¥


@main.command("build-deeplog")  # DeepLog ì…ë ¥ ìƒì„± ëª…ë ¹
@click.option("--parsed", "parsed_parquet", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True)  # parsed.parquet ê²½ë¡œ
@click.option("--out-dir", type=click.Path(file_okay=False, path_type=Path), required=True)  # ì¶œë ¥ ë””ë ‰í† ë¦¬
def build_deeplog_cmd(parsed_parquet: Path, out_dir: Path) -> None:  # DeepLog ì…ë ¥ ìƒì„±
    """DeepLog ì…ë ¥(vocab, sequences) ìƒì„±."""  # ì„¤ëª…
    build_deeplog_inputs(str(parsed_parquet), str(out_dir))  # ìƒì„± ì‹¤í–‰
    click.echo(f"Built DeepLog inputs under: {out_dir}")  # ì™„ë£Œ ë©”ì‹œì§€


@main.command("deeplog-train")  # DeepLog í•™ìŠµ ëª…ë ¹
@click.option("--seq", "sequences_parquet", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True)  # sequences.parquet
@click.option("--vocab", "vocab_json", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True)  # vocab.json
@click.option("--out", "model_out", type=click.Path(dir_okay=False, path_type=Path), required=True)  # ëª¨ë¸ ì €ì¥ ê²½ë¡œ
@click.option("--seq-len", type=int, default=50)  # ì‹œí€€ìŠ¤ ê¸¸ì´
@click.option("--epochs", type=int, default=3)  # ì—í­ ìˆ˜
def deeplog_train_cmd(sequences_parquet: Path, vocab_json: Path, model_out: Path, seq_len: int, epochs: int) -> None:  # í•™ìŠµ ì‹¤í–‰
    path = train_deeplog(str(sequences_parquet), str(vocab_json), str(model_out), seq_len=seq_len, epochs=epochs)  # í•™ìŠµ
    click.echo(f"Saved DeepLog model: {path}")  # ì €ì¥ ê²½ë¡œ ì¶œë ¥


@main.command("deeplog-infer")  # DeepLog ì¶”ë¡ (top-k)
@click.option("--seq", "sequences_parquet", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True)  # sequences.parquet
@click.option("--model", "model_path", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True)  # ëª¨ë¸ ê²½ë¡œ
@click.option("--vocab", "vocab_path", type=click.Path(exists=True, dir_okay=False, path_type=Path), default=None, help="vocab.json ê²½ë¡œ (ì˜ˆì¸¡/ì‹¤ì œ í…œí”Œë¦¿ í‘œì‹œìš©)")  # vocab ê²½ë¡œ
@click.option("--k", type=int, default=3)  # Top-K
def deeplog_infer_cmd(sequences_parquet: Path, model_path: Path, vocab_path: Path | None, k: int) -> None:  # ì¶”ë¡  ì‹¤í–‰
    """DeepLog ì¶”ë¡  (ê¸°ë³¸ top-k ë°©ì‹)."""  # ì„¤ëª…
    df = infer_deeplog_topk(str(sequences_parquet), str(model_path), vocab_path=str(vocab_path) if vocab_path else None, k=k)  # ì¶”ë¡  ìˆ˜í–‰
    out = Path(sequences_parquet).with_name("deeplog_infer.parquet")  # ì¶œë ¥ ê²½ë¡œ
    df.to_parquet(out, index=False)  # ì €ì¥
    rate = 1.0 - float(df["in_topk"].mean()) if len(df) > 0 else 0.0  # ìœ„ë°˜ìœ¨ ê³„ì‚°
    click.echo(f"Saved inference: {out} (violation_rate={rate:.3f})")  # ê²°ê³¼ ì¶œë ¥


@main.command("deeplog-infer-enhanced")  # í–¥ìƒëœ DeepLog ì¶”ë¡ 
@click.option("--seq", "sequences_parquet", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True, help="sequences.parquet íŒŒì¼ ê²½ë¡œ")  # ì‹œí€€ìŠ¤ íŒŒì¼
@click.option("--parsed", "parsed_parquet", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True, help="parsed.parquet íŒŒì¼ ê²½ë¡œ")  # íŒŒì‹± íŒŒì¼
@click.option("--model", "model_path", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True, help="DeepLog ëª¨ë¸ ê²½ë¡œ")  # ëª¨ë¸ ê²½ë¡œ
@click.option("--vocab", "vocab_path", type=click.Path(exists=True, dir_okay=False, path_type=Path), default=None, help="vocab.json ê²½ë¡œ (ë…¸ë²¨í‹° íƒì§€ìš©)")  # vocab ê²½ë¡œ
@click.option("--top-k", type=int, default=3, help="Top-K ê°’ (top-p ë¯¸ì„¤ì • ì‹œ ì‚¬ìš©)")  # Top-K
@click.option("--top-p", type=float, default=None, help="Top-P ê°’ (ì„¤ì • ì‹œ top-kë³´ë‹¤ ìš°ì„ )")  # Top-P
@click.option("--k-of-n-k", type=int, default=7, help="K-of-N íŒì •: Nê°œ ì¤‘ Kê°œ ì´ìƒ ì‹¤íŒ¨ ì‹œ ì•Œë¦¼")  # K-of-Nì˜ K
@click.option("--k-of-n-n", type=int, default=10, help="K-of-N íŒì •: ìŠ¬ë¼ì´ë”© ìœˆë„ìš° í¬ê¸°")  # K-of-Nì˜ N
@click.option("--cooldown-seq", type=int, default=60, help="ì‹œí€€ìŠ¤ ì‹¤íŒ¨ ì¿¨ë‹¤ìš´ (ì´ˆ)")  # SEQ ì¿¨ë‹¤ìš´
@click.option("--cooldown-novelty", type=int, default=60, help="ë…¸ë²¨í‹° ì¿¨ë‹¤ìš´ (ì´ˆ)")  # ë…¸ë²¨í‹° ì¿¨ë‹¤ìš´
@click.option("--session-timeout", type=int, default=300, help="ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ (ì´ˆ)")  # ì„¸ì…˜ íƒ€ì„ì•„ì›ƒ
@click.option("--entity-column", type=str, default="host", help="ì—”í‹°í‹° ì»¬ëŸ¼ëª… (host, process ë“±)")  # ì—”í‹°í‹° ì»¬ëŸ¼
@click.option("--no-novelty", is_flag=True, default=False, help="ë…¸ë²¨í‹° íƒì§€ ë¹„í™œì„±í™”")  # ë…¸ë²¨í‹° ë¹„í™œì„±í™”
@click.option("--out-dir", type=click.Path(file_okay=False, path_type=Path), default=None, help="ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: sequences.parquetê³¼ ê°™ì€ í´ë”)")  # ì¶œë ¥ í´ë”
def deeplog_infer_enhanced_cmd(
    sequences_parquet: Path,
    parsed_parquet: Path,
    model_path: Path,
    vocab_path: Optional[Path],
    top_k: int,
    top_p: Optional[float],
    k_of_n_k: int,
    k_of_n_n: int,
    cooldown_seq: int,
    cooldown_novelty: int,
    session_timeout: int,
    entity_column: str,
    no_novelty: bool,
    out_dir: Optional[Path]
) -> None:  # í–¥ìƒëœ ì¶”ë¡  ì‹¤í–‰
    """
    Enhanced DeepLog ì¶”ë¡ : top-k/top-p, K-of-N íŒì •, ì¿¨ë‹¤ìš´, ë…¸ë²¨í‹° íƒì§€, ì„¸ì…˜í™” ì§€ì›.

    ì•Œë¦¼ í­ì£¼ë¥¼ ë°©ì§€í•˜ê³  ì—”í‹°í‹°ë³„ ì„¸ì…˜ ê¸°ë°˜ ì´ìƒ íƒì§€ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì„¤ì •  # ê¸°ë³¸: ì‹œí€€ìŠ¤ íŒŒì¼ í´ë”
    if out_dir is None:
        out_dir = sequences_parquet.parent
    out_dir.mkdir(parents=True, exist_ok=True)

    # ì„¤ì • ìƒì„±  # ì•Œë¦¼/ìœˆë„ìš°/ì¿¨ë‹¤ìš´/ë…¸ë²¨í‹° ì˜µì…˜ í¬í•¨
    config = EnhancedInferenceConfig(
        top_k=top_k,
        top_p=top_p,
        k_of_n_k=k_of_n_k,
        k_of_n_n=k_of_n_n,
        cooldown_seq_fail=cooldown_seq,
        cooldown_novelty=cooldown_novelty,
        session_timeout=session_timeout,
        entity_column=entity_column,
        novelty_enabled=not no_novelty,
        vocab_path=str(vocab_path) if vocab_path else None
    )

    click.echo("ğŸš€ Enhanced DeepLog ì¶”ë¡  ì‹œì‘...")  # ì‹œì‘ ë¡œê·¸
    click.echo(f"  ğŸ“Š Top-{'P' if top_p else 'K'}: {top_p if top_p else top_k}")  # Top ì„ íƒ
    click.echo(f"  ğŸ¯ K-of-N: {k_of_n_k}/{k_of_n_n}")  # K-of-N ì •ë³´
    click.echo(f"  â° Cooldown: SEQ={cooldown_seq}s, NOVELTY={cooldown_novelty}s")  # ì¿¨ë‹¤ìš´
    click.echo(f"  ğŸ” ë…¸ë²¨í‹° íƒì§€: {'ON' if not no_novelty else 'OFF'}")  # ë…¸ë²¨í‹° On/Off
    click.echo(f"  ğŸ‘¤ ì—”í‹°í‹°: {entity_column}")  # ì—”í‹°í‹° ì»¬ëŸ¼

    # Enhanced inference ì‹¤í–‰  # ì„¸ë¶€/ì•Œë¦¼/ìš”ì•½ ë°˜í™˜
    detailed_df, alerts_df, summary = infer_deeplog_enhanced(
        str(sequences_parquet),
        str(parsed_parquet),
        str(model_path),
        config
    )

    # ê²°ê³¼ ì €ì¥  # íŒŒì¼ ì¶œë ¥ ê²½ë¡œ
    detailed_out = out_dir / "deeplog_enhanced_detailed.parquet"
    alerts_out = out_dir / "deeplog_enhanced_alerts.parquet"
    summary_out = out_dir / "deeplog_enhanced_summary.json"

    detailed_df.to_parquet(detailed_out, index=False)  # ìƒì„¸ ì €ì¥
    alerts_df.to_parquet(alerts_out, index=False)  # ì•Œë¦¼ ì €ì¥

    import json  # ìš”ì•½ JSON ì €ì¥
    with open(summary_out, 'w') as f:  # íŒŒì¼ ì—´ê¸°
        # datetimeì„ ë¬¸ìì—´ë¡œ ë³€í™˜  # ì§ë ¬í™” ì²˜ë¦¬
        summary_serializable = {}
        for key, value in summary.items():  # í‚¤ë³„ ì²˜ë¦¬
            if key == "novelty_aggregation":  # ë…¸ë²¨í‹° ì§‘ê³„ êµ¬ì¡°
                serializable_agg = {}
                for agg_key, agg_val in value.items():  # í•­ëª©ë³„ ë³€í™˜
                    serializable_agg[agg_key] = {
                        "count": agg_val["count"],
                        "first": agg_val["first"].isoformat() if hasattr(agg_val["first"], "isoformat") else str(agg_val["first"]),
                        "last": agg_val["last"].isoformat() if hasattr(agg_val["last"], "isoformat") else str(agg_val["last"])
                    }
                summary_serializable[key] = serializable_agg
            else:
                summary_serializable[key] = value
        json.dump(summary_serializable, f, indent=2)  # JSON ë¤í”„

    # ê²°ê³¼ ì¶œë ¥  # ì½˜ì†” ìš”ì•½
    click.echo("\nâœ… Enhanced DeepLog ì¶”ë¡  ì™„ë£Œ!")
    click.echo(f"\nğŸ“Š ìš”ì•½:")
    click.echo(f"  ì „ì²´ ì‹œí€€ìŠ¤: {summary['total_sequences']:,}ê°œ")
    click.echo(f"  ì‹¤íŒ¨ ì‹œí€€ìŠ¤: {summary['total_failures']:,}ê°œ")
    click.echo(f"  ë…¸ë²¨í‹° ë°œê²¬: {summary['total_novels']:,}ê°œ")
    click.echo(f"  ë°œìƒ ì•Œë¦¼: {summary['total_alerts']:,}ê°œ")

    if summary.get('alerts_by_type'):  # ìœ í˜•ë³„ ì•Œë¦¼ ê°œìˆ˜
        click.echo(f"\nğŸš¨ ì•Œë¦¼ ìœ í˜•ë³„:")
        for alert_type, count in summary['alerts_by_type'].items():
            click.echo(f"  - {alert_type}: {count}ê°œ")

    click.echo(f"\nğŸ“ ì¶œë ¥ íŒŒì¼:")  # ì‚°ì¶œë¬¼ ê²½ë¡œ ì•ˆë‚´
    click.echo(f"  ìƒì„¸ ê²°ê³¼: {detailed_out}")
    click.echo(f"  ì•Œë¦¼ ëª©ë¡: {alerts_out}")
    click.echo(f"  ìš”ì•½ ì •ë³´: {summary_out}")

    # ì•Œë¦¼ì´ ìˆìœ¼ë©´ ìƒ˜í”Œ í‘œì‹œ  # ìƒìœ„ 5ê°œ ì¶œë ¥
    if not alerts_df.empty:
        click.echo(f"\nğŸ”” ìµœê·¼ ì•Œë¦¼ ìƒ˜í”Œ (ìµœëŒ€ 5ê°œ):")
        for _, alert in alerts_df.head(5).iterrows():
            timestamp = alert['timestamp']
            entity = alert['entity']
            alert_type = alert['alert_type']
            template_id = alert.get('template_id', 'N/A')
            click.echo(f"  [{timestamp}] {entity} - {alert_type} (template: {template_id})")


@main.command("build-mscred")  # MS-CRED ì…ë ¥ ìƒì„±
@click.option("--parsed", "parsed_parquet", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True)  # parsed.parquet
@click.option("--out-dir", type=click.Path(file_okay=False, path_type=Path), required=True)  # ì¶œë ¥ í´ë”
@click.option("--window-size", type=int, default=50)  # ìœˆë„ìš° í¬ê¸°
@click.option("--stride", type=int, default=25)  # ìŠ¤íŠ¸ë¼ì´ë“œ
@click.option("--template-mapping-path", type=click.Path(dir_okay=False, path_type=Path), default=None, help="ê¸°ì¡´ template_mapping.json ê²½ë¡œ (ì¶”ë¡  ì‹œ ì‚¬ìš©)")  # í…œí”Œë¦¿ ë§¤í•‘ ê²½ë¡œ
def build_mscred_cmd(parsed_parquet: Path, out_dir: Path, window_size: int, stride: int, template_mapping_path: Path | None) -> None:  # ìƒì„± ì‹¤í–‰
    """MS-CRED ì…ë ¥(ìœˆë„ìš° ì¹´ìš´íŠ¸) ìƒì„±."""  # ì„¤ëª…
    build_mscred_window_counts(
        str(parsed_parquet),
        str(out_dir),
        window_size=window_size,
        stride=stride,
        template_mapping_path=str(template_mapping_path) if template_mapping_path else None
    )  # ìƒì„±
    click.echo(f"Built MS-CRED window counts under: {out_dir}")  # ì™„ë£Œ ë©”ì‹œì§€


@main.command("mscred-train")  # MS-CRED í•™ìŠµ
@click.option("--window-counts", "window_counts_parquet", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True)  # window_counts.parquet
@click.option("--out", "model_output", type=click.Path(dir_okay=False, path_type=Path), required=True)  # ëª¨ë¸ ì¶œë ¥ ê²½ë¡œ
@click.option("--epochs", type=int, default=50)  # ì—í­ ìˆ˜
def mscred_train_cmd(window_counts_parquet: Path, model_output: Path, epochs: int) -> None:  # í•™ìŠµ ì‹¤í–‰
    """MS-CRED ëª¨ë¸ í•™ìŠµ."""  # ì„¤ëª…
    from .mscred_model import train_mscred  # ì§€ì—° ì„í¬íŠ¸
    
    model_output.parent.mkdir(parents=True, exist_ok=True)  # í´ë” ìƒì„±
    stats = train_mscred(str(window_counts_parquet), str(model_output), epochs)  # í•™ìŠµ
    
    click.echo(f"MS-CRED í•™ìŠµ ì™„ë£Œ: {model_output}")  # ì™„ë£Œ
    click.echo(f"ìµœì¢… í•™ìŠµ ì†ì‹¤: {stats['final_train_loss']:.4f}")  # í•™ìŠµ ì†ì‹¤
    click.echo(f"ìµœì¢… ê²€ì¦ ì†ì‹¤: {stats['final_val_loss']:.4f}")  # ê²€ì¦ ì†ì‹¤


@main.command("mscred-infer")  # MS-CRED ì¶”ë¡ 
@click.option("--window-counts", "window_counts_parquet", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True)  # window_counts.parquet
@click.option("--model", "model_path", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True)  # ëª¨ë¸ ê²½ë¡œ
@click.option("--threshold", type=float, default=None, help="ìˆ˜ë™ ì„ê³„ê°’ ì§€ì • (ì§€ì •í•˜ì§€ ì•Šìœ¼ë©´ í•™ìŠµ ì‹œ ê³„ì‚°ëœ ì„ê³„ê°’ ì‚¬ìš©)")  # ìˆ˜ë™ ì„ê³„ê°’
@click.option("--threshold-method", type=click.Choice(['99percentile', '95percentile', '99.9percentile', '3sigma', 'mad']),
              default='99percentile', help="í•™ìŠµ ì‹œ ì €ì¥ëœ ì„ê³„ê°’ ì„ íƒ ë°©ë²• (ê¸°ë³¸ê°’: 99percentile)")  # ì„ê³„ê°’ ì„ íƒ ë°©ë²•
def mscred_infer_cmd(window_counts_parquet: Path, model_path: Path, threshold: float | None, threshold_method: str) -> None:  # ì¶”ë¡  ì‹¤í–‰
    """MS-CRED ì´ìƒ íƒì§€ ì¶”ë¡ ."""  # ì„¤ëª…
    from .mscred_model import infer_mscred  # ì§€ì—° ì„í¬íŠ¸

    out = Path(window_counts_parquet).with_name("mscred_infer.parquet")  # ì¶œë ¥ ê²½ë¡œ
    results_df = infer_mscred(str(window_counts_parquet), str(model_path), str(out), threshold, threshold_method)  # ì¶”ë¡ 

    anomaly_rate = results_df['is_anomaly'].mean()  # ì´ìƒë¥ 
    click.echo(f"Saved MS-CRED inference: {out}")  # ê²½ë¡œ
    click.echo(f"Anomaly rate: {anomaly_rate:.3f} ({results_df['is_anomaly'].sum()}/{len(results_df)})")  # ìš”ì•½


def _generate_enhanced_report(processed_dir: Path, with_samples: bool = True) -> str:
    """ê°œì„ ëœ ì½ê¸° ì‰¬ìš´ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print(f"[DEBUG] _generate_enhanced_report ì‹œì‘", flush=True)
    print(f"[DEBUG]   processed_dir={processed_dir}", flush=True)
    print(f"[DEBUG]   with_samples={with_samples}", flush=True)

    import pandas as pd
    from datetime import datetime
    print(f"[DEBUG] pandas, datetime import ì™„ë£Œ", flush=True)

    # ë¦¬í¬íŠ¸ í—¤ë”
    report = f"""# ğŸ“Š ë¡œê·¸ ì´ìƒ íƒì§€ ë¶„ì„ ë¦¬í¬íŠ¸

**ìƒì„± ì‹œê°„**: {datetime.now().strftime("%Y-%m-%d %H:%M:%S")}
**ë¶„ì„ ëŒ€ìƒ**: `{processed_dir}`

---

"""
    print(f"[DEBUG] ë¦¬í¬íŠ¸ í—¤ë” ìƒì„± ì™„ë£Œ", flush=True)

    # ë°ì´í„° ë¡œë“œ
    parsed_path = processed_dir / "parsed.parquet"
    # baseline_scores_enhanced.parquet ìš°ì„ , ì—†ìœ¼ë©´ baseline_scores.parquet
    base_path_enhanced = processed_dir / "baseline_scores_enhanced.parquet"
    base_path = base_path_enhanced if base_path_enhanced.exists() else processed_dir / "baseline_scores.parquet"
    deeplog_path = processed_dir / "deeplog_infer.parquet"
    mscred_path = processed_dir / "mscred_infer.parquet"
    vocab_path = processed_dir / "vocab.json"

    print(f"[DEBUG] íŒŒì¼ ê²½ë¡œ ì„¤ì •:", flush=True)
    print(f"[DEBUG]   parsed_path={parsed_path} (exists={parsed_path.exists()})", flush=True)
    print(f"[DEBUG]   base_path={base_path} (exists={base_path.exists()})", flush=True)
    print(f"[DEBUG]   deeplog_path={deeplog_path} (exists={deeplog_path.exists()})", flush=True)
    print(f"[DEBUG]   mscred_path={mscred_path} (exists={mscred_path.exists()})", flush=True)
    print(f"[DEBUG]   vocab_path={vocab_path} (exists={vocab_path.exists()})", flush=True)

    has_data = False

    # ì¢…í•© ìš”ì•½ ì„¹ì…˜
    report += "## ğŸ“‹ ì¢…í•© ìš”ì•½\n\n"
    report += "| íƒì§€ ë°©ë²• | ì´ìƒë¥  | ì‹¬ê°ë„ | ìƒíƒœ |\n"
    report += "|---------|--------|--------|------|\n"

    baseline_rate = None
    deeplog_viol = None
    mscred_rate = None

    # Baseline ë°ì´í„°
    if base_path.exists():
        s = pd.read_parquet(base_path)
        if len(s) > 0:
            has_data = True
            baseline_rate = float((s["is_anomaly"] == True).mean())
            severity = "ğŸŸ¢ ë‚®ìŒ" if baseline_rate < 0.05 else ("ğŸŸ¡ ì¤‘ê°„" if baseline_rate < 0.20 else "ğŸ”´ ë†’ìŒ")
            status = "ì •ìƒ ë²”ìœ„" if baseline_rate < 0.05 else ("ì¼ë¶€ ì´ìƒ ë°œê²¬" if baseline_rate < 0.20 else "ë‹¤ìˆ˜ ì´ìƒ ë°œê²¬")
            report += f"| Baseline (í†µê³„) | {baseline_rate:.1%} | {severity} | {status} |\n"

    # DeepLog ë°ì´í„°
    if deeplog_path.exists():
        d = pd.read_parquet(deeplog_path)
        if len(d) > 0:
            has_data = True
            # Enhanced ë²„ì „: prediction_ok ì‚¬ìš©, ê¸°ì¡´ ë²„ì „: in_topk ì‚¬ìš©
            if "prediction_ok" in d.columns:
                deeplog_viol = 1.0 - float(d["prediction_ok"].mean())
            elif "in_topk" in d.columns:
                deeplog_viol = 1.0 - float(d["in_topk"].mean())
            else:
                deeplog_viol = 0.0  # ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’
            severity = "ğŸŸ¢ ë‚®ìŒ" if deeplog_viol < 0.20 else ("ğŸŸ¡ ì¤‘ê°„" if deeplog_viol < 0.50 else "ğŸ”´ ë†’ìŒ")
            status = "ì˜ˆì¸¡ ê°€ëŠ¥" if deeplog_viol < 0.20 else ("íŒ¨í„´ ë³µì¡" if deeplog_viol < 0.50 else "ì˜ˆì¸¡ ì–´ë ¤ì›€")
            report += f"| DeepLog (ë”¥ëŸ¬ë‹) | {deeplog_viol:.1%} | {severity} | {status} |\n"

    # MS-CRED ë°ì´í„°
    if mscred_path.exists():
        m = pd.read_parquet(mscred_path)
        if len(m) > 0:
            has_data = True
            mscred_rate = float(m["is_anomaly"].mean())
            severity = "ğŸŸ¢ ë‚®ìŒ" if mscred_rate < 0.05 else ("ğŸŸ¡ ì¤‘ê°„" if mscred_rate < 0.20 else "ğŸ”´ ë†’ìŒ")
            status = "ì •ìƒ ë²”ìœ„" if mscred_rate < 0.05 else ("ì¼ë¶€ ì´ìƒ" if mscred_rate < 0.20 else "ë‹¤ìˆ˜ ì´ìƒ")
            report += f"| MS-CRED (ë©€í‹°ìŠ¤ì¼€ì¼) | {mscred_rate:.1%} | {severity} | {status} |\n"

    if not has_data:
        return "# ğŸ“Š ë¡œê·¸ ì´ìƒ íƒì§€ ë¶„ì„ ë¦¬í¬íŠ¸\n\n**ê²°ê³¼ ì—†ìŒ**: ë¶„ì„í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.\n"

    # ì£¼ìš” ë°œê²¬ì‚¬í•­
    report += "\n### âš ï¸ ì£¼ìš” ë°œê²¬ì‚¬í•­\n\n"
    findings = []

    if deeplog_viol and deeplog_viol > 0.50:
        findings.append(f"- **DeepLog ìœ„ë°˜ìœ¨ì´ {deeplog_viol:.1%}ë¡œ ë§¤ìš° ë†’ìŒ** â†’ ë¡œê·¸ íŒ¨í„´ì´ ë¶ˆê·œì¹™í•˜ê±°ë‚˜ í•™ìŠµ ë°ì´í„° ë¶€ì¡±")
    if baseline_rate and baseline_rate > 0.20:
        findings.append(f"- **Baselineì—ì„œ {baseline_rate:.1%}ì˜ ìœˆë„ìš°ì—ì„œ ì´ìƒ íƒì§€** â†’ ë‹¤ìˆ˜ êµ¬ê°„ì— ìƒˆë¡œìš´ íŒ¨í„´ ë°œê²¬")
    elif baseline_rate and baseline_rate > 0.05:
        findings.append(f"- **Baselineì—ì„œ {baseline_rate:.1%}ì˜ ìœˆë„ìš°ì—ì„œ ì´ìƒ íƒì§€** â†’ ì¼ë¶€ êµ¬ê°„ì— ìƒˆë¡œìš´ íŒ¨í„´ ë°œê²¬")
    if mscred_rate and mscred_rate > 0.20:
        findings.append(f"- **MS-CREDì—ì„œ {mscred_rate:.1%}ì˜ ì´ìƒ íƒì§€** â†’ ë¡œê·¸ íŒ¨í„´ êµ¬ì¡°ê°€ ë¹„ì •ìƒì ")

    if not findings:
        findings.append("- âœ… ëª¨ë“  íƒì§€ ë°©ë²•ì—ì„œ ì •ìƒ ë²”ìœ„ ë‚´ì˜ ê²°ê³¼")

    report += "\n".join(findings) + "\n\n---\n\n"

    # Baseline ìƒì„¸ ë¶„ì„
    if base_path.exists():
        s = pd.read_parquet(base_path)
        if len(s) > 0:
            report += "## ğŸ” Baseline ì´ìƒ íƒì§€ (í†µê³„ ê¸°ë°˜)\n\n"
            anomalous = s[s["is_anomaly"] == True]
            report += f"**ì´ìƒ ìœˆë„ìš°**: ì „ì²´ì˜ {baseline_rate:.1%} ({len(s)}ê°œ ì¤‘ {len(anomalous)}ê°œ)\n\n"

            if len(anomalous) > 0:
                report += "### ìƒìœ„ ì´ìƒ ìœˆë„ìš°\n\n"
                report += "| ì‹œì‘ ë¼ì¸ | ì´ìƒ ì ìˆ˜ | ìƒˆ í…œí”Œë¦¿ ë¹„ìœ¨ | ë¹ˆë„ Z-score |\n"
                report += "|---------|---------|--------------|-------------|\n"

                top_windows = s.sort_values("score", ascending=False).head(5)
                for _, row in top_windows.iterrows():
                    start = int(row["window_start_line"])
                    score = float(row["score"])
                    unseen = float(row.get("unseen_rate", 0))
                    freq_z = float(row.get("freq_z", 0))
                    report += f"| {start} | {score:.3f} | {unseen:.1%} | {freq_z:.2f} |\n"

                # ì‹¤ì œ ë¡œê·¸ ìƒ˜í”Œ ì¶”ê°€
                if parsed_path.exists():
                    dfp = pd.read_parquet(parsed_path)

                    # vocab ë¡œë“œí•˜ì—¬ í…œí”Œë¦¿ ID -> í…œí”Œë¦¿ í…ìŠ¤íŠ¸ ë§¤í•‘
                    template_map = {}
                    if vocab_path.exists():
                        import json
                        with open(vocab_path, 'r') as f:
                            vocab = json.load(f)
                        # vocabì€ template_id -> index ë§¤í•‘ì´ë¯€ë¡œ, templateë¥¼ dfpì—ì„œ ì¶”ì¶œ
                        for tid in dfp['template_id'].unique():
                            template_rows = dfp[dfp['template_id'] == tid]
                            if len(template_rows) > 0 and 'template' in template_rows.columns:
                                template_map[tid] = str(template_rows.iloc[0]['template'])

                    report += "\n### ğŸ“ ìƒìœ„ ìœˆë„ìš° ìƒì„¸ ë¶„ì„\n\n"
                    for _, row in top_windows.head(3).iterrows():  # ìƒìœ„ 3ê°œë§Œ
                        start = int(row["window_start_line"])
                        score = float(row["score"])
                        unseen = float(row.get("unseen_rate", 0))

                        report += f"#### ìœˆë„ìš° #{start}~ (ì ìˆ˜: {score:.3f})\n\n"

                        win_logs = dfp[(dfp["line_no"] >= start) & (dfp["line_no"] < start + 50)]
                        if len(win_logs) > 0:
                            # ì£¼ìš” í…œí”Œë¦¿ ë¶„ì„
                            top_templates = win_logs["template_id"].value_counts().head(3)
                            report += "**ì£¼ìš” í…œí”Œë¦¿ë“¤**:\n"
                            for tid, count in top_templates.items():
                                template_text = template_map.get(tid, f"Template ID: {tid}")
                                report += f"- `{template_text}` - {count}íšŒ ì¶œí˜„\n"

                            # ì‹¤ì œ ë¡œê·¸ ìƒ˜í”Œ (ì—ëŸ¬ ìš°ì„ )
                            error_logs = win_logs[win_logs['raw'].str.contains(
                                r'error|Error|ERROR|fail|Fail|FAIL|exception|Exception|warning|Warning|critical|Critical',
                                case=False, na=False, regex=True
                            )]

                            sample_logs = error_logs.head(3) if len(error_logs) > 0 else win_logs.head(3)

                            report += "\n**ì‹¤ì œ ë¡œê·¸ ìƒ˜í”Œ**:\n```\n"
                            for _, log in sample_logs.iterrows():
                                timestamp = log.get('timestamp', 'N/A')
                                raw = log.get('raw', 'N/A')
                                report += f"[{timestamp}] {raw}\n"
                            report += "```\n\n"

    # DeepLog ìƒì„¸ ë¶„ì„
    if deeplog_path.exists():
        d = pd.read_parquet(deeplog_path)
        if len(d) > 0:
            report += "---\n\n## ğŸ§  DeepLog ì´ìƒ íƒì§€ (ë”¥ëŸ¬ë‹ LSTM)\n\n"
            # Enhanced ë²„ì „: prediction_ok ì‚¬ìš©, ê¸°ì¡´ ë²„ì „: in_topk ì‚¬ìš©
            if "prediction_ok" in d.columns:
                violations = d[d["prediction_ok"] == False]
            elif "in_topk" in d.columns:
                violations = d[d["in_topk"] == False]
            else:
                violations = pd.DataFrame()  # ë¹ˆ DataFrame
            report += f"**ì˜ˆì¸¡ ì‹¤íŒ¨ìœ¨**: {deeplog_viol:.1%} (ì „ì²´ {len(d)}ê°œ ì¤‘ {len(violations)}ê°œ ì‹¤íŒ¨)\n\n"

            interpretation = ""
            if deeplog_viol < 0.20:
                interpretation = "âœ… **ì–‘í˜¸**: ë¡œê·¸ íŒ¨í„´ì´ ì¼ê´€ë˜ê³  ì˜ˆì¸¡ ê°€ëŠ¥í•©ë‹ˆë‹¤."
            elif deeplog_viol < 0.50:
                interpretation = "âš ï¸ **ì£¼ì˜**: ë¡œê·¸ íŒ¨í„´ì´ ë‹¤ì†Œ ë³µì¡í•˜ê±°ë‚˜ í•™ìŠµ ë°ì´í„°ê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
            else:
                interpretation = "ğŸ”´ **ê²½ê³ **: ë¡œê·¸ íŒ¨í„´ì´ ë§¤ìš° ë¶ˆê·œì¹™í•˜ê±°ë‚˜ ë¹„ì •ìƒì ì…ë‹ˆë‹¤. ì •ìƒ ë¡œê·¸ë¡œ ëª¨ë¸ ì¬í•™ìŠµì„ ê¶Œì¥í•©ë‹ˆë‹¤."

            report += f"**í•´ì„**: {interpretation}\n\n"

            # ì˜ˆì¸¡ ì‹¤íŒ¨ ìƒ˜í”Œ í‘œì‹œ (ì˜ˆì¸¡ê°’ vs ì‹¤ì œê°’)
            if len(violations) > 0 and with_samples:
                report += "### ğŸ” ì˜ˆì¸¡ ì‹¤íŒ¨ ìƒìœ„ ìƒ˜í”Œ\n\n"
                report += "ëª¨ë¸ì´ ì˜ˆì¸¡í•˜ì§€ ëª»í•œ íŒ¨í„´ë“¤ì…ë‹ˆë‹¤. ê° ìƒ˜í”Œì€ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ê³¼ ì‹¤ì œ ë°œìƒí•œ ê°’ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n\n"

                # vocabì´ ìˆëŠ”ì§€ í™•ì¸ (í…œí”Œë¦¿ ë¬¸ìì—´ í‘œì‹œìš©)
                has_template_info = "target_template" in d.columns and "predicted_templates" in d.columns

                if has_template_info:
                    # í…œí”Œë¦¿ ë¬¸ìì—´ ì •ë³´ê°€ ìˆëŠ” ê²½ìš°
                    sample_count = 0
                    for idx, row in violations.head(5).iterrows():
                        sample_count += 1
                        report += f"#### ìƒ˜í”Œ {sample_count}\n\n"
                        report += "| í•­ëª© | ë‚´ìš© |\n"
                        report += "|------|------|\n"
                        report += f"| **ì‹¤ì œ ë°œìƒ** | `{row.get('target_template', 'N/A')}` |\n"
                        report += f"| **ëª¨ë¸ ì˜ˆì¸¡ (Top-K)** | `{row.get('predicted_templates', 'N/A')}` |\n"
                        report += f"| **ë¶„ì„** | ëª¨ë¸ì´ ì˜ˆì¸¡í•œ íŒ¨í„´ê³¼ ë‹¤ë¥¸ ë¡œê·¸ê°€ ë°œìƒí•˜ì—¬ ì´ìƒìœ¼ë¡œ íƒì§€ë˜ì—ˆìŠµë‹ˆë‹¤. |\n\n"
                else:
                    # ì¸ë±ìŠ¤ ì •ë³´ë§Œ ìˆëŠ” ê²½ìš°
                    report += "| ìƒ˜í”Œ | ì‹¤ì œ í…œí”Œë¦¿ ì¸ë±ìŠ¤ | ì˜ˆì¸¡ Top-1 | ì˜ˆì¸¡ Top-2 | ì˜ˆì¸¡ Top-3 |\n"
                    report += "|------|-------------------|-----------|-----------|------------|\n"

                    for idx, row in violations.head(5).iterrows():
                        target = row.get('target', 'N/A')
                        pred1 = row.get('predicted_top1', '-')
                        pred2 = row.get('predicted_top2', '-')
                        pred3 = row.get('predicted_top3', '-')
                        report += f"| #{idx} | {target} | {pred1} | {pred2} | {pred3} |\n"

                    report += "\n**ì°¸ê³ **: vocab.jsonì„ ì‚¬ìš©í•˜ì—¬ ì¶”ë¡ í•˜ë©´ ì‹¤ì œ í…œí”Œë¦¿ ë¬¸ìì—´ì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"
                    report += "```bash\n"
                    report += "alog-detect deeplog-infer --seq sequences.parquet --model model.pth --vocab vocab.json\n"
                    report += "```\n\n"

    # MS-CRED ìƒì„¸ ë¶„ì„
    if mscred_path.exists():
        m = pd.read_parquet(mscred_path)
        if len(m) > 0:
            report += "---\n\n## ğŸ”¬ MS-CRED ì´ìƒ íƒì§€ (ë©€í‹°ìŠ¤ì¼€ì¼ ì˜¤í† ì¸ì½”ë”)\n\n"
            anomalous = m[m["is_anomaly"] == True]
            report += f"**ì´ìƒ ìœˆë„ìš°**: ì „ì²´ì˜ {mscred_rate:.1%} ({len(m)}ê°œ ì¤‘ {len(anomalous)}ê°œ)\n\n"

            if len(anomalous) > 0:
                report += "### ìƒìœ„ ì¬êµ¬ì„± ì˜¤ë¥˜\n\n"
                report += "| ìœˆë„ìš° ì¸ë±ìŠ¤ | ì¬êµ¬ì„± ì˜¤ë¥˜ | ì„ê³„ê°’ | ì˜¤ë¥˜/ì„ê³„ê°’ ë¹„ìœ¨ |\n"
                report += "|------------|-----------|--------|---------------|\n"

                top_errors = m.nlargest(5, 'reconstruction_error')
                for _, row in top_errors.iterrows():
                    idx = int(row["window_idx"])
                    error = float(row["reconstruction_error"])
                    threshold = float(row.get("threshold", 0))
                    ratio = error / threshold if threshold > 0 else 0
                    report += f"| {idx} | {error:.4f} | {threshold:.4f} | {ratio:.2f}x |\n"
                report += "\n"

    # ê¶Œì¥ì‚¬í•­
    report += "---\n\n## ğŸ’¡ ê¶Œì¥ì‚¬í•­\n\n"
    report += "### ğŸ”´ ì¦‰ì‹œ ì¡°ì¹˜ í•„ìš”\n\n"

    recommendations = []
    if baseline_rate and baseline_rate > 0.05:
        recommendations.append("- **ì´ìƒ ìœˆë„ìš° êµ¬ê°„ í™•ì¸**: Baselineì—ì„œ ë°œê²¬ëœ ì´ìƒ êµ¬ê°„ì˜ ë¡œê·¸ë¥¼ ìƒì„¸ ë¶„ì„í•˜ì„¸ìš”")
    if deeplog_viol and deeplog_viol > 0.50:
        recommendations.append("- **DeepLog ëª¨ë¸ ì¬í•™ìŠµ**: ì •ìƒ ë¡œê·¸ íŒ¨í„´ìœ¼ë¡œ ëª¨ë¸ì„ ì¬í•™ìŠµí•˜ì—¬ ì •í™•ë„ë¥¼ ê°œì„ í•˜ì„¸ìš”")
    if mscred_rate and mscred_rate > 0.20:
        recommendations.append("- **ë¡œê·¸ íŒ¨í„´ êµ¬ì¡° ë¶„ì„**: MS-CREDì—ì„œ ë°œê²¬ëœ êµ¬ì¡°ì  ì´ìƒì„ ë¶„ì„í•˜ì„¸ìš”")

    if not recommendations:
        recommendations.append("- âœ… í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœê°€ ì–‘í˜¸í•©ë‹ˆë‹¤. ì •ê¸°ì ì¸ ëª¨ë‹ˆí„°ë§ì„ ê³„ì†í•˜ì„¸ìš”")

    report += "\n".join(recommendations) + "\n\n"

    report += "### ğŸŸ¡ ì¶”ê°€ ë¶„ì„ ê¶Œì¥\n\n"
    report += "- **ì‹œê°„ëŒ€ë³„ ë¶„ì„**: `alog-detect analyze-temporal --data-dir <dir>` ì‹¤í–‰\n"
    report += "- **ìƒì„¸ ë¡œê·¸ ìƒ˜í”Œ í™•ì¸**: `alog-detect analyze-samples --processed-dir <dir>` ì‹¤í–‰\n"
    report += "- **ë¹„êµ ë¶„ì„**: ì—¬ëŸ¬ ì‹œìŠ¤í…œ ê°„ ë¡œê·¸ íŒ¨í„´ ë¹„êµ\n\n"

    if with_samples:
        report += "---\n\n"
        report += "## ğŸ“„ ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸\n\n"
        sample_report = processed_dir / "log_samples_analysis" / "anomaly_analysis_report.md"
        if sample_report.exists():
            report += f"âœ… ì‹¤ì œ ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.\n\n"
            report += f"**ìƒì„¸ ë¦¬í¬íŠ¸**: `{sample_report}`\n\n"
        else:
            report += "â³ ìƒì„¸ ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ì´ ì§„í–‰ ì¤‘ì…ë‹ˆë‹¤...\n\n"
    else:
        report += "---\n\n"
        report += "ğŸ’¡ **Tip**: `--with-samples` ì˜µì…˜ì„ ì‚¬ìš©í•˜ë©´ ì‹¤ì œ ì´ìƒ ë¡œê·¸ ìƒ˜í”Œê³¼ ìƒì„¸ ë¶„ì„ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n"

    report += "---\n\n"
    report += f"**ì°¸ê³ **: ì´ ë¦¬í¬íŠ¸ëŠ” ìë™ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì„± ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"

    return report


@main.command("report")  # ë¦¬í¬íŠ¸ ìƒì„±
@click.option("--processed-dir", type=click.Path(file_okay=False, path_type=Path), required=True)  # ì‚°ì¶œë¬¼ ë””ë ‰í† ë¦¬
@click.option("--with-samples/--no-samples", default=True, help="ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ í¬í•¨ (ê¸°ë³¸: í¬í•¨)")  # ìƒ˜í”Œ ë¶„ì„ ì˜µì…˜
def report_cmd(processed_dir: Path, with_samples: bool) -> None:  # ë¦¬í¬íŠ¸ ì‹¤í–‰
    """ì‚°ì¶œë¬¼ ìš”ì•½ ë¦¬í¬íŠ¸ ìƒì„± (ê°œì„ ëœ ì½ê¸° ì‰¬ìš´ í˜•ì‹)."""  # ì„¤ëª…
    print(f"[DEBUG] report_cmd í•¨ìˆ˜ ì‹œì‘", flush=True)
    print(f"[DEBUG] processed_dir = {processed_dir}", flush=True)
    print(f"[DEBUG] with_samples = {with_samples}", flush=True)

    import pandas as pd  # ì§€ì—­ ì„í¬íŠ¸
    print(f"[DEBUG] pandas import ì™„ë£Œ", flush=True)

    processed_dir.mkdir(parents=True, exist_ok=True)  # í´ë” ìƒì„±
    print(f"[DEBUG] ë””ë ‰í† ë¦¬ ìƒì„± ì™„ë£Œ: {processed_dir}", flush=True)

    click.echo("ğŸ“Š ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    print(f"[DEBUG] click.echo í˜¸ì¶œ ì™„ë£Œ", flush=True)

    # ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ë¨¼ì € ì‹¤í–‰ (with_samples=Trueì¸ ê²½ìš°)
    if with_samples:
        print(f"[DEBUG] with_samples=True, ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ì‹œì‘", flush=True)
        click.echo("ğŸ” ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ì¤‘...")
        try:
            print(f"[DEBUG] log_samples ëª¨ë“ˆ import ì‹œë„", flush=True)
            from .analyzers.log_samples import main as log_samples_main
            import sys
            print(f"[DEBUG] log_samples import ì™„ë£Œ", flush=True)

            # Save current sys.argv
            old_argv = sys.argv
            sys.argv = [
                "analyze-samples",
                str(processed_dir),
                "--output-dir", str(processed_dir / "log_samples_analysis")
            ]
            print(f"[DEBUG] sys.argv ì„¤ì • ì™„ë£Œ: {sys.argv}", flush=True)

            try:  # ë¶„ì„ ì‹¤í–‰ ë³´í˜¸
                print(f"[DEBUG] log_samples_main() í˜¸ì¶œ", flush=True)
                log_samples_main()
                print(f"[DEBUG] log_samples_main() ì™„ë£Œ", flush=True)
                click.echo("âœ… ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ì™„ë£Œ")
            finally:
                sys.argv = old_argv
                print(f"[DEBUG] sys.argv ë³µì› ì™„ë£Œ", flush=True)
        except Exception as e:
            print(f"[DEBUG] ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}", flush=True)
            import traceback
            traceback.print_exc()
            click.echo(f"âš ï¸ ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ì¤‘ ì˜¤ë¥˜: {e}", err=True)
    else:
        print(f"[DEBUG] with_samples=False, ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ìŠ¤í‚µ", flush=True)

    # ê°œì„ ëœ ë¦¬í¬íŠ¸ ìƒì„±
    print(f"[DEBUG] _generate_enhanced_report() í˜¸ì¶œ ì‹œì‘", flush=True)
    try:
        report_content = _generate_enhanced_report(processed_dir, with_samples)
        print(f"[DEBUG] _generate_enhanced_report() ì™„ë£Œ, ê¸¸ì´={len(report_content)}", flush=True)
    except Exception as e:
        print(f"[DEBUG] _generate_enhanced_report() ì¤‘ ì˜ˆì™¸: {e}", flush=True)
        import traceback
        traceback.print_exc()
        raise

    # ë¦¬í¬íŠ¸ ì €ì¥
    print(f"[DEBUG] ë¦¬í¬íŠ¸ íŒŒì¼ ì“°ê¸° ì‹œì‘", flush=True)
    out_md = processed_dir / "report.md"
    out_md.write_text(report_content)
    print(f"[DEBUG] ë¦¬í¬íŠ¸ íŒŒì¼ ì“°ê¸° ì™„ë£Œ: {out_md}", flush=True)

    click.echo(f"\nâœ… ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ!")
    click.echo(f"ğŸ“„ ì£¼ìš” ë¦¬í¬íŠ¸: {out_md}")
    print(f"[DEBUG] report_cmd í•¨ìˆ˜ ì¢…ë£Œ", flush=True)

    if with_samples:  # ìƒ˜í”Œ ë¶„ì„ ê²½ë¡œ ì¶œë ¥
        sample_report = processed_dir / "log_samples_analysis" / "anomaly_analysis_report.md"
        if sample_report.exists():
            click.echo(f"ğŸ“‹ ìƒì„¸ ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„: {sample_report}")
        sample_data = processed_dir / "log_samples_analysis" / "anomaly_samples.json"
        if sample_data.exists():
            click.echo(f"ğŸ“Š ìƒ˜í”Œ ë°ì´í„°: {sample_data}")


@main.command("gen-synth")  # í•©ì„± ë¡œê·¸ ìƒì„±
@click.option("--out", "out_path", type=click.Path(dir_okay=False, path_type=Path), required=True)  # ì¶œë ¥ ê²½ë¡œ
@click.option("--lines", "num_lines", type=int, default=5000)  # ë¼ì¸ ìˆ˜
@click.option("--anomaly-rate", type=float, default=0.02)  # ì´ìƒ ë¹„ìœ¨
def gen_synth_cmd(out_path: Path, num_lines: int, anomaly_rate: float) -> None:  # ìƒì„± ì‹¤í–‰
    """í•©ì„± ì¥ê¸° ë¡œê·¸ ìƒì„± (ì •ìƒ+ì´ìƒ í˜¼í•©)."""  # ì„¤ëª…
    p = generate_synthetic_log(str(out_path), num_lines=num_lines, anomaly_rate=anomaly_rate)  # ìƒì„± í˜¸ì¶œ
    click.echo(f"âœ… Generated synthetic log: {p}")  # ê²°ê³¼ ì¶œë ¥
    click.echo(f"ğŸ“Š Labels: {p}.labels.parquet")


@main.command("gen-training-data")  # í•™ìŠµìš© ë°ì´í„° ìƒì„±
@click.option("--out", "out_path", type=click.Path(dir_okay=False, path_type=Path), required=True)  # ì¶œë ¥ ê²½ë¡œ
@click.option("--lines", "num_lines", type=int, default=10000, help="ìƒì„±í•  ë¡œê·¸ ë¼ì¸ ìˆ˜")  # ë¼ì¸ ìˆ˜
@click.option("--host", default="train-host", help="í˜¸ìŠ¤íŠ¸ëª…")  # í˜¸ìŠ¤íŠ¸ëª…
def gen_training_data_cmd(out_path: Path, num_lines: int, host: str) -> None:  # í•™ìŠµ ë°ì´í„° ìƒì„± ì‹¤í–‰
    """í•™ìŠµìš© ì •ìƒ ë¡œê·¸ ë°ì´í„° ìƒì„± (100% ì •ìƒ ë¡œê·¸)."""  # ì„¤ëª…
    click.echo("ğŸ“š í•™ìŠµìš© ì •ìƒ ë¡œê·¸ ë°ì´í„° ìƒì„± ì¤‘...")
    p = generate_training_data(str(out_path), num_lines=num_lines, host=host)  # ìƒì„± í˜¸ì¶œ
    click.echo(f"âœ… Generated training data: {p}")
    click.echo(f"   ğŸ“Š Lines: {num_lines} (ëª¨ë‘ ì •ìƒ)")
    click.echo(f"   ğŸ“‹ Labels: {p}.labels.parquet")
    click.echo(f"\nğŸ’¡ Tip: ì´ ë°ì´í„°ë¡œ ëª¨ë¸ì„ í•™ìŠµí•˜ì„¸ìš”:")
    click.echo(f"   alog-detect parse --input {p} --out-dir data/processed/train")
    click.echo(f"   alog-detect build-deeplog --parsed data/processed/train/parsed.parquet --out-dir data/processed/train")
    click.echo(f"   alog-detect deeplog-train --seq data/processed/train/sequences.parquet --vocab data/processed/train/vocab.json --out models/deeplog.pth")


@main.command("gen-inference-normal")  # ì¶”ë¡ ìš© ì •ìƒ ë°ì´í„° ìƒì„±
@click.option("--out", "out_path", type=click.Path(dir_okay=False, path_type=Path), required=True)  # ì¶œë ¥ ê²½ë¡œ
@click.option("--lines", "num_lines", type=int, default=1000, help="ìƒì„±í•  ë¡œê·¸ ë¼ì¸ ìˆ˜")  # ë¼ì¸ ìˆ˜
@click.option("--host", default="test-host", help="í˜¸ìŠ¤íŠ¸ëª…")  # í˜¸ìŠ¤íŠ¸ëª…
def gen_inference_normal_cmd(out_path: Path, num_lines: int, host: str) -> None:  # ì¶”ë¡ ìš© ì •ìƒ ë°ì´í„° ìƒì„±
    """ì¶”ë¡ ìš© ì •ìƒ ë¡œê·¸ ë°ì´í„° ìƒì„± (False Positive í…ŒìŠ¤íŠ¸ìš©, 100% ì •ìƒ)."""  # ì„¤ëª…
    click.echo("âœ… ì¶”ë¡ ìš© ì •ìƒ ë¡œê·¸ ë°ì´í„° ìƒì„± ì¤‘...")
    p = generate_inference_normal(str(out_path), num_lines=num_lines, host=host)  # ìƒì„± í˜¸ì¶œ
    click.echo(f"âœ… Generated inference normal data: {p}")
    click.echo(f"   ğŸ“Š Lines: {num_lines} (ëª¨ë‘ ì •ìƒ)")
    click.echo(f"   ğŸ“‹ Labels: {p}.labels.parquet")
    click.echo(f"\nğŸ’¡ Tip: ëª¨ë¸ì´ ì´ ë°ì´í„°ë¥¼ ì •ìƒìœ¼ë¡œ ì¸ì‹í•´ì•¼ í•©ë‹ˆë‹¤ (False Positive í…ŒìŠ¤íŠ¸):")
    click.echo(f"   alog-detect parse --input {p} --out-dir data/processed/test_normal")
    click.echo(f"   alog-detect deeplog-infer --seq data/processed/test_normal/sequences.parquet --model models/deeplog.pth --k 3")


@main.command("gen-inference-anomaly")  # ì¶”ë¡ ìš© ë¹„ì •ìƒ ë°ì´í„° ìƒì„±
@click.option("--out", "out_path", type=click.Path(dir_okay=False, path_type=Path), required=True)  # ì¶œë ¥ ê²½ë¡œ
@click.option("--lines", "num_lines", type=int, default=1000, help="ìƒì„±í•  ë¡œê·¸ ë¼ì¸ ìˆ˜")  # ë¼ì¸ ìˆ˜
@click.option("--anomaly-rate", type=float, default=0.15, help="ì´ìƒ ë¡œê·¸ ë¹„ìœ¨ (ê¸°ë³¸: 15%)")  # ì´ìƒ ë¹„ìœ¨
@click.option("--anomaly-types", multiple=True, type=click.Choice(["unseen", "error", "attack", "crash", "burst"]),
              help="í¬í•¨í•  ì´ìƒ íƒ€ì… (ì—¬ëŸ¬ ê°œ ì„ íƒ ê°€ëŠ¥, ê¸°ë³¸: ëª¨ë‘)")  # ì´ìƒ íƒ€ì…
@click.option("--host", default="test-host", help="í˜¸ìŠ¤íŠ¸ëª…")  # í˜¸ìŠ¤íŠ¸ëª…
def gen_inference_anomaly_cmd(out_path: Path, num_lines: int, anomaly_rate: float,
                             anomaly_types: tuple[str, ...], host: str) -> None:  # ì¶”ë¡ ìš© ë¹„ì •ìƒ ë°ì´í„° ìƒì„±
    """ì¶”ë¡ ìš© ë¹„ì •ìƒ ë¡œê·¸ ë°ì´í„° ìƒì„± (True Positive í…ŒìŠ¤íŠ¸ìš©).

    ì´ìƒ íƒ€ì…:
    - unseen: í•™ìŠµ ì‹œ ë³´ì§€ ëª»í•œ ìƒˆë¡œìš´ í…œí”Œë¦¿
    - error: ì—ëŸ¬ ë©”ì‹œì§€ (ERROR, CRITICAL, FATAL)
    - attack: ë³´ì•ˆ ê³µê²© ì‹œë®¬ë ˆì´ì…˜ (SSH brute force, SYN flood)
    - crash: ì‹œìŠ¤í…œ í¬ë˜ì‹œ (ì„œë¹„ìŠ¤ ì‹¤íŒ¨, kernel panic)
    - burst: íŠ¹ì • í…œí”Œë¦¿ ê¸‰ì¦ (10-30ê°œ ì—°ì†)
    """  # ì„¤ëª…
    click.echo("ğŸš¨ ì¶”ë¡ ìš© ë¹„ì •ìƒ ë¡œê·¸ ë°ì´í„° ìƒì„± ì¤‘...")

    # anomaly_typesê°€ ë¹„ì–´ìˆìœ¼ë©´ None (ëª¨ë‘ í¬í•¨)
    types_list = list(anomaly_types) if anomaly_types else None

    p = generate_inference_anomaly(
        str(out_path),
        num_lines=num_lines,
        anomaly_rate=anomaly_rate,
        anomaly_types=types_list,
        host=host
    )  # ìƒì„± í˜¸ì¶œ

    click.echo(f"âœ… Generated inference anomaly data: {p}")
    click.echo(f"   ğŸ“Š Lines: {num_lines}")
    click.echo(f"   ğŸš¨ Target anomaly rate: {anomaly_rate:.1%}")
    click.echo(f"   ğŸ“‹ Labels: {p}.labels.parquet")
    click.echo(f"   ğŸ“ˆ Metadata: {p}.meta.json")

    # ë©”íƒ€ë°ì´í„° ì½ì–´ì„œ ì‹¤ì œ í†µê³„ í‘œì‹œ
    import json
    meta_path = Path(str(p) + ".meta.json")
    if meta_path.exists():
        with open(meta_path, 'r') as f:
            meta = json.load(f)
        click.echo(f"\nğŸ“Š ìƒì„± í†µê³„:")
        click.echo(f"   ì‹¤ì œ ì´ìƒë¥ : {meta['anomaly_rate_actual']:.1%} ({meta['anomaly_count']}/{meta['total_lines']}ê°œ)")
        if meta.get('anomaly_type_distribution'):
            click.echo(f"   ì´ìƒ íƒ€ì…ë³„ ë¶„í¬:")
            for anom_type, count in meta['anomaly_type_distribution'].items():
                click.echo(f"      - {anom_type}: {count}ê°œ")

    click.echo(f"\nğŸ’¡ Tip: ëª¨ë¸ì´ ì´ ë°ì´í„°ì—ì„œ ì´ìƒì„ íƒì§€í•´ì•¼ í•©ë‹ˆë‹¤ (True Positive í…ŒìŠ¤íŠ¸):")
    click.echo(f"   alog-detect parse --input {p} --out-dir data/processed/test_anomaly")
    click.echo(f"   alog-detect deeplog-infer --seq data/processed/test_anomaly/sequences.parquet --model models/deeplog.pth --k 3")
    click.echo(f"   alog-detect eval --processed-dir data/processed/test_anomaly --labels {p}.labels.parquet")


@main.command("eval")  # í‰ê°€ ëª…ë ¹
@click.option("--processed-dir", type=click.Path(file_okay=False, path_type=Path), required=True)  # ì‚°ì¶œë¬¼ í´ë”
@click.option("--labels", "labels_path", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True)  # ë¼ë²¨ ê²½ë¡œ
@click.option("--window-size", type=int, default=50)  # ìœˆë„ìš° í¬ê¸°
@click.option("--seq-len", type=int, default=50)  # ì‹œí€€ìŠ¤ ê¸¸ì´
def eval_cmd(processed_dir: Path, labels_path: Path, window_size: int, seq_len: int) -> None:  # í‰ê°€ ì‹¤í–‰
    """ë² ì´ìŠ¤ë¼ì¸/DeepLog í‰ê°€(Precision/Recall/F1)."""  # ì„¤ëª…
    out_lines = []  # ì¶œë ¥ ë¼ì¸
    base = processed_dir / "baseline_scores.parquet"  # ë² ì´ìŠ¤ë¼ì¸ ê²½ë¡œ
    if base.exists():
        p, r, f1 = evaluate_baseline(str(base), str(labels_path), window_size)  # ë² ì´ìŠ¤ë¼ì¸ í‰ê°€
        out_lines.append(f"Baseline PRF1: P={p:.3f} R={r:.3f} F1={f1:.3f}")  # í¬ë§·íŒ…
    dlinf = processed_dir / "deeplog_infer.parquet"  # DeepLog ê²½ë¡œ
    if dlinf.exists():
        p, r, f1 = evaluate_deeplog(str(dlinf), str(labels_path), seq_len)  # DeepLog í‰ê°€
        out_lines.append(f"DeepLog PRF1: P={p:.3f} R={r:.3f} F1={f1:.3f}")  # í¬ë§·íŒ…
    if not out_lines:  # í‰ê°€ ëŒ€ìƒ ì—†ì„ ë•Œ
        out_lines = ["No artifacts to evaluate."]  # ë©”ì‹œì§€
    (processed_dir / "eval.txt").write_text("\n".join(out_lines))  # íŒŒì¼ ì €ì¥
    click.echo("\n".join(out_lines))  # ì½˜ì†” ì¶œë ¥


@main.command("analyze-samples")  # ì´ìƒ ìƒ˜í”Œ ë¶„ì„
@click.option("--processed-dir", type=click.Path(file_okay=False, path_type=Path), required=True)  # ì‚°ì¶œë¬¼ í´ë”
@click.option("--output-dir", type=click.Path(file_okay=False, path_type=Path), default=None)  # ì¶œë ¥ í´ë”
@click.option("--max-samples", type=int, default=5, help="íƒ€ì…ë³„ ìµœëŒ€ ìƒ˜í”Œ ìˆ˜")  # ìµœëŒ€ ìƒ˜í”Œ ìˆ˜
@click.option("--context-lines", type=int, default=3, help="ì „í›„ ë§¥ë½ ë¼ì¸ ìˆ˜")  # ë¬¸ë§¥ ë¼ì¸ ìˆ˜
def analyze_samples_cmd(processed_dir: Path, output_dir: Path, max_samples: int, context_lines: int) -> None:  # ë¶„ì„ ì‹¤í–‰
    """ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ ì¶”ì¶œ ë° ë¶„ì„."""  # ì„¤ëª…
    from .analyzers.log_samples import main as log_samples_main  # ë¶„ì„ ì—”íŠ¸ë¦¬
    import sys  # argv ì¡°ì‘ìš©
    
    if output_dir is None:
        output_dir = processed_dir / "log_samples_analysis"  # ê¸°ë³¸ ì¶œë ¥ í´ë”
    
    click.echo("ğŸ” ì´ìƒ ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ì‹œì‘...")  # ì‹œì‘ ë¡œê·¸
    
    # Save current sys.argv  # ì™¸ë¶€ ë©”ì¸ í˜¸í™˜
    old_argv = sys.argv
    sys.argv = [
        "analyze-samples",
        str(processed_dir),
        "--output-dir", str(output_dir),
        "--max-samples", str(max_samples),
        "--context-lines", str(context_lines)
    ]
    
    try:
        result_code = log_samples_main()  # ì‹¤í–‰
    finally:
        sys.argv = old_argv  # ë³µì›
    
    if result_code is None:
        result_code = 0  # ê¸°ë³¸ ì„±ê³µ ì½”ë“œ
    result = type('obj', (object,), {'returncode': result_code, 'stdout': '', 'stderr': ''})  # ê°„ë‹¨ ë˜í•‘
    
    if result.returncode == 0:
        click.echo("âœ… ë¡œê·¸ ìƒ˜í”Œ ë¶„ì„ ì™„ë£Œ!")
        click.echo(f"ğŸ“„ ì‚¬ëŒì´ ì½ê¸° ì‰¬ìš´ ë¦¬í¬íŠ¸: {output_dir / 'anomaly_analysis_report.md'}")
        click.echo(f"ğŸ“Š ìƒì„¸ ë¶„ì„ ë°ì´í„°: {output_dir / 'anomaly_samples.json'}")
    else:
        click.echo(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {result.stderr}")
        return
    
    # ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥  # ë°©ë²•ë³„ ì´ìƒ ê°œìˆ˜ ì¶œë ¥
    sample_data_file = output_dir / "anomaly_samples.json"
    if sample_data_file.exists():
        import json
        try:
            with open(sample_data_file, 'r') as f:
                data = json.load(f)
            
            total_anomalies = 0
            for method, results in data.items():
                anomaly_count = results.get('anomaly_count', 0)
                total_anomalies += anomaly_count
                click.echo(f"  ğŸ“Š {method}: {anomaly_count}ê°œ ì´ìƒ ë°œê²¬")
            
            click.echo(f"ğŸš¨ ì´ ì´ìƒ ê°œìˆ˜: {total_anomalies}ê°œ")
        except Exception as e:
            click.echo(f"âš ï¸ ìš”ì•½ ìƒì„± ì‹¤íŒ¨: {e}")


@main.command("convert-onnx")  # ONNX ë³€í™˜
@click.option("--deeplog-model", type=click.Path(exists=True, dir_okay=False, path_type=Path), help="DeepLog ëª¨ë¸ ê²½ë¡œ")  # DeepLog ëª¨ë¸
@click.option("--mscred-model", type=click.Path(exists=True, dir_okay=False, path_type=Path), help="MS-CRED ëª¨ë¸ ê²½ë¡œ")  # MS-CRED ëª¨ë¸
@click.option("--vocab", type=click.Path(exists=True, dir_okay=False, path_type=Path), help="ì–´íœ˜ ì‚¬ì „ ê²½ë¡œ")  # vocab.json
@click.option("--output-dir", type=click.Path(file_okay=False, path_type=Path), default="models/onnx", help="ONNX ì¶œë ¥ ë””ë ‰í† ë¦¬")  # ì¶œë ¥ í´ë”
@click.option("--validate", is_flag=True, default=False, help="ë³€í™˜ í›„ ê²€ì¦ ì‹¤í–‰")  # ê²€ì¦ ì‹¤í–‰
@click.option("--seq-len", type=int, default=None, help="DeepLog ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸: ëª¨ë¸ì— ì €ì¥ëœ ê°’ ì‚¬ìš©, ONNXëŠ” dynamic_axesë¡œ ë‹¤ì–‘í•œ ê¸¸ì´ ì§€ì›)")  # ì‹œí€€ìŠ¤ ê¸¸ì´
@click.option("--feature-dim", type=int, default=None, help="MS-CRED í”¼ì²˜ ì°¨ì› (í…œí”Œë¦¿ ê°œìˆ˜, ê¸°ë³¸: ìë™ ê°ì§€)")  # í”¼ì²˜ ì°¨ì›
@click.option("--portable", is_flag=True, default=False, help="ë²”ìš© ìµœì í™” ëª¨ë“œ (ëª¨ë“  í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥, í•˜ë“œì›¨ì–´ íŠ¹í™” ìµœì í™” ì œì™¸)")  # ë²”ìš© ìµœì í™”
def convert_onnx_cmd(deeplog_model: Path, mscred_model: Path, vocab: Path, output_dir: Path, validate: bool, seq_len: Optional[int], feature_dim: Optional[int], portable: bool) -> None:  # ë³€í™˜ ì‹¤í–‰
    """PyTorch ëª¨ë¸ì„ ONNX í¬ë§·ìœ¼ë¡œ ë³€í™˜."""  # ì„¤ëª…
    try:
        # í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ëª¨ë“ˆ ë™ì  ì„í¬íŠ¸  # ë³€í™˜ ìœ í‹¸ ì‚¬ìš©
        import sys
        sys.path.append(str(Path(__file__).parent.parent))
        from hybrid_system.training.model_converter import convert_all_models
        
        if not deeplog_model and not mscred_model:
            click.echo("âŒ ìµœì†Œ í•˜ë‚˜ì˜ ëª¨ë¸ ê²½ë¡œë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
            return
        
        click.echo("ğŸ”„ ONNX ë³€í™˜ ì‹œì‘...")
        
        if portable:
            click.echo("ğŸŒ ë²”ìš© ìµœì í™” ëª¨ë“œ: ëª¨ë“  í™˜ê²½ì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ìƒì„±")
        else:
            click.echo("âš¡ ìµœëŒ€ ìµœì í™” ëª¨ë“œ: í˜„ì¬ í•˜ë“œì›¨ì–´ì— íŠ¹í™”ëœ ëª¨ë¸ ìƒì„±")
        
        # vocabì—ì„œ í…œí”Œë¦¿ ê°œìˆ˜ ì¶”ì¶œ (MS-CREDìš©)
        if mscred_model and feature_dim is None and vocab:
            try:
                with open(vocab, 'r') as f:
                    import json
                    vocab_dict = json.load(f)
                    feature_dim = len(vocab_dict)
                    click.echo(f"ğŸ“Š vocabì—ì„œ í…œí”Œë¦¿ ê°œìˆ˜ ê°ì§€: {feature_dim}")
            except Exception as e:
                click.echo(f"âš ï¸ vocabì—ì„œ í…œí”Œë¦¿ ê°œìˆ˜ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                feature_dim = 100
        
        results = convert_all_models(
            str(deeplog_model) if deeplog_model else "",
            str(mscred_model) if mscred_model else "",
            str(vocab) if vocab else "",
            str(output_dir),
            seq_len=seq_len,
            feature_dim=feature_dim,
            portable=portable
        )
        
        click.echo("\nğŸ‰ ONNX ë³€í™˜ ì™„ë£Œ!")
        for model_name, result in results.items():
            if 'error' in result:
                click.echo(f"âŒ {model_name}: {result['error']}")
            else:
                click.echo(f"âœ… {model_name}: {result['onnx_path']}")
                if 'optimized_path' in result:
                    click.echo(f"âš¡ ìµœì í™”ë¨: {result['optimized_path']}")
        
        click.echo(f"ğŸ“ ë³€í™˜ ê²°ê³¼: {output_dir}")
        
    except ImportError as e:
        click.echo(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        click.echo("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•˜ì„¸ìš”:")
        click.echo("   pip install -r requirements_hybrid.txt")
    except Exception as e:
        click.echo(f"âŒ ONNX ë³€í™˜ ì‹¤íŒ¨: {e}")


@main.command("hybrid-pipeline")  # í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸
@click.option("--log-file", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True, help="ì…ë ¥ ë¡œê·¸ íŒŒì¼")  # ì…ë ¥ ë¡œê·¸
@click.option("--output-dir", type=click.Path(file_okay=False, path_type=Path), help="ì¶œë ¥ ë””ë ‰í† ë¦¬")  # ì¶œë ¥ í´ë”
@click.option("--auto-deploy", is_flag=True, default=True, help="ìë™ ë°°í¬ ì¤€ë¹„")  # ìë™ ë°°í¬
@click.option("--models-dir", type=click.Path(file_okay=False, path_type=Path), default="models", help="ëª¨ë¸ ì €ì¥ ë””ë ‰í† ë¦¬")  # ëª¨ë¸ í´ë”
def hybrid_pipeline_cmd(log_file: Path, output_dir: Path, auto_deploy: bool, models_dir: Path) -> None:  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    """í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ (í•™ìŠµ â†’ ONNX ë³€í™˜ â†’ ë°°í¬ ì¤€ë¹„)."""  # ì„¤ëª…
    try:
        # í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ëª¨ë“ˆ ë™ì  ì„í¬íŠ¸  # íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤ ë¡œë“œ
        import sys
        sys.path.append(str(Path(__file__).parent.parent))
        from hybrid_system.training.auto_converter import AutoConverter
        
        click.echo("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ ì‹œì‘...")
        
        converter = AutoConverter(
            models_dir=str(models_dir),
            onnx_dir=str(models_dir / "onnx"),
            deployment_dir=str(models_dir / "deployment")
        )
        
        results = converter.run_full_pipeline(str(log_file), auto_deploy)  # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        click.echo(f"\nğŸ‰ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ: {results['status']}")
        
        if 'training' in results['stages']:
            training = results['stages']['training']
            click.echo(f"ğŸ“Š í•™ìŠµëœ ëª¨ë¸:")
            for model_name, model_info in training.get('models', {}).items():
                click.echo(f"  - {model_name}: {model_info['path']}")
        
        if 'conversion' in results['stages']:
            conversion = results['stages']['conversion']
            click.echo(f"ğŸ”„ ë³€í™˜ëœ ëª¨ë¸:")
            for model_name, result in conversion.items():
                if 'error' not in result:
                    click.echo(f"  - {model_name}: {result['onnx_path']}")
        
        if 'deployment' in results['stages']:
            deployment = results['stages']['deployment']
            click.echo(f"ğŸ“¦ ë°°í¬ ì¤€ë¹„ ì™„ë£Œ:")
            click.echo(f"  - ëª¨ë¸ ê°œìˆ˜: {len(deployment['models'])}ê°œ")
            click.echo(f"  - íŒŒì¼ ê°œìˆ˜: {len(deployment['files'])}ê°œ")
        
        click.echo(f"ğŸ“ ê²°ê³¼ ìœ„ì¹˜: {models_dir / 'deployment'}")
        
    except ImportError as e:
        click.echo(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ëª¨ë“ˆì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
        click.echo("ğŸ’¡ ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ì˜ì¡´ì„±ì„ ì„¤ì¹˜í•˜ì„¸ìš”:")
        click.echo("   pip install -r requirements_hybrid.txt")
    except Exception as e:
        click.echo(f"âŒ í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")


@main.command("analyze-temporal")  # ì‹œê°„ ê¸°ë°˜ ë¶„ì„
@click.option("--data-dir", type=click.Path(exists=True, file_okay=False, path_type=Path), required=True, help="ë¶„ì„í•  ë°ì´í„° ë””ë ‰í† ë¦¬")  # ë°ì´í„° í´ë”
@click.option("--output-dir", type=click.Path(file_okay=False, path_type=Path), default=None, help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")  # ì¶œë ¥ í´ë”
def analyze_temporal_cmd(data_dir: Path, output_dir: Path) -> None:  # ì‹¤í–‰
    """ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ë¶„ì„."""  # ì„¤ëª…
    from .analyzers.temporal import main as temporal_main  # ëª¨ë“ˆ ë©”ì¸
    import sys  # argv ì¡°ì‘
    
    # ì„ì‹œë¡œ sys.argv ì¡°ì‘  # í•˜ìœ„ CLI í˜¸í™˜
    old_argv = sys.argv
    sys.argv = ['temporal', '--data-dir', str(data_dir)]
    if output_dir:
        sys.argv.extend(['--output-dir', str(output_dir)])
    
    try:
        temporal_main()  # ì‹¤í–‰
    finally:
        sys.argv = old_argv  # ë³µì›


@main.command("analyze-comparative")  # ë¹„êµ ë¶„ì„
@click.option("--target", type=click.Path(exists=True, dir_okay=False, path_type=Path), required=True, help="Target íŒŒì¼")  # íƒ€ê¹ƒ íŒŒì¼
@click.option("--baselines", multiple=True, required=True, help="Baseline íŒŒì¼ë“¤")  # ë² ì´ìŠ¤ë¼ì¸ë“¤
@click.option("--output-dir", type=click.Path(file_okay=False, path_type=Path), default=None, help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")  # ì¶œë ¥ í´ë”
def analyze_comparative_cmd(target: Path, baselines: tuple, output_dir: Path) -> None:  # ì‹¤í–‰
    """ë¹„êµ ê¸°ë°˜ ì´ìƒ íƒì§€ ë¶„ì„."""  # ì„¤ëª…
    from .analyzers.comparative import main as comparative_main  # ë©”ì¸
    import sys  # argv ì¡°ì‘
    
    # ì„ì‹œë¡œ sys.argv ì¡°ì‘  # í•˜ìœ„ CLI í˜¸í™˜
    old_argv = sys.argv
    sys.argv = ['comparative', '--target', str(target)]
    for baseline in baselines:
        sys.argv.extend(['--baselines', baseline])
    if output_dir:
        sys.argv.extend(['--output-dir', str(output_dir)])
    
    try:
        comparative_main()  # ì‹¤í–‰
    finally:
        sys.argv = old_argv  # ë³µì›


@main.command("analyze-mscred")  # MS-CRED ë¶„ì„
@click.option("--data-dir", type=click.Path(exists=True, file_okay=False, path_type=Path), required=True, help="MS-CRED ê²°ê³¼ ë””ë ‰í† ë¦¬")  # ë°ì´í„° í´ë”
@click.option("--output-dir", type=click.Path(file_okay=False, path_type=Path), default=None, help="ë¶„ì„ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")  # ì¶œë ¥ í´ë”
def analyze_mscred_cmd(data_dir: Path, output_dir: Path) -> None:  # ì‹¤í–‰
    """MS-CRED ì „ìš© ë¶„ì„."""  # ì„¤ëª…
    from .analyzers.mscred_analysis import main as mscred_main  # ë©”ì¸
    import sys  # argv ì¡°ì‘
    
    # ì„ì‹œë¡œ sys.argv ì¡°ì‘  # í•˜ìœ„ CLI í˜¸í™˜
    old_argv = sys.argv
    sys.argv = ['mscred', '--data-dir', str(data_dir)]
    if output_dir:
        sys.argv.extend(['--output-dir', str(output_dir)])
    
    try:
        mscred_main()  # ì‹¤í–‰
    finally:
        sys.argv = old_argv  # ë³µì›


@main.command("validate-baseline")
@click.argument("baseline_files", nargs=-1, required=True)
@click.option("--output-dir", type=click.Path(file_okay=False, path_type=Path), default=None, help="ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬")
def validate_baseline_cmd(baseline_files: tuple, output_dir: Path) -> None:
    """ë² ì´ìŠ¤ë¼ì¸ íŒŒì¼ í’ˆì§ˆ ê²€ì¦."""
    from .analyzers.baseline_validation import main as baseline_main
    import sys

    # ì„ì‹œë¡œ sys.argv ì¡°ì‘
    old_argv = sys.argv
    sys.argv = ['baseline'] + list(baseline_files)
    if output_dir:
        sys.argv.extend(['--output-dir', str(output_dir)])

    try:
        baseline_main()
    finally:
        sys.argv = old_argv


if __name__ == "__main__":
    main()


