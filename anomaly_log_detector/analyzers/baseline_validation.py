#!/usr/bin/env python3  # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì…”ë±…  
"""Baseline í’ˆì§ˆ ê²€ì¦ ë„êµ¬ ìš”ì•½

- ëª©ì : baseline í›„ë³´(ë³´í†µ parsed.parquet) íŒŒì¼ë“¤ì˜ í’ˆì§ˆì„ ì‚¬ì „ ì ê²€/í•„í„°ë§
- ë°©ë²•: í…œí”Œë¦¿ ë¶„í¬, ì—ëŸ¬/ê²½ê³  ë¹„ìœ¨, ì‹œê°„ ì¼ê´€ì„± ë“± ë‹¤ìˆ˜ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì ìˆ˜í™”
- ì¶œë ¥: ê° íŒŒì¼ì˜ ìœ íš¨ì„±/ì ìˆ˜/ë¬¸ì œ ëª©ë¡/í†µê³„ ë° ë¦¬í¬íŠ¸ ìƒì„±
"""  # ëª¨ë“ˆ ìš”ì•½ ì„¤ëª…
import pandas as pd  # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬
import numpy as np  # ìˆ˜ì¹˜ ì—°ì‚°
from pathlib import Path  # ê²½ë¡œ ì²˜ë¦¬
from typing import List, Dict, Tuple, Optional  # íƒ€ì… íŒíŠ¸
import argparse  # CLI ì¸ì íŒŒì„œ
import json  # JSON ì…ì¶œë ¥

class BaselineValidator:  # baseline í’ˆì§ˆ ê²€ì¦ê¸°
    def __init__(self):  # ì´ˆê¸°í™”
        # ì •ìƒ baselineì˜ ê¸°ì¤€ê°’ë“¤ (ê²½í—˜ì  ì„ê³„ê°’)  # íŠœë‹ ê°€ëŠ¥
        self.thresholds = {
            'max_error_rate': 0.02,      # 2% ì´í•˜ ì—ëŸ¬ìœ¨
            'max_warning_rate': 0.05,    # 5% ì´í•˜ ê²½ê³ ìœ¨  
            'min_template_count': 10,    # ìµœì†Œ 10ê°œ í…œí”Œë¦¿
            'max_rare_template_ratio': 0.3,  # í¬ê·€ í…œí”Œë¦¿ 30% ì´í•˜
            'min_log_count': 100,        # ìµœì†Œ 100ê°œ ë¡œê·¸
            'max_template_entropy': 10,  # í…œí”Œë¦¿ ì—”íŠ¸ë¡œí”¼ ìƒí•œ
            'min_template_entropy': 2,   # í…œí”Œë¦¿ ì—”íŠ¸ë¡œí”¼ í•˜í•œ
        }
    
    def validate_single_baseline(self, parsed_file: str) -> Dict:  # ë‹¨ì¼ íŒŒì¼ í’ˆì§ˆ ê²€ì¦
        """ë‹¨ì¼ baseline íŒŒì¼ì„ ê²€ì¦í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
        try:
            df = pd.read_parquet(parsed_file)  # Parquet ë¡œë“œ
            file_name = Path(parsed_file).name  # íŒŒì¼ëª…
            
            # ê¸°ë³¸ í†µê³„ ê³„ì‚°  # ë°ì´í„° ì¡´ì¬ì„± í™•ì¸
            total_logs = len(df)  # ì´ ë¡œê·¸ ìˆ˜
            if total_logs == 0:  # ë¹„ì–´ìˆìœ¼ë©´ ë°”ë¡œ ì‹¤íŒ¨
                return {
                    'file': file_name,
                    'valid': False,
                    'score': 0.0,
                    'issues': ['Empty dataset'],
                    'stats': {}
                }
            
            # í…œí”Œë¦¿ ë¶„ì„  # ë¶„í¬/í¬ê·€ë„ ì¸¡ì •
            template_counts = df['template_id'].value_counts()  # í…œí”Œë¦¿ë³„ ë¹ˆë„
            unique_templates = len(template_counts)  # ê³ ìœ  í…œí”Œë¦¿ ìˆ˜
            rare_templates = len([t for t, count in template_counts.items() if count == 1])  # 1íšŒ ë“±ì¥ í…œí”Œë¦¿ ìˆ˜
            rare_template_ratio = rare_templates / max(unique_templates, 1)  # í¬ê·€ í…œí”Œë¦¿ ë¹„ìœ¨
            
            # í…œí”Œë¦¿ ë¶„í¬ ì—”íŠ¸ë¡œí”¼  # ë‹¤ì–‘ì„± ì²™ë„
            template_probs = template_counts / total_logs  # í™•ë¥  ë¶„í¬
            template_entropy = -sum(p * np.log2(p) for p in template_probs if p > 0)  # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
            
            # ì—ëŸ¬/ê²½ê³  ë¡œê·¸ ë¶„ì„  # í‚¤ì›Œë“œ ê¸°ë°˜ ë¹„ìœ¨ ì¶”ì •
            error_keywords = ['error', 'ERROR', 'fail', 'FAIL', 'exception', 'Exception', 'panic', 'fatal', 'crash']  # ì—ëŸ¬ í‚¤ì›Œë“œ
            warning_keywords = ['warn', 'WARN', 'warning', 'WARNING', 'deprecated']  # ê²½ê³  í‚¤ì›Œë“œ
            
            error_logs = df[df['raw'].str.contains('|'.join(error_keywords), case=False, na=False)]  # ì—ëŸ¬ í¬í•¨ í–‰
            warning_logs = df[df['raw'].str.contains('|'.join(warning_keywords), case=False, na=False)]  # ê²½ê³  í¬í•¨ í–‰
            
            error_rate = len(error_logs) / total_logs  # ì—ëŸ¬ìœ¨
            warning_rate = len(warning_logs) / total_logs  # ê²½ê³ ìœ¨
            
            # ì‹œê°„ ì¼ê´€ì„± ê²€ì‚¬  # í° ì‹œê°„ ê°­ íƒì§€
            time_consistency = True  # ê¸°ë³¸ True
            time_gaps = []  # í° ê°­ ëª©ë¡
            if 'timestamp' in df.columns:  # íƒ€ì„ìŠ¤íƒ¬í”„ ì¡´ì¬ ì‹œ
                df_sorted = df.sort_values('timestamp').copy()  # ì‹œê°„ ì •ë ¬
                df_sorted['timestamp'] = pd.to_datetime(df_sorted['timestamp'], errors='coerce')  # datetime ë³€í™˜
                valid_timestamps = df_sorted.dropna(subset=['timestamp'])  # ìœ íš¨í•œ í–‰ë§Œ
                
                if len(valid_timestamps) > 1:  # 2ê°œ ì´ìƒì¼ ë•Œë§Œ
                    time_diffs = valid_timestamps['timestamp'].diff().dt.total_seconds()  # ì—°ì† ì°¨ì´(ì´ˆ)
                    # 1ì‹œê°„ ì´ìƒ ê°­ì´ ìˆìœ¼ë©´ ì˜ì‹¬  # ì„ê³„ 3600ì´ˆ
                    large_gaps = time_diffs[time_diffs > 3600]  # í° ê°­ë§Œ ì¶”ì¶œ
                    time_gaps = large_gaps.tolist()  # ë¦¬ìŠ¤íŠ¸í™”
                    time_consistency = len(large_gaps) < total_logs * 0.1  # 10% ì´í•˜ì´ë©´ ì¼ê´€ì„± ì–‘í˜¸
            
            # ë¡œê·¸ ë°€ë„ ë¶„ì„ (ì‹œê°„ë‹¹ ë¡œê·¸ ìˆ˜)  # ë°€ë„ ì§€í‘œ
            logs_per_hour = None  # ê¸°ë³¸ None
            if 'timestamp' in df.columns and len(df) > 1:  # ê³„ì‚° ê°€ëŠ¥ ì¡°ê±´
                try:
                    start_time = pd.to_datetime(df['timestamp'].min())  # ì‹œì‘ ì‹œê°„
                    end_time = pd.to_datetime(df['timestamp'].max())  # ì¢…ë£Œ ì‹œê°„
                    duration_hours = (end_time - start_time).total_seconds() / 3600  # ì‹œê°„ ì°¨
                    if duration_hours > 0:
                        logs_per_hour = total_logs / duration_hours  # ì‹œê°„ë‹¹ ë¡œê·¸ ìˆ˜
                except:
                    pass  # ë³€í™˜ ì‹¤íŒ¨ ì‹œ ë¬´ì‹œ
            
            # í†µê³„ ì •ë¦¬  # ë¦¬í¬íŠ¸ìš© í†µê³„ ë¬¶ìŒ
            stats = {
                'total_logs': total_logs,
                'unique_templates': unique_templates,
                'rare_templates': rare_templates,
                'rare_template_ratio': rare_template_ratio,
                'template_entropy': template_entropy,
                'error_rate': error_rate,
                'warning_rate': warning_rate,
                'time_consistency': time_consistency,
                'large_time_gaps': len(time_gaps),
                'logs_per_hour': logs_per_hour
            }
            
            # ê²€ì¦ ê²°ê³¼ ê³„ì‚°  # ê°ì  ë°©ì‹ìœ¼ë¡œ ì ìˆ˜ ì‚°ì¶œ
            issues = []  # ì´ìŠˆ ëª©ë¡
            score = 1.0  # ì™„ë²½í•œ ì ìˆ˜ì—ì„œ ì‹œì‘í•˜ì—¬ ë¬¸ì œë§ˆë‹¤ ê°ì 
            
            # ê° ê¸°ì¤€ë³„ ê²€ì‚¬  # ì„ê³„ ì´ˆê³¼/ë¯¸ë‹¬ ì‹œ ê°ì  ë° ì´ìŠˆ ê¸°ë¡
            if error_rate > self.thresholds['max_error_rate']:
                issues.append(f"ë†’ì€ ì—ëŸ¬ìœ¨: {error_rate:.2%} (ê¸°ì¤€: {self.thresholds['max_error_rate']:.2%})")
                score -= 0.3
            
            if warning_rate > self.thresholds['max_warning_rate']:
                issues.append(f"ë†’ì€ ê²½ê³ ìœ¨: {warning_rate:.2%} (ê¸°ì¤€: {self.thresholds['max_warning_rate']:.2%})")
                score -= 0.2
            
            if unique_templates < self.thresholds['min_template_count']:
                issues.append(f"í…œí”Œë¦¿ ë¶€ì¡±: {unique_templates}ê°œ (ê¸°ì¤€: {self.thresholds['min_template_count']}ê°œ)")
                score -= 0.2
            
            if rare_template_ratio > self.thresholds['max_rare_template_ratio']:
                issues.append(f"í¬ê·€ í…œí”Œë¦¿ ê³¼ë‹¤: {rare_template_ratio:.2%} (ê¸°ì¤€: {self.thresholds['max_rare_template_ratio']:.2%})")
                score -= 0.1
            
            if total_logs < self.thresholds['min_log_count']:
                issues.append(f"ë¡œê·¸ ìˆ˜ ë¶€ì¡±: {total_logs}ê°œ (ê¸°ì¤€: {self.thresholds['min_log_count']}ê°œ)")
                score -= 0.3
            
            if template_entropy > self.thresholds['max_template_entropy']:
                issues.append(f"í…œí”Œë¦¿ ì—”íŠ¸ë¡œí”¼ ê³¼ë‹¤: {template_entropy:.2f} (ê¸°ì¤€: {self.thresholds['max_template_entropy']})")
                score -= 0.2
            
            if template_entropy < self.thresholds['min_template_entropy']:
                issues.append(f"í…œí”Œë¦¿ ë‹¤ì–‘ì„± ë¶€ì¡±: {template_entropy:.2f} (ê¸°ì¤€: {self.thresholds['min_template_entropy']})")
                score -= 0.1
            
            if not time_consistency:
                issues.append("ì‹œê°„ ì¼ê´€ì„± ë¬¸ì œ: í° ì‹œê°„ ê°­ì´ ë‹¤ìˆ˜ ì¡´ì¬")
                score -= 0.1
            
            # ìµœì¢… ì ìˆ˜ ì¡°ì •  # í•˜í•œ 0.0 ì ìš©
            score = max(0.0, score)
            is_valid = score >= 0.7 and len(issues) <= 2  # 70% ì´ìƒ ì ìˆ˜, ë¬¸ì œ 2ê°œ ì´í•˜
            
            return {
                'file': file_name,
                'valid': is_valid,
                'score': score,
                'issues': issues,
                'stats': stats
            }
            
        except Exception as e:  # ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ê²°ê³¼
            return {
                'file': Path(parsed_file).name,
                'valid': False,
                'score': 0.0,
                'issues': [f'ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}'],
                'stats': {}
            }
    
    def validate_multiple_baselines(self, baseline_files: List[str]) -> Dict:  # ë‹¤ì¤‘ íŒŒì¼ ê²€ì¦
        """ì—¬ëŸ¬ baseline íŒŒì¼ë“¤ì„ ê²€ì¦í•˜ê³  í•„í„°ë§í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
        
        print(f"ğŸ” {len(baseline_files)}ê°œ baseline íŒŒì¼ ê²€ì¦ ì¤‘...")  # ì§„í–‰ ë¡œê·¸
        
        results = []  # ê°œë³„ ê²°ê³¼ ì €ì¥ì†Œ
        for baseline_file in baseline_files:  # íŒŒì¼ë³„ ë°˜ë³µ
            print(f"   ê²€ì¦: {Path(baseline_file).name}")  # íŒŒì¼ í‘œì‹œ
            result = self.validate_single_baseline(baseline_file)  # ë‹¨ì¼ ê²€ì¦
            results.append(result)  # ëˆ„ì 
        
        # ê²°ê³¼ ë¶„ì„  # ìœ íš¨/ë¬´íš¨ ë¶„ë¦¬
        valid_baselines = [r for r in results if r['valid']]
        invalid_baselines = [r for r in results if not r['valid']]
        
        # ìƒí˜¸ ì¼ê´€ì„± ê²€ì‚¬ (validí•œ ê²ƒë“¤ë¼ë¦¬)  # ì •ìƒ í›„ë³´ ê°„ ë¶„ì‚° ì²´í¬
        if len(valid_baselines) > 1:
            consistency_issues = self._check_mutual_consistency(valid_baselines, baseline_files)
        else:
            consistency_issues = []  # ë¹„êµ ë¶ˆê°€ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸
        
        print(f"\nğŸ“Š ê²€ì¦ ê²°ê³¼:")  # ìš”ì•½ ë¡œê·¸
        print(f"   âœ… ìœ íš¨: {len(valid_baselines)}ê°œ")
        print(f"   âŒ ë¬´íš¨: {len(invalid_baselines)}ê°œ")
        print(f"   âš ï¸  ì¼ê´€ì„± ë¬¸ì œ: {len(consistency_issues)}ê°œ")
        
        return {
            'total_files': len(baseline_files),  # ì´ íŒŒì¼ ìˆ˜
            'valid_count': len(valid_baselines),  # ìœ íš¨ ê°œìˆ˜
            'invalid_count': len(invalid_baselines),  # ë¬´íš¨ ê°œìˆ˜
            'valid_baselines': valid_baselines,  # ìœ íš¨ ë¦¬ìŠ¤íŠ¸
            'invalid_baselines': invalid_baselines,  # ë¬´íš¨ ë¦¬ìŠ¤íŠ¸
            'consistency_issues': consistency_issues,  # ì¼ê´€ì„± ì´ìŠˆ
            'recommended_baselines': [r['file'] for r in valid_baselines if r['score'] >= 0.8]  # ì¶”ì²œ íŒŒì¼
        }
    
    def _check_mutual_consistency(self, valid_results: List[Dict], baseline_files: List[str]) -> List[Dict]:  # ìƒí˜¸ ì¼ê´€ì„± ê²€ì‚¬
        """ìœ íš¨í•œ baselineë“¤ ê°„ì˜ ìƒí˜¸ ì¼ê´€ì„±ì„ ê²€ì‚¬í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
        
        consistency_issues = []  # ì´ìŠˆ ëˆ„ì 
        
        # ì£¼ìš” ì§€í‘œë“¤ì˜ ë¶„í¬ ë¶„ì„  # ë¹„êµ ëŒ€ìƒ ë©”íŠ¸ë¦­
        metrics = ['error_rate', 'warning_rate', 'template_entropy', 'logs_per_hour']
        
        for metric in metrics:  # ë©”íŠ¸ë¦­ë³„ ë°˜ë³µ
            values = []  # ê°’ ìˆ˜ì§‘
            for result in valid_results:  # ê° ê²°ê³¼ì—ì„œ
                val = result['stats'].get(metric)  # ê°’ ì¶”ì¶œ
                if val is not None:
                    values.append(val)  # ìˆ˜ì§‘
            
            if len(values) < 2:  # ë¹„êµ ë¶ˆê°€ ì‹œ ìŠ¤í‚µ
                continue
                
            mean_val = np.mean(values)  # í‰ê· 
            std_val = np.std(values)  # í‘œì¤€í¸ì°¨
            
            # Outlier íƒì§€ (2Ïƒ ë°–ì˜ ê°’ë“¤)  # ê°„ë‹¨í•œ ì´ìƒì¹˜ ê·œì¹™
            for i, (result, val) in enumerate(zip(valid_results, values)):
                if abs(val - mean_val) > 2 * std_val:
                    consistency_issues.append({
                        'file': result['file'],  # íŒŒì¼ëª…
                        'metric': metric,  # ë©”íŠ¸ë¦­ëª…
                        'value': val,  # ì‹¤ì œ ê°’
                        'mean': mean_val,  # í‰ê· 
                        'std': std_val,  # í‘œì¤€í¸ì°¨
                        'description': f'{metric}ì´ ë‹¤ë¥¸ baselineë“¤ê³¼ {abs(val - mean_val)/max(mean_val, 1e-6)*100:.1f}% ì°¨ì´'  # ì„¤ëª…
                    })
        
        return consistency_issues  # ì´ìŠˆ ëª©ë¡ ë°˜í™˜
    
    def generate_validation_report(self, validation_result: Dict, output_file: str = None) -> str:  # ë¦¬í¬íŠ¸ ìƒì„±
        """ê²€ì¦ ê²°ê³¼ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
        
        report = f"""# Baseline íŒŒì¼ ê²€ì¦ ë¦¬í¬íŠ¸

## ğŸ“Š ì „ì²´ ìš”ì•½
- **ì´ íŒŒì¼ ìˆ˜**: {validation_result['total_files']}ê°œ
- **ìœ íš¨í•œ íŒŒì¼**: {validation_result['valid_count']}ê°œ
- **ë¬´íš¨í•œ íŒŒì¼**: {validation_result['invalid_count']}ê°œ
- **ì¶”ì²œ íŒŒì¼**: {len(validation_result['recommended_baselines'])}ê°œ

## âœ… ìœ íš¨í•œ Baseline íŒŒì¼ë“¤

"""
        
        for result in validation_result['valid_baselines']:  # ìœ íš¨ íŒŒì¼ ì„¹ì…˜
            stats = result['stats']  # í†µê³„
            report += f"""### {result['file']}
- **í’ˆì§ˆ ì ìˆ˜**: {result['score']:.2f}/1.0
- **ë¡œê·¸ ìˆ˜**: {stats.get('total_logs', 0):,}ê°œ
- **í…œí”Œë¦¿ ìˆ˜**: {stats.get('unique_templates', 0)}ê°œ
- **ì—ëŸ¬ìœ¨**: {stats.get('error_rate', 0):.2%}
- **ê²½ê³ ìœ¨**: {stats.get('warning_rate', 0):.2%}
- **í…œí”Œë¦¿ ì—”íŠ¸ë¡œí”¼**: {stats.get('template_entropy', 0):.2f}
"""
            if result['issues']:  # ê²½ë¯¸ ì´ìŠˆ ë‚˜ì—´
                report += f"- **ê²½ë¯¸í•œ ì´ìŠˆ**: {', '.join(result['issues'])}\n"
            report += "\n"  # ê³µë°± ë¼ì¸
        
        if validation_result['invalid_baselines']:  # ë¬´íš¨ íŒŒì¼ ì„¹ì…˜
            report += "## âŒ ë¬´íš¨í•œ Baseline íŒŒì¼ë“¤\n\n"
            for result in validation_result['invalid_baselines']:
                report += f"""### {result['file']}
- **í’ˆì§ˆ ì ìˆ˜**: {result['score']:.2f}/1.0
- **ì£¼ìš” ë¬¸ì œ**: {', '.join(result['issues'])}

"""
        
        if validation_result['consistency_issues']:  # ì¼ê´€ì„± ì´ìŠˆ ì„¹ì…˜
            report += "## âš ï¸  ì¼ê´€ì„± ë¬¸ì œ\n\n"
            for issue in validation_result['consistency_issues']:
                report += f"- **{issue['file']}**: {issue['description']}\n"
            report += "\n"
        
        report += f"""## ğŸ’¡ ê¶Œì¥ì‚¬í•­

### ì‚¬ìš© ê¶Œì¥ íŒŒì¼ë“¤
{chr(10).join('- ' + f for f in validation_result['recommended_baselines'])}

### í’ˆì§ˆ ê°œì„  ë°©ë²•
1. **ì—ëŸ¬ìœ¨ì´ ë†’ì€ íŒŒì¼ë“¤**: í•´ë‹¹ ì‹œê¸°ì˜ ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
2. **ë¡œê·¸ ìˆ˜ê°€ ì ì€ íŒŒì¼ë“¤**: ë” ê¸´ ê¸°ê°„ì˜ ë¡œê·¸ ìˆ˜ì§‘
3. **í…œí”Œë¦¿ ë‹¤ì–‘ì„± ë¬¸ì œ**: ë‹¤ì–‘í•œ ì‹œìŠ¤í…œ í™œë™ì´ í¬í•¨ëœ ê¸°ê°„ ì„ íƒ
4. **ì‹œê°„ ì¼ê´€ì„± ë¬¸ì œ**: ë¡œê·¸ ìˆ˜ì§‘ ì¤‘ë‹¨ êµ¬ê°„ í™•ì¸

### ì´ìƒíƒì§€ ì •í™•ë„ í–¥ìƒì„ ìœ„í•œ ì¡°ì¹˜
- í’ˆì§ˆ ì ìˆ˜ 0.8 ì´ìƒ íŒŒì¼ë“¤ë§Œ baselineìœ¼ë¡œ ì‚¬ìš©
- ìµœì†Œ 3ê°œ ì´ìƒì˜ ìœ íš¨í•œ baseline í™•ë³´
- ì¼ê´€ì„± ë¬¸ì œê°€ ìˆëŠ” íŒŒì¼ë“¤ì€ ì œì™¸ ê³ ë ¤
"""
        
        if output_file:  # íŒŒì¼ë¡œ ì €ì¥
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(report)
            print(f"ğŸ“„ ê²€ì¦ ë¦¬í¬íŠ¸ ì €ì¥: {output_file}")
        
        return report  # ë¦¬í¬íŠ¸ ë¬¸ìì—´ ë°˜í™˜

def validate_baseline_files(baseline_files: List[str], output_dir: str = None):  # ìƒìœ„ í—¬í¼
    """Baseline íŒŒì¼ë“¤ì„ ê²€ì¦í•˜ê³  ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
    
    if output_dir:  # ì¶œë ¥ ë””ë ‰í† ë¦¬ ì¤€ë¹„
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
    
    validator = BaselineValidator()  # ê²€ì¦ê¸° ìƒì„±
    
    # ê²€ì¦ ì‹¤í–‰  # ë‹¤ì¤‘ íŒŒì¼ ì²˜ë¦¬
    result = validator.validate_multiple_baselines(baseline_files)
    
    # ê²°ê³¼ ì €ì¥  # ì˜µì…˜ì— ë”°ë¼ íŒŒì¼ ì €ì¥
    if output_dir:
        # JSON ê²°ê³¼ ì €ì¥
        with open(output_path / "validation_result.json", 'w', encoding='utf-8') as f:
            json.dump(result, f, indent=2, ensure_ascii=False, default=str)
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report = validator.generate_validation_report(result, output_path / "baseline_validation_report.md")
        
        print(f"ğŸ“‚ ê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_path}")
    else:
        validator.generate_validation_report(result)  # ì½˜ì†” ì¶œë ¥ìš©
    
    return result  # ê²°ê³¼ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜

def main():  # CLI ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
    parser = argparse.ArgumentParser(description="Baseline íŒŒì¼ í’ˆì§ˆ ê²€ì¦ ë„êµ¬")  # ì¸ì íŒŒì„œ
    parser.add_argument("baseline_files", nargs='+', help="ê²€ì¦í•  baseline íŒŒì¼ë“¤ (parsed.parquet)")  # ì…ë ¥ íŒŒì¼ë“¤
    parser.add_argument("--output-dir", help="ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬")  # ì¶œë ¥ í´ë”
    parser.add_argument("--score-threshold", type=float, default=0.7, help="ìœ íš¨ baseline ìµœì†Œ ì ìˆ˜ (ê¸°ë³¸: 0.7)")  # (ë¯¸ì‚¬ìš©) ì˜ˆë¹„ ì˜µì…˜
    
    args = parser.parse_args()  # ì¸ì íŒŒì‹±
    
    # ì„ê³„ê°’ ì—…ë°ì´íŠ¸  # í•„ìš” ì‹œ thresholdsë¥¼ ì¡°ì • ê°€ëŠ¥ (í˜„ì¬ëŠ” ê¸°ë³¸ê°’ ì‚¬ìš©)
    validator = BaselineValidator()  # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    
    result = validate_baseline_files(args.baseline_files, args.output_dir)  # ê²€ì¦ ì‹¤í–‰
    
    print(f"\nğŸ¯ ê¶Œì¥ baseline íŒŒì¼ë“¤:")  # ê¶Œì¥ ëª©ë¡ ì¶œë ¥
    for file in result['recommended_baselines']:
        print(f"   âœ… {file}")

if __name__ == "__main__":  # ì§ì ‘ ì‹¤í–‰ ì‹œ
    main()  # ë©”ì¸ í˜¸ì¶œ
