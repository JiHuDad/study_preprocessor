#!/usr/bin/env python3  # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì…”ë±…
"""ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ë„êµ¬ ìš”ì•½

- ëª©ì : ê³¼ê±° ì‹œê°„ëŒ€/ìš”ì¼ íŒ¨í„´ì„ í•™ìŠµí•˜ê³  í˜„ì¬ ë¡œê·¸ì™€ ë¹„êµí•˜ì—¬ ì´ìƒ íƒì§€
- ë°©ë²•: ì‹œê°„ëŒ€ë³„ ë³¼ë¥¨ í¸ì°¨, í…œí”Œë¦¿ ë¶„í¬ ë³€í™”(ì‹ ê·œ/ëˆ„ë½ í…œí”Œë¦¿) ê²€ì¶œ
- ì¶œë ¥: temporal_profiles.json, temporal_anomalies.json, temporal_report.md
"""  # ëª¨ë“ˆ ìš”ì•½ ì„¤ëª…
import pandas as pd  # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬
import numpy as np  # ìˆ˜ì¹˜ ê³„ì‚°
import json  # JSON ì…ì¶œë ¥
from pathlib import Path  # ê²½ë¡œ ì²˜ë¦¬
from datetime import datetime, timedelta  # ì‹œê°„ ì²˜ë¦¬
from typing import Dict, List, Optional  # íƒ€ì… íŒíŠ¸
import argparse  # CLI ì¸ì íŒŒì„œ
from collections import defaultdict  # (ë¯¸ì‚¬ìš©) ê¸°ë³¸ dict

class TemporalAnomalyDetector:  # ì‹œê°„ ê¸°ë°˜ ì´ìƒíƒì§€ê¸°
    def __init__(self):  # ì´ˆê¸°í™”
        self.hourly_profiles = {}  # ì‹œê°„ëŒ€ë³„ í”„ë¡œíŒŒì¼ ì €ì¥ì†Œ
        self.daily_profiles = {}  # ìš”ì¼ë³„ í”„ë¡œíŒŒì¼ ì €ì¥ì†Œ
        self.template_baseline = {}  # í…œí”Œë¦¿ ê¸°ì¤€ì¹˜ (ì˜ˆë¹„)
    
    def build_temporal_profiles(self, parsed_df: pd.DataFrame) -> Dict:  # ì‹œê°„/ìš”ì¼ í”„ë¡œíŒŒì¼ êµ¬ì¶•
        """ì‹œê°„ëŒ€ë³„/ìš”ì¼ë³„ ì •ìƒ í”„ë¡œíŒŒì¼ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
        
        # timestampë¥¼ datetimeìœ¼ë¡œ ë³€í™˜  # íŒŒìƒ ì»¬ëŸ¼ ìƒì„±
        parsed_df['datetime'] = pd.to_datetime(parsed_df['timestamp'])
        parsed_df['hour'] = parsed_df['datetime'].dt.hour
        parsed_df['weekday'] = parsed_df['datetime'].dt.weekday  # 0=Monday
        parsed_df['date'] = parsed_df['datetime'].dt.date
        
        profiles = {
            'hourly': {},  # ì‹œê°„ëŒ€ë³„ í”„ë¡œíŒŒì¼
            'daily': {},  # ìš”ì¼ë³„ í”„ë¡œíŒŒì¼
            'baseline_period': {
                'start': parsed_df['datetime'].min(),  # ê¸°ì¤€ ì‹œì‘
                'end': parsed_df['datetime'].max(),  # ê¸°ì¤€ ì¢…ë£Œ
                'total_days': (parsed_df['datetime'].max() - parsed_df['datetime'].min()).days + 1  # ì´ ì¼ìˆ˜
            }
        }
        
        # ì‹œê°„ëŒ€ë³„ í”„ë¡œíŒŒì¼ (0-23ì‹œ)
        for hour in range(24):  # ê° ì‹œê°„ëŒ€ ìˆœíšŒ
            hour_data = parsed_df[parsed_df['hour'] == hour]
            if len(hour_data) > 0:
                template_dist = hour_data['template_id'].value_counts(normalize=True).to_dict()  # í…œí”Œë¦¿ ë¶„í¬
                profiles['hourly'][hour] = {
                    'volume_mean': len(hour_data) / profiles['baseline_period']['total_days'],  # ì¼ í‰ê·  ë³¼ë¥¨
                    'volume_std': len(hour_data) * 0.1,  # ì„ì‹œ í‘œì¤€í¸ì°¨ ì¶”ì •ì¹˜
                    'template_distribution': template_dist,  # ë¶„í¬
                    'top_templates': list(hour_data['template_id'].value_counts().head(5).index),  # ìƒìœ„ í…œí”Œë¦¿
                    'unique_templates': len(hour_data['template_id'].unique())  # ê³ ìœ  í…œí”Œë¦¿ ìˆ˜
                }
        
        # ìš”ì¼ë³„ í”„ë¡œíŒŒì¼ (0=ì›”ìš”ì¼)
        for weekday in range(7):  # ì›”~ì¼ ìˆœíšŒ
            day_data = parsed_df[parsed_df['weekday'] == weekday]
            if len(day_data) > 0:
                template_dist = day_data['template_id'].value_counts(normalize=True).to_dict()  # í…œí”Œë¦¿ ë¶„í¬
                profiles['daily'][weekday] = {
                    'volume_mean': len(day_data) / (profiles['baseline_period']['total_days'] // 7 + 1),  # ìš”ì¼ í‰ê·  ë³¼ë¥¨
                    'template_distribution': template_dist,  # ë¶„í¬
                    'top_templates': list(day_data['template_id'].value_counts().head(10).index),  # ìƒìœ„ í…œí”Œë¦¿
                    'unique_templates': len(day_data['template_id'].unique())  # ê³ ìœ  í…œí”Œë¦¿ ìˆ˜
                }
        
        return profiles  # í”„ë¡œíŒŒì¼ ë°˜í™˜
    
    def detect_temporal_anomalies(self, parsed_df: pd.DataFrame, profiles: Dict, 
                                time_window_hours: int = 1) -> List[Dict]:  # ì‹œê°„ëŒ€ ë¹„êµ ì´ìƒ íƒì§€
        """ì‹œê°„ëŒ€ë³„ í”„ë¡œíŒŒì¼ê³¼ ë¹„êµí•˜ì—¬ ì´ìƒì„ íƒì§€í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
        
        parsed_df['datetime'] = pd.to_datetime(parsed_df['timestamp'])  # íŒŒìƒ ì»¬ëŸ¼ ì¬ìƒì„±
        parsed_df['hour'] = parsed_df['datetime'].dt.hour
        parsed_df['weekday'] = parsed_df['datetime'].dt.weekday
        
        anomalies = []  # ì´ìƒ ëª©ë¡
        
        # ì‹œê°„ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë¶„ì„
        for hour in sorted(parsed_df['hour'].unique()):  # ê´€ì¸¡ëœ ì‹œê°„ëŒ€ ìˆœíšŒ
            hour_data = parsed_df[parsed_df['hour'] == hour]
            
            if hour not in profiles['hourly']:
                # í”„ë¡œíŒŒì¼ì´ ì—†ëŠ” ì‹œê°„ëŒ€ëŠ” ëª¨ë‘ ì´ìƒìœ¼ë¡œ í‘œì‹œ
                anomalies.append({
                    'type': 'temporal_unseen_hour',  # ë¯¸í•™ìŠµ ì‹œê°„ëŒ€
                    'hour': hour,
                    'severity': 'high',
                    'description': f'{hour}ì‹œëŠ” ê³¼ê±° ë°ì´í„°ì— ì—†ëŠ” ì‹œê°„ëŒ€ì…ë‹ˆë‹¤',
                    'actual_volume': len(hour_data),
                    'expected_volume': 'unknown'
                })
                continue
            
            expected_profile = profiles['hourly'][hour]  # ê¸°ëŒ€ í”„ë¡œíŒŒì¼
            actual_volume = len(hour_data)  # ì‹¤ì œ ë³¼ë¥¨
            expected_volume = expected_profile['volume_mean']  # ê¸°ëŒ€ ë³¼ë¥¨
            
            # ë³¼ë¥¨ ì´ìƒ ê²€ì‚¬  # ìƒëŒ€ í¸ì°¨ ê³„ì‚°
            volume_deviation = abs(actual_volume - expected_volume) / max(expected_volume, 1)
            if volume_deviation > 0.5:  # 50% ì´ìƒ ì°¨ì´
                anomalies.append({
                    'type': 'temporal_volume_anomaly',  # ë³¼ë¥¨ ì´ìƒ
                    'hour': hour,
                    'severity': 'high' if volume_deviation > 1.0 else 'medium',
                    'description': f'{hour}ì‹œ ë¡œê·¸ ë³¼ë¥¨ì´ ì˜ˆìƒê³¼ {volume_deviation:.1%} ì°¨ì´',
                    'actual_volume': actual_volume,
                    'expected_volume': expected_volume,
                    'deviation': volume_deviation
                })
            
            # í…œí”Œë¦¿ ë¶„í¬ ì´ìƒ ê²€ì‚¬  # ì‹ ê·œ/ëˆ„ë½ í…œí”Œë¦¿ í™•ì¸
            if len(hour_data) > 0:
                actual_templates = set(hour_data['template_id'].unique())
                expected_templates = set(expected_profile['top_templates'])
                
                # ìƒˆë¡œìš´ í…œí”Œë¦¿ ë°œê²¬
                new_templates = actual_templates - expected_templates
                if new_templates and len(new_templates) > len(expected_templates) * 0.2:
                    anomalies.append({
                        'type': 'temporal_new_templates',  # ì‹ ê·œ í…œí”Œë¦¿ ì¦ê°€
                        'hour': hour,
                        'severity': 'medium',
                        'description': f'{hour}ì‹œì— {len(new_templates)}ê°œì˜ ìƒˆë¡œìš´ í…œí”Œë¦¿ ë°œê²¬',
                        'new_templates': list(new_templates)[:5],  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                        'expected_templates_count': len(expected_templates),
                        'new_templates_count': len(new_templates)
                    })
                
                # ê¸°ì¡´ ì£¼ìš” í…œí”Œë¦¿ ëˆ„ë½
                missing_templates = expected_templates - actual_templates
                if missing_templates:
                    anomalies.append({
                        'type': 'temporal_missing_templates',  # ì£¼ìš” í…œí”Œë¦¿ ëˆ„ë½
                        'hour': hour,
                        'severity': 'medium',
                        'description': f'{hour}ì‹œì— í‰ì†Œ ì£¼ìš” í…œí”Œë¦¿ {len(missing_templates)}ê°œ ëˆ„ë½',
                        'missing_templates': list(missing_templates),
                        'missing_count': len(missing_templates)
                    })
        
        return anomalies  # ì´ìƒ ëª©ë¡ ë°˜í™˜
    
    def generate_temporal_report(self, profiles: Dict, anomalies: List[Dict], 
                               output_path: str) -> str:  # ë¦¬í¬íŠ¸ ìƒì„±
        """ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
        
        report = f"""# ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ë¦¬í¬íŠ¸

## ğŸ“Š ê¸°ì¤€ í”„ë¡œíŒŒì¼ ì •ë³´
- **ë¶„ì„ ê¸°ê°„**: {profiles['baseline_period']['start']} ~ {profiles['baseline_period']['end']}
- **ì´ ë¶„ì„ ì¼ìˆ˜**: {profiles['baseline_period']['total_days']}ì¼
- **ì‹œê°„ëŒ€ë³„ í”„ë¡œíŒŒì¼**: {len(profiles['hourly'])}ê°œ
- **ìš”ì¼ë³„ í”„ë¡œíŒŒì¼**: {len(profiles['daily'])}ê°œ

## ğŸš¨ ë°œê²¬ëœ ì´ìƒ í˜„ìƒ

"""
        
        if not anomalies:  # ì´ìƒ ì—†ìŒ
            report += "âœ… ì‹œê°„ ê¸°ë°˜ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n"
        else:
            # ì‹¬ê°ë„ë³„ ë¶„ë¥˜  # high/medium ê·¸ë£¹
            high_severity = [a for a in anomalies if a['severity'] == 'high']
            medium_severity = [a for a in anomalies if a['severity'] == 'medium']
            
            if high_severity:
                report += f"### ğŸ”´ ì‹¬ê°í•œ ì´ìƒ ({len(high_severity)}ê°œ)\n\n"
                for anomaly in high_severity:
                    report += f"- **{anomaly['type']}** ({anomaly['hour']}ì‹œ): {anomaly['description']}\n"
                report += "\n"
            
            if medium_severity:
                report += f"### ğŸŸ¡ ì£¼ì˜ í•„ìš” ({len(medium_severity)}ê°œ)\n\n"
                for anomaly in medium_severity:
                    report += f"- **{anomaly['type']}** ({anomaly['hour']}ì‹œ): {anomaly['description']}\n"
                report += "\n"
        
        # ì‹œê°„ëŒ€ë³„ ìš”ì•½  # ê° ì‹œê°„ëŒ€ í•µì‹¬ ì •ë³´
        report += "## ğŸ“ˆ ì‹œê°„ëŒ€ë³„ í™œë™ íŒ¨í„´\n\n"
        if 'hourly' in profiles:
            for hour in sorted(profiles['hourly'].keys()):
                profile = profiles['hourly'][hour]
                report += f"- **{hour:02d}ì‹œ**: í‰ê·  {profile['volume_mean']:.1f}ê°œ ë¡œê·¸, ì£¼ìš” í…œí”Œë¦¿ {profile['unique_templates']}ê°œ\n"
        
        # íŒŒì¼ë¡œ ì €ì¥  # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        return report  # ë¦¬í¬íŠ¸ ë¬¸ìì—´ ë°˜í™˜

def analyze_temporal_patterns(data_dir: str, output_dir: str = None):  # ì‹œê°„ ê¸°ë°˜ íƒì§€ ì‹¤í–‰
    """ì‹œê°„ íŒ¨í„´ ê¸°ë°˜ ì´ìƒ íƒì§€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
    
    data_path = Path(data_dir)  # ë°ì´í„° í´ë”
    if output_dir is None:
        output_dir = data_path / "temporal_analysis"  # ê¸°ë³¸ ì¶œë ¥ í´ë”
    output_path = Path(output_dir)  # Path í™”
    output_path.mkdir(parents=True, exist_ok=True)  # í´ë” ìƒì„±
    
    print("ğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ì‹œì‘...")  # ì‹œì‘ ë¡œê·¸
    
    # ë°ì´í„° ë¡œë“œ
    parsed_df = pd.read_parquet(data_path / "parsed.parquet")  # íŒŒì‹± ë¡œê·¸ ë¡œë“œ
    print(f"ğŸ“Š ë¡œë“œëœ ë¡œê·¸: {len(parsed_df)}ê°œ")
    
    # íƒì§€ê¸° ì´ˆê¸°í™”
    detector = TemporalAnomalyDetector()  # ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    
    # í”„ë¡œíŒŒì¼ êµ¬ì¶•
    print("ğŸ“ˆ ì‹œê°„ëŒ€ë³„ í”„ë¡œíŒŒì¼ êµ¬ì¶• ì¤‘...")
    profiles = detector.build_temporal_profiles(parsed_df)  # í”„ë¡œíŒŒì¼ ìƒì„±
    
    # í”„ë¡œíŒŒì¼ ì €ì¥
    with open(output_path / "temporal_profiles.json", 'w') as f:
        # datetime ê°ì²´ëŠ” JSON serializableí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜
        profiles_copy = profiles.copy()
        profiles_copy['baseline_period']['start'] = str(profiles_copy['baseline_period']['start'])
        profiles_copy['baseline_period']['end'] = str(profiles_copy['baseline_period']['end'])
        json.dump(profiles_copy, f, indent=2, ensure_ascii=False)
    
    # ì´ìƒ íƒì§€
    print("ğŸ” ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ì¤‘...")
    anomalies = detector.detect_temporal_anomalies(parsed_df, profiles)  # ì´ìƒ íƒì§€
    
    # ì´ìƒ íƒì§€ ê²°ê³¼ ì €ì¥ (numpy typesì„ Python native typesìœ¼ë¡œ ë³€í™˜)  # ì§ë ¬í™” ë³´ì •
    def convert_numpy_types(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        return obj
    
    anomalies_serializable = convert_numpy_types(anomalies)
    with open(output_path / "temporal_anomalies.json", 'w') as f:
        json.dump(anomalies_serializable, f, indent=2, ensure_ascii=False)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    print("ğŸ“„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    report = detector.generate_temporal_report(
        profiles, anomalies, 
        output_path / "temporal_report.md"
    )
    
    print(f"âœ… ì™„ë£Œ! ê²°ê³¼ëŠ” {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸš¨ ë°œê²¬ëœ ì´ìƒ: {len(anomalies)}ê°œ")
    
    # ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥  # ì‹¬ê°ë„ ì¹´ìš´íŠ¸
    if anomalies:
        high_count = len([a for a in anomalies if a['severity'] == 'high'])
        medium_count = len([a for a in anomalies if a['severity'] == 'medium'])
        print(f"   - ì‹¬ê°: {high_count}ê°œ, ì£¼ì˜: {medium_count}ê°œ")
    
    return anomalies, profiles  # ê²°ê³¼ ë°˜í™˜

def main():  # CLI ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
    parser = argparse.ArgumentParser(description="ì‹œê°„ íŒ¨í„´ ê¸°ë°˜ ë¡œê·¸ ì´ìƒ íƒì§€")  # ì¸ì íŒŒì„œ
    parser.add_argument("--data-dir", required=True, 
                       help="ë¶„ì„í•  ë°ì´í„° ë””ë ‰í† ë¦¬ (parsed.parquet í¬í•¨)")  # ë°ì´í„° í´ë”
    parser.add_argument("--output-dir", 
                       help="ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data-dir/temporal_analysis)")  # ì¶œë ¥ í´ë”
    
    args = parser.parse_args()  # íŒŒì‹±
    analyze_temporal_patterns(args.data_dir, args.output_dir)  # ì‹¤í–‰

if __name__ == "__main__":  # ì§ì ‘ ì‹¤í–‰ ì‹œ
    main()  # ë©”ì¸ í˜¸ì¶œ
