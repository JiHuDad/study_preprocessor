#!/usr/bin/env python3  # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì…”ë±…
"""íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€ ë„êµ¬ ìš”ì•½

- ëª©ì : Target ë¡œê·¸ì™€ ì—¬ëŸ¬ Baseline ë¡œê·¸ì˜ ë¶„í¬/ì§€í‘œë¥¼ ë¹„êµí•˜ì—¬ ì´ìƒ ì°¨ì´ë¥¼ íƒì§€
- ë°©ë²•: í…œí”Œë¦¿ ë¶„í¬(KL, Jaccard), ìˆ˜ì¹˜ ì§€í‘œ(Z-score), ê³ ìœ  í…œí”Œë¦¿ ê³¼ë‹¤ ì—¬ë¶€ ë“± ë¶„ì„
- ì¶œë ¥: í”„ë¡œíŒŒì¼ JSON, ì´ìƒ ëª©ë¡ JSON, ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸
"""  # ëª¨ë“ˆ ìš”ì•½ ì„¤ëª…
import pandas as pd  # ë°ì´í„°í”„ë ˆì„ ì²˜ë¦¬
import numpy as np  # ìˆ˜ì¹˜ ê³„ì‚°
import json  # JSON ì…ì¶œë ¥
from pathlib import Path  # ê²½ë¡œ ì²˜ë¦¬
from typing import Dict, List, Tuple, Optional  # íƒ€ì… íŒíŠ¸
import argparse  # CLI ì¸ì íŒŒì„œ
from collections import Counter  # (ë¯¸ì‚¬ìš©) ì¹´ìš´í„° ìœ í‹¸

class ComparativeAnomalyDetector:  # ë¹„êµ ê¸°ë°˜ ì´ìƒíƒì§€ê¸°
    def __init__(self):  # ì´ˆê¸°í™”
        self.baseline_profiles = {}  # ë² ì´ìŠ¤ë¼ì¸ í”„ë¡œíŒŒì¼ ìºì‹œ(ì˜µì…˜)
        self.comparison_threshold = 0.3  # 30% ì´ìƒ ì°¨ì´ëŠ” ì´ìƒìœ¼ë¡œ ê°„ì£¼ (ì˜ˆë¹„ ì„ê³„)
    
    def extract_file_profile(self, parsed_df: pd.DataFrame, file_name: str) -> Dict:  # ë‹¨ì¼ íŒŒì¼ í”„ë¡œíŒŒì¼ ì¶”ì¶œ
        """ë‹¨ì¼ íŒŒì¼ì˜ í”„ë¡œíŒŒì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""  # API ì„¤ëª…
        
        if len(parsed_df) == 0:  # ë¹ˆ ë°ì´í„°ì…‹ ì²˜ë¦¬
            return {
                'file_name': file_name,  # íŒŒì¼ëª…
                'total_logs': 0,  # ì´ ë¡œê·¸ ìˆ˜
                'unique_templates': 0,  # ê³ ìœ  í…œí”Œë¦¿ ìˆ˜
                'template_distribution': {},  # í…œí”Œë¦¿ ë¶„í¬
                'error': 'Empty dataset'  # ì˜¤ë¥˜ ë©”ì‹œì§€
            }
        
        # ê¸°ë³¸ í†µê³„  # í…œí”Œë¦¿ ë¶„í¬ ê³„ì‚°
        template_counts = parsed_df['template_id'].value_counts()
        template_dist = (template_counts / len(parsed_df)).to_dict()
        
        # í˜¸ìŠ¤íŠ¸ë³„ ë¶„í¬  # host ì»¬ëŸ¼ì´ ìˆì„ ë•Œë§Œ ê³„ì‚°
        host_dist = parsed_df['host'].value_counts(normalize=True).to_dict() if 'host' in parsed_df.columns else {}
        
        # í”„ë¡œì„¸ìŠ¤ë³„ ë¶„í¬  # process ì»¬ëŸ¼ì´ ìˆì„ ë•Œë§Œ ê³„ì‚°
        process_dist = parsed_df['process'].value_counts(normalize=True).to_dict() if 'process' in parsed_df.columns else {}
        
        # ì‹œê°„ ë²”ìœ„  # ì‹œì‘/ë/ì§€ì†ì‹œê°„(ì‹œê°„)
        time_range = {
            'start': str(parsed_df['timestamp'].min()) if 'timestamp' in parsed_df.columns else None,
            'end': str(parsed_df['timestamp'].max()) if 'timestamp' in parsed_df.columns else None,
            'duration_hours': None
        }
        
        if time_range['start'] and time_range['end']:  # ì§€ì†ì‹œê°„ ê³„ì‚°
            start_dt = pd.to_datetime(time_range['start'])
            end_dt = pd.to_datetime(time_range['end'])
            time_range['duration_hours'] = (end_dt - start_dt).total_seconds() / 3600
        
        # ì—ëŸ¬/ê²½ê³  ë¡œê·¸ ë¹„ìœ¨ (í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì •)  # ë‹¨ìˆœ íœ´ë¦¬ìŠ¤í‹±
        error_keywords = ['error', 'ERROR', 'fail', 'FAIL', 'exception', 'Exception', 'panic', 'fatal']
        warning_keywords = ['warn', 'WARN', 'warning', 'WARNING']
        
        error_logs = parsed_df[parsed_df['raw'].str.contains('|'.join(error_keywords), case=False, na=False)]  # ì—ëŸ¬ í¬í•¨
        warning_logs = parsed_df[parsed_df['raw'].str.contains('|'.join(warning_keywords), case=False, na=False)]  # ê²½ê³  í¬í•¨
        
        profile = {
            'file_name': file_name,  # íŒŒì¼ëª…
            'total_logs': len(parsed_df),  # ì´ ë¡œê·¸ ìˆ˜
            'unique_templates': len(template_counts),  # ê³ ìœ  í…œí”Œë¦¿ ìˆ˜
            'template_distribution': template_dist,  # í…œí”Œë¦¿ ë¶„í¬
            'top_templates': list(template_counts.head(10).index),  # ìƒìœ„ í…œí”Œë¦¿
            'host_distribution': host_dist,  # í˜¸ìŠ¤íŠ¸ ë¶„í¬
            'process_distribution': process_dist,  # í”„ë¡œì„¸ìŠ¤ ë¶„í¬
            'time_range': time_range,  # ì‹œê°„ ë²”ìœ„
            'error_rate': len(error_logs) / len(parsed_df),  # ì—ëŸ¬ìœ¨
            'warning_rate': len(warning_logs) / len(parsed_df),  # ê²½ê³ ìœ¨
            'logs_per_hour': len(parsed_df) / max(time_range['duration_hours'], 1) if time_range['duration_hours'] else None,  # ì‹œê°„ë‹¹ ë¡œê·¸
            'template_entropy': self._calculate_entropy(template_dist),  # ì—”íŠ¸ë¡œí”¼
            'rare_templates_count': len([t for t, count in template_counts.items() if count == 1])  # 1íšŒ ë“±ì¥ í…œí”Œë¦¿ ìˆ˜
        }
        
        return profile  # í”„ë¡œíŒŒì¼ ë°˜í™˜
    
    def _calculate_entropy(self, distribution: Dict) -> float:  # ì—”íŠ¸ë¡œí”¼ ê³„ì‚°
        """ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
        if not distribution:  # ë¹ˆ ë¶„í¬ ì²˜ë¦¬
            return 0.0
        
        probs = list(distribution.values())  # í™•ë¥  ë¦¬ìŠ¤íŠ¸
        probs = [p for p in probs if p > 0]  # 0 í™•ë¥  ì œê±°
        
        if not probs:  # ìœ íš¨ ê°’ ì—†ìŒ
            return 0.0
        
        return -sum(p * np.log2(p) for p in probs)  # ìƒ¤ë…¼ ì—”íŠ¸ë¡œí”¼
    
    def compare_distributions(self, target_dist: Dict, baseline_dist: Dict) -> Dict:  # ë¶„í¬ ë¹„êµ
        """ë‘ ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
        
        # KL Divergence ê³„ì‚° (ë¶„í¬ ê°„ ì°¨ì´ ì¸¡ì •)  # ë¹„ëŒ€ì¹­ì„± ì£¼ì˜
        def kl_divergence(p, q):
            # ì‘ì€ ê°’ìœ¼ë¡œ smoothing  # 0 ë¶„ëª¨ ë°©ì§€
            epsilon = 1e-10
            all_keys = set(p.keys()) | set(q.keys())  # ëª¨ë“  í‚¤ í•©ì§‘í•©
            
            p_smooth = {k: p.get(k, 0) + epsilon for k in all_keys}  # í‰í™œí™”
            q_smooth = {k: q.get(k, 0) + epsilon for k in all_keys}
            
            # ì •ê·œí™”  # í™•ë¥ ë¡œ ë³€í™˜
            p_sum = sum(p_smooth.values())
            q_sum = sum(q_smooth.values())
            p_norm = {k: v/p_sum for k, v in p_smooth.items()}
            q_norm = {k: v/q_sum for k, v in q_smooth.items()}
            
            return sum(p_norm[k] * np.log(p_norm[k] / q_norm[k]) for k in all_keys)  # KL ê³„ì‚°
        
        kl_div = kl_divergence(target_dist, baseline_dist)  # KL ê°’
        
        # ê³µí†µ í…œí”Œë¦¿ê³¼ ìœ ë‹ˆí¬ í…œí”Œë¦¿ ë¶„ì„  # ì§‘í•© ê¸°ë°˜ ë¹„êµ
        target_templates = set(target_dist.keys())
        baseline_templates = set(baseline_dist.keys())
        
        common_templates = target_templates & baseline_templates  # êµì§‘í•©
        target_only = target_templates - baseline_templates  # íƒ€ê¹ƒì—ë§Œ ì¡´ì¬
        baseline_only = baseline_templates - target_templates  # ë² ì´ìŠ¤ë¼ì¸ì—ë§Œ ì¡´ì¬
        
        # ìƒìœ„ í…œí”Œë¦¿ ì°¨ì´ ë¶„ì„  # ìƒìœ„ nê°œ ì¶”ì¶œê¸°
        def get_top_templates(dist, n=5):
            return sorted(dist.items(), key=lambda x: x[1], reverse=True)[:n]
        
        target_top = get_top_templates(target_dist)  # íƒ€ê¹ƒ ìƒìœ„
        baseline_top = get_top_templates(baseline_dist)  # ë² ì´ìŠ¤ë¼ì¸ ìƒìœ„
        
        return {
            'kl_divergence': kl_div,  # KL ê°’
            'common_templates': len(common_templates),  # ê³µí†µ ê°œìˆ˜
            'target_only_templates': len(target_only),  # íƒ€ê¹ƒ ìœ ë‹ˆí¬ ê°œìˆ˜
            'baseline_only_templates': len(baseline_only),  # ë² ì´ìŠ¤ë¼ì¸ ìœ ë‹ˆí¬ ê°œìˆ˜
            'jaccard_similarity': len(common_templates) / len(target_templates | baseline_templates) if target_templates | baseline_templates else 0,  # ìì¹´ë“œ ìœ ì‚¬ë„
            'target_top_templates': target_top,  # íƒ€ê¹ƒ ìƒìœ„ ëª©ë¡
            'baseline_top_templates': baseline_top,  # ë² ì´ìŠ¤ë¼ì¸ ìƒìœ„ ëª©ë¡
            'unique_in_target': list(target_only)[:10],  # ìµœëŒ€ 10ê°œ  # íƒ€ê¹ƒ ìœ ë‹ˆí¬ ìƒ˜í”Œ
            'missing_from_target': list(baseline_only)[:10]  # ìµœëŒ€ 10ê°œ  # íƒ€ê¹ƒì—ì„œ ëˆ„ë½ëœ ìƒ˜í”Œ
        }
    
    def detect_comparative_anomalies(self, target_profile: Dict, 
                                   baseline_profiles: List[Dict]) -> List[Dict]:  # ë¹„êµ ê¸°ë°˜ ì´ìƒ íƒì§€
        """Target íŒŒì¼ê³¼ Baseline íŒŒì¼ë“¤ì„ ë¹„êµí•˜ì—¬ ì´ìƒì„ íƒì§€í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
        
        anomalies = []  # ì´ìƒ ê²°ê³¼ ëª©ë¡
        
        # Baseline í‰ê·  ê³„ì‚°  # ë² ì´ìŠ¤ë¼ì¸ì´ ì—†ìœ¼ë©´ ì¹˜ëª…ì  ì´ìŠˆ ë¦¬í„´
        if not baseline_profiles:
            return [{
                'type': 'no_baseline',
                'severity': 'high',
                'description': 'Baseline íŒŒì¼ì´ ì—†ì–´ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
            }]
        
        # ìˆ˜ì¹˜ ì§€í‘œë“¤ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°  # ê¸°ì¤€ ë¶„í¬ ì‚°ì¶œ
        metrics = ['total_logs', 'unique_templates', 'error_rate', 'warning_rate', 
                  'template_entropy', 'rare_templates_count']
        
        baseline_stats = {}  # ë©”íŠ¸ë¦­ë³„ í†µê³„
        for metric in metrics:
            values = [p.get(metric, 0) for p in baseline_profiles if p.get(metric) is not None]
            if values:
                baseline_stats[metric] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values)
                }
        
        # ê° ì§€í‘œë³„ ì´ìƒ ê²€ì‚¬  # Z-score ê¸°ë°˜
        for metric, stats in baseline_stats.items():
            target_value = target_profile.get(metric, 0)
            if target_value is None:
                continue
                
            # Z-score ê³„ì‚°  # í‘œì¤€í¸ì°¨ 0 ë°©ì§€
            z_score = abs(target_value - stats['mean']) / max(stats['std'], 1e-6)
            
            if z_score > 2.0:  # 2Ïƒ ì´ìƒ ì°¨ì´ë©´ ì´ìƒ
                deviation_pct = abs(target_value - stats['mean']) / max(stats['mean'], 1e-6)
                anomalies.append({
                    'type': f'metric_anomaly_{metric}',
                    'severity': 'high' if z_score > 3.0 else 'medium',
                    'description': f'{metric}ì´ baseline ëŒ€ë¹„ {deviation_pct:.1%} ì°¨ì´ (Z-score: {z_score:.2f})',
                    'target_value': target_value,
                    'baseline_mean': stats['mean'],
                    'baseline_std': stats['std'],
                    'z_score': z_score
                })
        
        # í…œí”Œë¦¿ ë¶„í¬ ë¹„êµ  # í‰ê·  ë¶„í¬ì™€ íƒ€ê¹ƒ ë¶„í¬ ë¹„êµ
        if baseline_profiles and 'template_distribution' in target_profile:
            # ëª¨ë“  baselineì˜ í…œí”Œë¦¿ ë¶„í¬ë¥¼ í‰ê· í™”  # í‚¤ë³„ í•©ì‚° í›„ í‰ê· 
            baseline_combined = {}
            for profile in baseline_profiles:
                for template, prob in profile.get('template_distribution', {}).items():
                    baseline_combined[template] = baseline_combined.get(template, 0) + prob
            
            # í‰ê· í™”  # ìƒ˜í”Œ ìˆ˜ë¡œ ë‚˜ëˆ”
            baseline_count = len(baseline_profiles)
            baseline_avg_dist = {k: v/baseline_count for k, v in baseline_combined.items()}
            
            # ë¶„í¬ ë¹„êµ  # KL/Jaccard/ìœ ë‹ˆí¬ ë“±
            comparison = self.compare_distributions(
                target_profile['template_distribution'], 
                baseline_avg_dist
            )
            
            # KL Divergenceê°€ ë†’ìœ¼ë©´ ì´ìƒ  # ì„ê³„ 1.0/2.0
            if comparison['kl_divergence'] > 1.0:  # ì„ê³„ê°’
                anomalies.append({
                    'type': 'template_distribution_anomaly',
                    'severity': 'high' if comparison['kl_divergence'] > 2.0 else 'medium',
                    'description': f'í…œí”Œë¦¿ ë¶„í¬ê°€ baselineê³¼ í¬ê²Œ ë‹¤ë¦„ (KL: {comparison["kl_divergence"]:.3f})',
                    'kl_divergence': comparison['kl_divergence'],
                    'jaccard_similarity': comparison['jaccard_similarity'],
                    'unique_templates': comparison['target_only_templates'],
                    'missing_templates': comparison['baseline_only_templates']
                })
            
            # ê³ ìœ  í…œí”Œë¦¿ì´ ë§ìœ¼ë©´ ì´ìƒ  # í‰ê·  ë¶„í¬ í¬ê¸°ì˜ 20% ì´ˆê³¼
            if comparison['target_only_templates'] > len(baseline_avg_dist) * 0.2:
                anomalies.append({
                    'type': 'excessive_unique_templates',
                    'severity': 'medium',
                    'description': f'Targetì—ë§Œ ìˆëŠ” í…œí”Œë¦¿ì´ {comparison["target_only_templates"]}ê°œë¡œ ê³¼ë‹¤',
                    'unique_templates_count': comparison['target_only_templates'],
                    'unique_templates': comparison['unique_in_target']
                })
        
        return anomalies  # ì´ìƒ ëª©ë¡ ë°˜í™˜
    
    def generate_comparative_report(self, target_profile: Dict, baseline_profiles: List[Dict], 
                                  anomalies: List[Dict], output_path: str) -> str:  # ë¹„êµ ë¦¬í¬íŠ¸ ìƒì„±
        """ë¹„êµ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
        
        report = f"""# íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€ ë¦¬í¬íŠ¸

## ğŸ“Š ë¶„ì„ ëŒ€ìƒ
- **Target íŒŒì¼**: {target_profile['file_name']}
- **Baseline íŒŒì¼**: {len(baseline_profiles)}ê°œ
- **Target ë¡œê·¸ ìˆ˜**: {target_profile['total_logs']:,}ê°œ
- **Target í…œí”Œë¦¿ ìˆ˜**: {target_profile['unique_templates']}ê°œ

## ğŸ“ˆ Baseline í†µê³„
"""
        
        if baseline_profiles:  # ë² ì´ìŠ¤ë¼ì¸ ìš”ì•½ í†µê³„
            total_logs = [p['total_logs'] for p in baseline_profiles]
            unique_templates = [p['unique_templates'] for p in baseline_profiles]
            error_rates = [p.get('error_rate', 0) for p in baseline_profiles]
            
            report += f"""
- **í‰ê·  ë¡œê·¸ ìˆ˜**: {np.mean(total_logs):,.0f}ê°œ (Â±{np.std(total_logs):.0f})
- **í‰ê·  í…œí”Œë¦¿ ìˆ˜**: {np.mean(unique_templates):.0f}ê°œ (Â±{np.std(unique_templates):.0f})
- **í‰ê·  ì—ëŸ¬ìœ¨**: {np.mean(error_rates):.2%} (Â±{np.std(error_rates):.2%})

### Baseline íŒŒì¼ ëª©ë¡
"""
            for i, profile in enumerate(baseline_profiles, 1):  # íŒŒì¼ë³„ ê°œìš”
                report += f"{i}. {profile['file_name']} - {profile['total_logs']:,}ê°œ ë¡œê·¸, {profile['unique_templates']}ê°œ í…œí”Œë¦¿\n"
        
        # ì´ìƒ íƒì§€ ê²°ê³¼  # ì„¹ì…˜ í—¤ë”
        report += "\n## ğŸš¨ ë°œê²¬ëœ ì´ìƒ í˜„ìƒ\n\n"
        
        if not anomalies:  # ì´ìƒ ì—†ìŒ ë©”ì‹œì§€
            report += "âœ… ë¹„êµ ë¶„ì„ì—ì„œ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n"
        else:
            # ì‹¬ê°ë„ë³„ ë¶„ë¥˜  # high/medium ê·¸ë£¹í™”
            high_severity = [a for a in anomalies if a['severity'] == 'high']
            medium_severity = [a for a in anomalies if a['severity'] == 'medium']
            
            if high_severity:
                report += f"### ğŸ”´ ì‹¬ê°í•œ ì´ìƒ ({len(high_severity)}ê°œ)\n\n"
                for anomaly in high_severity:
                    report += f"- **{anomaly['type']}**: {anomaly['description']}\n"
                report += "\n"
            
            if medium_severity:
                report += f"### ğŸŸ¡ ì£¼ì˜ í•„ìš” ({len(medium_severity)}ê°œ)\n\n"
                for anomaly in medium_severity:
                    report += f"- **{anomaly['type']}**: {anomaly['description']}\n"
                report += "\n"
        
        # ìƒì„¸ ë¹„êµ í‘œ  # ì£¼ìš” ì§€í‘œ í‘œë¡œ í‘œì‹œ
        report += "## ğŸ“‹ ìƒì„¸ ë¹„êµí‘œ\n\n"
        report += "| ì§€í‘œ | Target | Baseline í‰ê·  | ì°¨ì´ |\n"
        report += "|------|---------|---------------|------|\n"
        
        if baseline_profiles:  # í‘œ ë‚´ìš© ìƒì„±
            metrics = [
                ('ë¡œê·¸ ìˆ˜', 'total_logs'),
                ('í…œí”Œë¦¿ ìˆ˜', 'unique_templates'), 
                ('ì—ëŸ¬ìœ¨', 'error_rate'),
                ('ê²½ê³ ìœ¨', 'warning_rate'),
                ('ì—”íŠ¸ë¡œí”¼', 'template_entropy')
            ]
            
            for metric_name, metric_key in metrics:
                target_val = target_profile.get(metric_key, 0)
                baseline_vals = [p.get(metric_key, 0) for p in baseline_profiles if p.get(metric_key) is not None]
                
                if baseline_vals:
                    baseline_mean = np.mean(baseline_vals)
                    diff_pct = (target_val - baseline_mean) / max(baseline_mean, 1e-6) * 100
                    
                    if metric_key in ['error_rate', 'warning_rate']:
                        report += f"| {metric_name} | {target_val:.2%} | {baseline_mean:.2%} | {diff_pct:+.1f}% |\n"
                    elif metric_key == 'template_entropy':
                        report += f"| {metric_name} | {target_val:.2f} | {baseline_mean:.2f} | {diff_pct:+.1f}% |\n"
                    else:
                        report += f"| {metric_name} | {target_val:,} | {baseline_mean:,.0f} | {diff_pct:+.1f}% |\n"
        
        # íŒŒì¼ë¡œ ì €ì¥  # ë§ˆí¬ë‹¤ìš´ ë¦¬í¬íŠ¸ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        return report  # ë¦¬í¬íŠ¸ ë¬¸ìì—´ ë°˜í™˜

def compare_log_files(target_file: str, baseline_files: List[str], output_dir: str = None):  # íŒŒì¼ ë¹„êµ ì‹¤í–‰
    """ì—¬ëŸ¬ ë¡œê·¸ íŒŒì¼ì„ ë¹„êµí•˜ì—¬ ì´ìƒì„ íƒì§€í•©ë‹ˆë‹¤."""  # API ì„¤ëª…
    
    target_path = Path(target_file)  # íƒ€ê¹ƒ ê²½ë¡œ
    if output_dir is None:  # ê¸°ë³¸ ì¶œë ¥ í´ë”
        output_dir = target_path.parent / "comparative_analysis"
    output_path = Path(output_dir)  # Path í™”
    output_path.mkdir(parents=True, exist_ok=True)  # í´ë” ìƒì„±
    
    print("ğŸ” íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€ ì‹œì‘...")  # ì‹œì‘ ë¡œê·¸
    
    detector = ComparativeAnomalyDetector()  # íƒì§€ê¸° ìƒì„±
    
    # Target íŒŒì¼ ë¶„ì„  # í”„ë¡œíŒŒì¼ë§
    print(f"ğŸ“Š Target íŒŒì¼ ë¶„ì„: {target_path.name}")
    target_df = pd.read_parquet(target_file)
    target_profile = detector.extract_file_profile(target_df, target_path.name)
    
    # Baseline íŒŒì¼ë“¤ ë¶„ì„  # ìˆœì°¨ í”„ë¡œíŒŒì¼ë§
    baseline_profiles = []
    print(f"ğŸ“Š Baseline íŒŒì¼ {len(baseline_files)}ê°œ ë¶„ì„...")
    
    for baseline_file in baseline_files:
        baseline_path = Path(baseline_file)
        try:
            baseline_df = pd.read_parquet(baseline_file)
            profile = detector.extract_file_profile(baseline_df, baseline_path.name)
            baseline_profiles.append(profile)
            print(f"   âœ… {baseline_path.name}: {profile['total_logs']}ê°œ ë¡œê·¸")
        except Exception as e:
            print(f"   âŒ {baseline_path.name}: ë¡œë“œ ì‹¤íŒ¨ ({e})")
    
    # í”„ë¡œíŒŒì¼ ì €ì¥  # numpy íƒ€ì… ì§ë ¬í™” ë³´ì • í¬í•¨
    # í”„ë¡œíŒŒì¼ ì €ì¥ (numpy types ë³€í™˜)
    def convert_numpy_types(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        return obj
    
    all_profiles = {
        'target': convert_numpy_types(target_profile),  # íƒ€ê¹ƒ í”„ë¡œíŒŒì¼
        'baselines': convert_numpy_types(baseline_profiles)  # ë² ì´ìŠ¤ë¼ì¸ í”„ë¡œíŒŒì¼ë“¤
    }
    with open(output_path / "file_profiles.json", 'w') as f:
        json.dump(all_profiles, f, indent=2, ensure_ascii=False)
    
    # ì´ìƒ íƒì§€  # ë¹„êµ ë¶„ì„ ì‹¤í–‰
    print("ğŸ” ë¹„êµ ë¶„ì„ ì¤‘...")
    anomalies = detector.detect_comparative_anomalies(target_profile, baseline_profiles)
    
    anomalies_serializable = convert_numpy_types(anomalies)  # ì§ë ¬í™” ë³´ì •
    with open(output_path / "comparative_anomalies.json", 'w') as f:
        json.dump(anomalies_serializable, f, indent=2, ensure_ascii=False)
    
    # ë¦¬í¬íŠ¸ ìƒì„±  # ë§ˆí¬ë‹¤ìš´ ì €ì¥
    print("ğŸ“„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    report = detector.generate_comparative_report(
        target_profile, baseline_profiles, anomalies,
        output_path / "comparative_report.md"
    )
    
    print(f"âœ… ì™„ë£Œ! ê²°ê³¼ëŠ” {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸš¨ ë°œê²¬ëœ ì´ìƒ: {len(anomalies)}ê°œ")
    
    # ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥  # ì‹¬ê°ë„ ì¹´ìš´íŠ¸
    if anomalies:
        high_count = len([a for a in anomalies if a['severity'] == 'high'])
        medium_count = len([a for a in anomalies if a['severity'] == 'medium'])
        print(f"   - ì‹¬ê°: {high_count}ê°œ, ì£¼ì˜: {medium_count}ê°œ")
    
    return anomalies, target_profile, baseline_profiles  # ê²°ê³¼ íŠœí”Œ ë°˜í™˜

def main():  # CLI ì—”íŠ¸ë¦¬ í¬ì¸íŠ¸
    parser = argparse.ArgumentParser(description="íŒŒì¼ë³„ ë¹„êµ ë¡œê·¸ ì´ìƒ íƒì§€")  # ì¸ì íŒŒì„œ
    parser.add_argument("--target", required=True, 
                       help="ë¶„ì„í•  target íŒŒì¼ (parsed.parquet)")  # íƒ€ê¹ƒ íŒŒì¼
    parser.add_argument("--baselines", required=True, nargs='+',
                       help="ë¹„êµí•  baseline íŒŒì¼ë“¤ (parsed.parquet)")  # ë² ì´ìŠ¤ë¼ì¸ íŒŒì¼ë“¤
    parser.add_argument("--output-dir", 
                       help="ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬")  # ì¶œë ¥ í´ë”
    
    args = parser.parse_args()  # íŒŒì‹±
    compare_log_files(args.target, args.baselines, args.output_dir)  # ì‹¤í–‰

if __name__ == "__main__":  # ì§ì ‘ ì‹¤í–‰ ì‹œ
    main()  # ë©”ì¸ í˜¸ì¶œ
