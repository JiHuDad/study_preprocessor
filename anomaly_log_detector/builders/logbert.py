"""LogBERT ê¸°ë°˜ ë¡œê·¸ ì´ìƒíƒì§€ ëª¨ë“ˆ

LogBERTëŠ” BERT(Bidirectional Encoder Representations from Transformers)ë¥¼ ì‚¬ìš©í•˜ì—¬
ë¡œê·¸ ì‹œí€€ìŠ¤ì˜ ì •ìƒ íŒ¨í„´ì„ í•™ìŠµí•˜ê³  ì´ìƒì„ íƒì§€í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- build_logbert_inputs: LogBERT í•™ìŠµìš© ì…ë ¥ ë°ì´í„° ìƒì„±
- train_logbert: BERT ëª¨ë¸ í•™ìŠµ (Masked Language Modeling)
- infer_logbert: í•™ìŠµëœ ëª¨ë¸ë¡œ ì´ìƒ ë¡œê·¸ íƒì§€
"""

from __future__ import annotations

from pathlib import Path
import json
from typing import Dict, Optional, List, Tuple
from dataclasses import dataclass
import pandas as pd
import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
import numpy as np


def build_logbert_inputs(
    parsed_parquet: str | Path,
    out_dir: str | Path,
    template_col: str = "template",
    vocab_path: str | Path | None = None,
    max_seq_len: int = 512
) -> None:
    """LogBERT í•™ìŠµì„ ìœ„í•œ ì…ë ¥ ë°ì´í„° ìƒì„± (vocab.json, sequences.parquet).

    Args:
        parsed_parquet: íŒŒì‹±ëœ ë¡œê·¸ ë°ì´í„° Parquet íŒŒì¼ ê²½ë¡œ
        out_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬ ê²½ë¡œ
        template_col: í…œí”Œë¦¿ ì»¬ëŸ¼ëª… (ê¸°ë³¸ê°’: "template")
        vocab_path: ê¸°ì¡´ vocab.json ê²½ë¡œ (ì„ íƒì‚¬í•­)
        max_seq_len: ìµœëŒ€ ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 512)
    """
    out = Path(out_dir)
    out.mkdir(parents=True, exist_ok=True)
    df = pd.read_parquet(parsed_parquet)

    # Vocab ë¡œë“œ ë˜ëŠ” ìƒì„±
    if vocab_path and Path(vocab_path).exists():
        with open(vocab_path, 'r') as f:
            vocab: Dict[str, int] = json.load(f)
        print(f"âœ… ê¸°ì¡´ vocab ì‚¬ìš©: {vocab_path} (í¬ê¸°: {len(vocab)})")
    else:
        # BERT íŠ¹ìˆ˜ í† í° ì¶”ê°€
        unique_templates = [t for t in df[template_col].dropna().astype(str).unique()]
        # íŠ¹ìˆ˜ í† í°: [PAD], [CLS], [SEP], [MASK]
        special_tokens = ["[PAD]", "[CLS]", "[SEP]", "[MASK]", "[UNK]"]
        all_tokens = special_tokens + sorted(unique_templates)
        vocab = {t: i for i, t in enumerate(all_tokens)}

        # vocab.json ì €ì¥
        vocab_file = out / "vocab.json"
        vocab_file.write_text(json.dumps(vocab, indent=2))
        print(f"âœ… ìƒˆë¡œìš´ vocab ìƒì„±: {vocab_file} (í¬ê¸°: {len(vocab)})")

    # íŠ¹ìˆ˜ í† í° ì¸ë±ìŠ¤ ì €ì¥
    special_indices = {
        "pad_token_id": vocab.get("[PAD]", 0),
        "cls_token_id": vocab.get("[CLS]", 1),
        "sep_token_id": vocab.get("[SEP]", 2),
        "mask_token_id": vocab.get("[MASK]", 3),
        "unk_token_id": vocab.get("[UNK]", 4)
    }
    (out / "special_tokens.json").write_text(json.dumps(special_indices, indent=2))

    # í…œí”Œë¦¿ì„ ì¸ë±ìŠ¤ë¡œ ë§¤í•‘
    df = df.sort_values(["timestamp", "line_no"], kind="stable", na_position="first")
    df["template_index"] = df[template_col].map(vocab).fillna(special_indices["unk_token_id"]).astype("Int64")

    # ì‹œí€€ìŠ¤ë¡œ ì €ì¥
    df[["line_no", "timestamp", "host", "template_index"]].to_parquet(
        out / "sequences.parquet", index=False
    )
    print(f"âœ… ì‹œí€€ìŠ¤ ì €ì¥: {out / 'sequences.parquet'}")


class LogBERTDataset(Dataset):
    """LogBERTìš© ë°ì´í„°ì…‹ í´ë˜ìŠ¤ (Masked Language Modeling)"""

    def __init__(
        self,
        sequences: List[int],
        seq_len: int = 128,
        mask_ratio: float = 0.15,
        mask_token_id: int = 3,
        pad_token_id: int = 0
    ):
        """
        Args:
            sequences: í…œí”Œë¦¿ ì¸ë±ìŠ¤ ì‹œí€€ìŠ¤
            seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´
            mask_ratio: ë§ˆìŠ¤í‚¹ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.15)
            mask_token_id: [MASK] í† í° ID
            pad_token_id: [PAD] í† í° ID
        """
        self.seq_len = seq_len
        self.mask_ratio = mask_ratio
        self.mask_token_id = mask_token_id
        self.pad_token_id = pad_token_id

        # ì‹œí€€ìŠ¤ë¥¼ ê³ ì • ê¸¸ì´ë¡œ ë¶„í• 
        self.sequences = []
        for i in range(0, len(sequences), seq_len):
            seq = sequences[i:i + seq_len]
            # íŒ¨ë”©
            if len(seq) < seq_len:
                seq = seq + [pad_token_id] * (seq_len - len(seq))
            self.sequences.append(seq)

    def __len__(self) -> int:
        return len(self.sequences)

    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        Returns:
            input_ids: ë§ˆìŠ¤í‚¹ëœ ì…ë ¥ ì‹œí€€ìŠ¤
            labels: ì›ë³¸ ë ˆì´ë¸” (ë§ˆìŠ¤í‚¹ë˜ì§€ ì•Šì€ ìœ„ì¹˜ëŠ” -100)
            attention_mask: ì–´í…ì…˜ ë§ˆìŠ¤í¬ (íŒ¨ë”© ìœ„ì¹˜ëŠ” 0)
        """
        seq = self.sequences[idx].copy()
        labels = [-100] * self.seq_len  # -100ì€ loss ê³„ì‚°ì—ì„œ ë¬´ì‹œë¨

        # ì–´í…ì…˜ ë§ˆìŠ¤í¬ ìƒì„± (íŒ¨ë”©ì´ ì•„ë‹Œ ìœ„ì¹˜ëŠ” 1)
        attention_mask = [1 if token != self.pad_token_id else 0 for token in seq]

        # ëœë¤ ë§ˆìŠ¤í‚¹
        for i in range(self.seq_len):
            if seq[i] != self.pad_token_id and np.random.random() < self.mask_ratio:
                labels[i] = seq[i]  # ì›ë³¸ ë ˆì´ë¸” ì €ì¥
                seq[i] = self.mask_token_id  # ë§ˆìŠ¤í‚¹

        return (
            torch.tensor(seq, dtype=torch.long),
            torch.tensor(labels, dtype=torch.long),
            torch.tensor(attention_mask, dtype=torch.long)
        )


class LogBERTModel(nn.Module):
    """LogBERT ëª¨ë¸ (ê°„ì†Œí™”ëœ BERT ì•„í‚¤í…ì²˜)"""

    def __init__(
        self,
        vocab_size: int,
        hidden_size: int = 768,
        num_hidden_layers: int = 6,
        num_attention_heads: int = 12,
        intermediate_size: int = 3072,
        max_position_embeddings: int = 512,
        dropout: float = 0.1
    ):
        super().__init__()
        self.hidden_size = hidden_size

        # Embedding layers
        self.token_embeddings = nn.Embedding(vocab_size, hidden_size, padding_idx=0)
        self.position_embeddings = nn.Embedding(max_position_embeddings, hidden_size)
        self.layer_norm = nn.LayerNorm(hidden_size)
        self.dropout = nn.Dropout(dropout)

        # Transformer encoder layers
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_attention_heads,
            dim_feedforward=intermediate_size,
            dropout=dropout,
            batch_first=True
        )
        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_hidden_layers)

        # MLM head
        self.mlm_head = nn.Linear(hidden_size, vocab_size)

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        Args:
            input_ids: [batch_size, seq_len]
            attention_mask: [batch_size, seq_len]

        Returns:
            logits: [batch_size, seq_len, vocab_size]
        """
        batch_size, seq_len = input_ids.size()

        # Position IDs ìƒì„±
        position_ids = torch.arange(seq_len, dtype=torch.long, device=input_ids.device)
        position_ids = position_ids.unsqueeze(0).expand(batch_size, -1)

        # Embeddings
        token_embeds = self.token_embeddings(input_ids)
        position_embeds = self.position_embeddings(position_ids)
        embeddings = token_embeds + position_embeds
        embeddings = self.layer_norm(embeddings)
        embeddings = self.dropout(embeddings)

        # Attention mask ë³€í™˜ (0 -> -inf, 1 -> 0)
        if attention_mask is not None:
            attention_mask = attention_mask.bool()
            attention_mask = ~attention_mask  # ë°˜ì „ (Trueì¸ ê³³ì„ ë§ˆìŠ¤í‚¹)

        # Transformer encoding
        hidden_states = self.encoder(embeddings, src_key_padding_mask=attention_mask)

        # MLM prediction
        logits = self.mlm_head(hidden_states)

        return logits


def train_logbert(
    sequences_parquet: str | Path,
    vocab_json: str | Path,
    out_path: str | Path,
    seq_len: int = 128,
    epochs: int = 10,
    batch_size: int = 32,
    lr: float = 5e-5,
    mask_ratio: float = 0.15,
    hidden_size: int = 256,
    num_layers: int = 4,
    num_heads: int = 8
) -> Path:
    """LogBERT ëª¨ë¸ í•™ìŠµ

    Args:
        sequences_parquet: ì‹œí€€ìŠ¤ ë°ì´í„° Parquet íŒŒì¼ ê²½ë¡œ
        vocab_json: ì–´íœ˜ ì‚¬ì „ JSON íŒŒì¼ ê²½ë¡œ
        out_path: ì €ì¥í•  ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 128)
        epochs: í•™ìŠµ ì—í­ ìˆ˜ (ê¸°ë³¸ê°’: 10)
        batch_size: ë°°ì¹˜ í¬ê¸° (ê¸°ë³¸ê°’: 32)
        lr: í•™ìŠµë¥  (ê¸°ë³¸ê°’: 5e-5)
        mask_ratio: ë§ˆìŠ¤í‚¹ ë¹„ìœ¨ (ê¸°ë³¸ê°’: 0.15)
        hidden_size: ì€ë‹‰ì¸µ í¬ê¸° (ê¸°ë³¸ê°’: 256)
        num_layers: Transformer ë ˆì´ì–´ ìˆ˜ (ê¸°ë³¸ê°’: 4)
        num_heads: Attention head ìˆ˜ (ê¸°ë³¸ê°’: 8)

    Returns:
        í•™ìŠµëœ ëª¨ë¸ ì €ì¥ ê²½ë¡œ
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸ”§ ì‚¬ìš© ì¥ì¹˜: {device}")

    # Vocab ë¡œë“œ
    with open(vocab_json, 'r') as f:
        vocab: Dict[str, int] = json.load(f)
    vocab_size = len(vocab)
    print(f"ğŸ“š Vocab í¬ê¸°: {vocab_size}")

    # íŠ¹ìˆ˜ í† í° ë¡œë“œ
    special_tokens_path = Path(vocab_json).parent / "special_tokens.json"
    if special_tokens_path.exists():
        with open(special_tokens_path, 'r') as f:
            special_tokens = json.load(f)
    else:
        special_tokens = {
            "pad_token_id": 0,
            "mask_token_id": 3
        }

    # ì‹œí€€ìŠ¤ ë¡œë“œ
    df = pd.read_parquet(sequences_parquet)
    sequences = df["template_index"].dropna().astype(int).tolist()
    print(f"ğŸ“Š ì´ ë¡œê·¸ ìˆ˜: {len(sequences)}")

    # ë°ì´í„°ì…‹ ìƒì„±
    dataset = LogBERTDataset(
        sequences=sequences,
        seq_len=seq_len,
        mask_ratio=mask_ratio,
        mask_token_id=special_tokens["mask_token_id"],
        pad_token_id=special_tokens["pad_token_id"]
    )
    print(f"ğŸ“¦ ë°ì´í„°ì…‹ í¬ê¸°: {len(dataset)} ì‹œí€€ìŠ¤")

    # ë°ì´í„° ë¡œë”
    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # ëª¨ë¸ ìƒì„±
    model = LogBERTModel(
        vocab_size=vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=num_layers,
        num_attention_heads=num_heads,
        max_position_embeddings=seq_len
    ).to(device)

    # ì˜µí‹°ë§ˆì´ì € ë° ì†ì‹¤ í•¨ìˆ˜
    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)
    criterion = nn.CrossEntropyLoss(ignore_index=-100)  # -100ì€ ë¬´ì‹œ

    # í•™ìŠµ
    model.train()
    print(f"ğŸš€ í•™ìŠµ ì‹œì‘ ({epochs} epochs)...")

    for epoch in range(epochs):
        total_loss = 0
        num_batches = 0

        for input_ids, labels, attention_mask in loader:
            input_ids = input_ids.to(device)
            labels = labels.to(device)
            attention_mask = attention_mask.to(device)

            optimizer.zero_grad()

            # Forward pass
            logits = model(input_ids, attention_mask)

            # Loss ê³„ì‚° (ë§ˆìŠ¤í‚¹ëœ í† í°ì— ëŒ€í•´ì„œë§Œ)
            loss = criterion(logits.view(-1, vocab_size), labels.view(-1))

            # Backward pass
            loss.backward()
            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

        avg_loss = total_loss / num_batches if num_batches > 0 else 0
        print(f"Epoch {epoch + 1}/{epochs} - Loss: {avg_loss:.4f}")

    # ëª¨ë¸ ì €ì¥
    out_path = Path(out_path)
    out_path.parent.mkdir(parents=True, exist_ok=True)

    torch.save({
        "vocab_size": vocab_size,
        "state_dict": model.state_dict(),
        "seq_len": seq_len,
        "hidden_size": hidden_size,
        "num_layers": num_layers,
        "num_heads": num_heads,
        "special_tokens": special_tokens
    }, out_path)

    print(f"âœ… ëª¨ë¸ ì €ì¥: {out_path}")
    return out_path


@torch.no_grad()
def infer_logbert(
    sequences_parquet: str | Path,
    model_path: str | Path,
    vocab_json: str | Path,
    threshold_percentile: float = 95.0,
    seq_len: int = 128
) -> pd.DataFrame:
    """LogBERT ì´ìƒ íƒì§€ ì¶”ë¡ 

    Args:
        sequences_parquet: ì‹œí€€ìŠ¤ ë°ì´í„° Parquet íŒŒì¼ ê²½ë¡œ
        model_path: í•™ìŠµëœ ëª¨ë¸ íŒŒì¼ ê²½ë¡œ
        vocab_json: vocab.json íŒŒì¼ ê²½ë¡œ
        threshold_percentile: ì´ìƒ íŒì • ì„ê³„ê°’ ë°±ë¶„ìœ„ìˆ˜ (ê¸°ë³¸ê°’: 95.0)
        seq_len: ì‹œí€€ìŠ¤ ê¸¸ì´

    Returns:
        ì¶”ë¡  ê²°ê³¼ DataFrame (seq_idx, avg_loss, is_anomaly)
    """
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # ëª¨ë¸ ë¡œë“œ
    state = torch.load(model_path, map_location=device)
    vocab_size = state["vocab_size"]
    hidden_size = state.get("hidden_size", 256)
    num_layers = state.get("num_layers", 4)
    num_heads = state.get("num_heads", 8)
    special_tokens = state.get("special_tokens", {"pad_token_id": 0})

    model = LogBERTModel(
        vocab_size=vocab_size,
        hidden_size=hidden_size,
        num_hidden_layers=num_layers,
        num_attention_heads=num_heads,
        max_position_embeddings=seq_len
    ).to(device)
    model.load_state_dict(state["state_dict"])
    model.eval()

    print(f"âœ… ëª¨ë¸ ë¡œë“œ: {model_path}")

    # ì‹œí€€ìŠ¤ ë¡œë“œ
    df = pd.read_parquet(sequences_parquet)
    sequences = df["template_index"].dropna().astype(int).tolist()

    # ë°ì´í„°ì…‹ ìƒì„± (ì¶”ë¡ ìš© - ë§ˆìŠ¤í‚¹ ì—†ìŒ)
    pad_token_id = special_tokens["pad_token_id"]

    # ì‹œí€€ìŠ¤ë¥¼ ê³ ì • ê¸¸ì´ë¡œ ë¶„í• 
    seq_list = []
    for i in range(0, len(sequences), seq_len):
        seq = sequences[i:i + seq_len]
        if len(seq) < seq_len:
            seq = seq + [pad_token_id] * (seq_len - len(seq))
        seq_list.append(seq)

    # ê° ì‹œí€€ìŠ¤ì˜ perplexity ê³„ì‚°
    losses = []
    criterion = nn.CrossEntropyLoss(reduction='none')

    print(f"ğŸ” ì¶”ë¡  ì‹œì‘ ({len(seq_list)} ì‹œí€€ìŠ¤)...")

    for seq in seq_list:
        input_ids = torch.tensor([seq], dtype=torch.long).to(device)
        attention_mask = torch.tensor(
            [[1 if token != pad_token_id else 0 for token in seq]],
            dtype=torch.long
        ).to(device)

        # Forward pass
        logits = model(input_ids, attention_mask)

        # Loss ê³„ì‚° (ê° í† í°ì— ëŒ€í•´)
        labels = input_ids.clone()
        loss = criterion(logits.view(-1, vocab_size), labels.view(-1))

        # ìœ íš¨í•œ í† í°(íŒ¨ë”© ì•„ë‹˜)ì— ëŒ€í•œ í‰ê·  loss
        valid_mask = (attention_mask.view(-1) == 1)
        if valid_mask.sum() > 0:
            avg_loss = loss[valid_mask].mean().item()
        else:
            avg_loss = 0.0

        losses.append(avg_loss)

    # ì´ìƒ íƒì§€ (ì„ê³„ê°’ ê¸°ë°˜)
    losses_array = np.array(losses)
    threshold = np.percentile(losses_array, threshold_percentile)
    is_anomaly = losses_array > threshold

    print(f"ğŸ“Š Loss í†µê³„:")
    print(f"  - í‰ê· : {losses_array.mean():.4f}")
    print(f"  - ì¤‘ì•™ê°’: {np.median(losses_array):.4f}")
    print(f"  - ì„ê³„ê°’ (p{threshold_percentile}): {threshold:.4f}")
    print(f"  - ì´ìƒ ì‹œí€€ìŠ¤ ìˆ˜: {is_anomaly.sum()} / {len(losses)}")

    # ê²°ê³¼ DataFrame ìƒì„±
    result_df = pd.DataFrame({
        "seq_idx": range(len(losses)),
        "avg_loss": losses,
        "is_anomaly": is_anomaly,
        "threshold": threshold
    })

    return result_df
