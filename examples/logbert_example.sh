#!/bin/bash
# LogBERT ì‚¬ìš© ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸

set -e  # ì˜¤ë¥˜ ë°œìƒ ì‹œ ì¤‘ë‹¨

echo "ğŸ¤– LogBERT ë¡œê·¸ ì´ìƒíƒì§€ ì˜ˆì œ"
echo "================================"
echo ""

# í™˜ê²½ ë³€ìˆ˜ ì„¤ì •
WORK_DIR=${1:-"data/logbert_example"}
LOG_FILE=${2:-"data/raw/synth_long.log"}

echo "ğŸ“ ì‘ì—… ë””ë ‰í† ë¦¬: $WORK_DIR"
echo "ğŸ“„ ë¡œê·¸ íŒŒì¼: $LOG_FILE"
echo ""

# ì‘ì—… ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p "$WORK_DIR"
mkdir -p .cache

# 1. ë¡œê·¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ í•©ì„± ë¡œê·¸ ìƒì„±
if [ ! -f "$LOG_FILE" ]; then
    echo "ğŸ“ 1. í•©ì„± ë¡œê·¸ ìƒì„±..."
    mkdir -p "$(dirname "$LOG_FILE")"
    alog-detect gen-synth \
        --out "$LOG_FILE" \
        --lines 2000 \
        --anomaly-rate 0.05
    echo "âœ… í•©ì„± ë¡œê·¸ ìƒì„± ì™„ë£Œ: $LOG_FILE"
    echo ""
fi

# 2. ë¡œê·¸ íŒŒì‹±
echo "ğŸ”§ 2. ë¡œê·¸ íŒŒì‹± ë° ì „ì²˜ë¦¬..."
alog-detect parse \
    --input "$LOG_FILE" \
    --out-dir "$WORK_DIR" \
    --drain-state .cache/drain3_logbert.json
echo "âœ… íŒŒì‹± ì™„ë£Œ: $WORK_DIR/parsed.parquet"
echo ""

# 3. LogBERT ì…ë ¥ ìƒì„±
echo "ğŸ“¦ 3. LogBERT ì…ë ¥ ë°ì´í„° ìƒì„±..."
alog-detect build-logbert \
    --parsed "$WORK_DIR/parsed.parquet" \
    --out-dir "$WORK_DIR" \
    --max-seq-len 512
echo "âœ… LogBERT ì…ë ¥ ìƒì„± ì™„ë£Œ:"
echo "   - $WORK_DIR/vocab.json"
echo "   - $WORK_DIR/sequences.parquet"
echo "   - $WORK_DIR/special_tokens.json"
echo ""

# 4. LogBERT ëª¨ë¸ í•™ìŠµ
echo "ğŸ“ 4. LogBERT ëª¨ë¸ í•™ìŠµ..."
echo "   (ì‘ì€ ëª¨ë¸ë¡œ ë¹ ë¥¸ í•™ìŠµ - ì‹¤ì œ ì‚¬ìš© ì‹œ íŒŒë¼ë¯¸í„° ì¡°ì • í•„ìš”)"
alog-detect logbert-train \
    --seq "$WORK_DIR/sequences.parquet" \
    --vocab "$WORK_DIR/vocab.json" \
    --out .cache/logbert_example.pth \
    --seq-len 64 \
    --epochs 5 \
    --batch-size 16 \
    --hidden-size 128 \
    --num-layers 3 \
    --num-heads 4 \
    --lr 0.0001
echo "âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: .cache/logbert_example.pth"
echo ""

# 5. LogBERT ì´ìƒíƒì§€ ì¶”ë¡ 
echo "ğŸ” 5. LogBERT ì´ìƒíƒì§€ ì¶”ë¡ ..."
alog-detect logbert-infer \
    --seq "$WORK_DIR/sequences.parquet" \
    --model .cache/logbert_example.pth \
    --vocab "$WORK_DIR/vocab.json" \
    --threshold-percentile 90.0 \
    --seq-len 64
echo "âœ… ì¶”ë¡  ì™„ë£Œ: $WORK_DIR/logbert_infer.parquet"
echo ""

# 6. ê²°ê³¼ ìš”ì•½
echo "ğŸ“Š 6. ê²°ê³¼ ìš”ì•½"
echo "================================"
python3 - <<EOF
import pandas as pd

# ì¶”ë¡  ê²°ê³¼ ë¡œë“œ
results = pd.read_parquet('$WORK_DIR/logbert_infer.parquet')

print(f"ì´ ì‹œí€€ìŠ¤ ìˆ˜: {len(results)}")
print(f"ì´ìƒ ì‹œí€€ìŠ¤ ìˆ˜: {results['is_anomaly'].sum()}")
print(f"ì´ìƒë¥ : {results['is_anomaly'].mean():.2%}")
print(f"\nLoss í†µê³„:")
print(f"  - ìµœì†Œ: {results['avg_loss'].min():.4f}")
print(f"  - ìµœëŒ€: {results['avg_loss'].max():.4f}")
print(f"  - í‰ê· : {results['avg_loss'].mean():.4f}")
print(f"  - ì¤‘ì•™ê°’: {results['avg_loss'].median():.4f}")
print(f"  - ì„ê³„ê°’: {results['threshold'].iloc[0]:.4f}")

# ìƒìœ„ ì´ìƒ ì‹œí€€ìŠ¤ í‘œì‹œ
if results['is_anomaly'].sum() > 0:
    print(f"\nğŸš¨ ìƒìœ„ ì´ìƒ ì‹œí€€ìŠ¤ (Top 5):")
    top_anomalies = results[results['is_anomaly']].nlargest(5, 'avg_loss')
    for idx, row in top_anomalies.iterrows():
        print(f"  ì‹œí€€ìŠ¤ #{row['seq_idx']}: Loss={row['avg_loss']:.4f}")
EOF

echo ""
echo "âœ… LogBERT ì˜ˆì œ ì™„ë£Œ!"
echo ""
echo "ğŸ“ ìƒì„±ëœ íŒŒì¼:"
echo "   - $WORK_DIR/parsed.parquet"
echo "   - $WORK_DIR/vocab.json"
echo "   - $WORK_DIR/sequences.parquet"
echo "   - $WORK_DIR/special_tokens.json"
echo "   - $WORK_DIR/logbert_infer.parquet"
echo "   - .cache/logbert_example.pth"
echo ""
echo "ğŸ’¡ Tip: ì‹¤ì œ ì‚¬ìš© ì‹œì—ëŠ” ë‹¤ìŒ íŒŒë¼ë¯¸í„°ë¥¼ ì¡°ì •í•˜ì„¸ìš”:"
echo "   - --seq-len: ì‹œí€€ìŠ¤ ê¸¸ì´ (128~512)"
echo "   - --epochs: ì—í­ ìˆ˜ (10~30)"
echo "   - --hidden-size: ì€ë‹‰ì¸µ í¬ê¸° (256~768)"
echo "   - --num-layers: ë ˆì´ì–´ ìˆ˜ (4~12)"
echo "   - --num-heads: Attention head ìˆ˜ (8~16)"
