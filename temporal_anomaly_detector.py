#!/usr/bin/env python3
"""
ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ë„êµ¬
- ì‹œê°„ëŒ€ë³„/ìš”ì¼ë³„ í”„ë¡œíŒŒì¼ í•™ìŠµ
- ê³¼ê±° ë™ì¼ ì‹œê°„ëŒ€ì™€ í˜„ì¬ íŒ¨í„´ ë¹„êµ
"""
import pandas as pd
import numpy as np
import json
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import argparse
from collections import defaultdict

class TemporalAnomalyDetector:
    def __init__(self):
        self.hourly_profiles = {}
        self.daily_profiles = {}
        self.template_baseline = {}
    
    def build_temporal_profiles(self, parsed_df: pd.DataFrame) -> Dict:
        """ì‹œê°„ëŒ€ë³„/ìš”ì¼ë³„ ì •ìƒ í”„ë¡œíŒŒì¼ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."""
        
        # timestampë¥¼ datetimeìœ¼ë¡œ ë³€í™˜
        parsed_df['datetime'] = pd.to_datetime(parsed_df['timestamp'])
        parsed_df['hour'] = parsed_df['datetime'].dt.hour
        parsed_df['weekday'] = parsed_df['datetime'].dt.weekday  # 0=Monday
        parsed_df['date'] = parsed_df['datetime'].dt.date
        
        profiles = {
            'hourly': {},
            'daily': {},
            'baseline_period': {
                'start': parsed_df['datetime'].min(),
                'end': parsed_df['datetime'].max(),
                'total_days': (parsed_df['datetime'].max() - parsed_df['datetime'].min()).days + 1
            }
        }
        
        # ì‹œê°„ëŒ€ë³„ í”„ë¡œíŒŒì¼ (0-23ì‹œ)
        for hour in range(24):
            hour_data = parsed_df[parsed_df['hour'] == hour]
            if len(hour_data) > 0:
                template_dist = hour_data['template_id'].value_counts(normalize=True).to_dict()
                profiles['hourly'][hour] = {
                    'volume_mean': len(hour_data) / profiles['baseline_period']['total_days'],
                    'volume_std': len(hour_data) * 0.1,  # ì„ì‹œ ì¶”ì •ì¹˜
                    'template_distribution': template_dist,
                    'top_templates': list(hour_data['template_id'].value_counts().head(5).index),
                    'unique_templates': len(hour_data['template_id'].unique())
                }
        
        # ìš”ì¼ë³„ í”„ë¡œíŒŒì¼ (0=ì›”ìš”ì¼)
        for weekday in range(7):
            day_data = parsed_df[parsed_df['weekday'] == weekday]
            if len(day_data) > 0:
                template_dist = day_data['template_id'].value_counts(normalize=True).to_dict()
                profiles['daily'][weekday] = {
                    'volume_mean': len(day_data) / (profiles['baseline_period']['total_days'] // 7 + 1),
                    'template_distribution': template_dist,
                    'top_templates': list(day_data['template_id'].value_counts().head(10).index),
                    'unique_templates': len(day_data['template_id'].unique())
                }
        
        return profiles
    
    def detect_temporal_anomalies(self, parsed_df: pd.DataFrame, profiles: Dict, 
                                time_window_hours: int = 1) -> List[Dict]:
        """ì‹œê°„ëŒ€ë³„ í”„ë¡œíŒŒì¼ê³¼ ë¹„êµí•˜ì—¬ ì´ìƒì„ íƒì§€í•©ë‹ˆë‹¤."""
        
        parsed_df['datetime'] = pd.to_datetime(parsed_df['timestamp'])
        parsed_df['hour'] = parsed_df['datetime'].dt.hour
        parsed_df['weekday'] = parsed_df['datetime'].dt.weekday
        
        anomalies = []
        
        # ì‹œê°„ë³„ë¡œ ê·¸ë£¹í™”í•˜ì—¬ ë¶„ì„
        for hour in sorted(parsed_df['hour'].unique()):
            hour_data = parsed_df[parsed_df['hour'] == hour]
            
            if hour not in profiles['hourly']:
                # í”„ë¡œíŒŒì¼ì´ ì—†ëŠ” ì‹œê°„ëŒ€ëŠ” ëª¨ë‘ ì´ìƒìœ¼ë¡œ í‘œì‹œ
                anomalies.append({
                    'type': 'temporal_unseen_hour',
                    'hour': hour,
                    'severity': 'high',
                    'description': f'{hour}ì‹œëŠ” ê³¼ê±° ë°ì´í„°ì— ì—†ëŠ” ì‹œê°„ëŒ€ì…ë‹ˆë‹¤',
                    'actual_volume': len(hour_data),
                    'expected_volume': 'unknown'
                })
                continue
            
            expected_profile = profiles['hourly'][hour]
            actual_volume = len(hour_data)
            expected_volume = expected_profile['volume_mean']
            
            # ë³¼ë¥¨ ì´ìƒ ê²€ì‚¬
            volume_deviation = abs(actual_volume - expected_volume) / max(expected_volume, 1)
            if volume_deviation > 0.5:  # 50% ì´ìƒ ì°¨ì´
                anomalies.append({
                    'type': 'temporal_volume_anomaly',
                    'hour': hour,
                    'severity': 'high' if volume_deviation > 1.0 else 'medium',
                    'description': f'{hour}ì‹œ ë¡œê·¸ ë³¼ë¥¨ì´ ì˜ˆìƒê³¼ {volume_deviation:.1%} ì°¨ì´',
                    'actual_volume': actual_volume,
                    'expected_volume': expected_volume,
                    'deviation': volume_deviation
                })
            
            # í…œí”Œë¦¿ ë¶„í¬ ì´ìƒ ê²€ì‚¬
            if len(hour_data) > 0:
                actual_templates = set(hour_data['template_id'].unique())
                expected_templates = set(expected_profile['top_templates'])
                
                # ìƒˆë¡œìš´ í…œí”Œë¦¿ ë°œê²¬
                new_templates = actual_templates - expected_templates
                if new_templates and len(new_templates) > len(expected_templates) * 0.2:
                    anomalies.append({
                        'type': 'temporal_new_templates',
                        'hour': hour,
                        'severity': 'medium',
                        'description': f'{hour}ì‹œì— {len(new_templates)}ê°œì˜ ìƒˆë¡œìš´ í…œí”Œë¦¿ ë°œê²¬',
                        'new_templates': list(new_templates)[:5],  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                        'expected_templates_count': len(expected_templates),
                        'new_templates_count': len(new_templates)
                    })
                
                # ê¸°ì¡´ ì£¼ìš” í…œí”Œë¦¿ ëˆ„ë½
                missing_templates = expected_templates - actual_templates
                if missing_templates:
                    anomalies.append({
                        'type': 'temporal_missing_templates',
                        'hour': hour,
                        'severity': 'medium',
                        'description': f'{hour}ì‹œì— í‰ì†Œ ì£¼ìš” í…œí”Œë¦¿ {len(missing_templates)}ê°œ ëˆ„ë½',
                        'missing_templates': list(missing_templates),
                        'missing_count': len(missing_templates)
                    })
        
        return anomalies
    
    def generate_temporal_report(self, profiles: Dict, anomalies: List[Dict], 
                               output_path: str) -> str:
        """ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        report = f"""# ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ë¦¬í¬íŠ¸

## ğŸ“Š ê¸°ì¤€ í”„ë¡œíŒŒì¼ ì •ë³´
- **ë¶„ì„ ê¸°ê°„**: {profiles['baseline_period']['start']} ~ {profiles['baseline_period']['end']}
- **ì´ ë¶„ì„ ì¼ìˆ˜**: {profiles['baseline_period']['total_days']}ì¼
- **ì‹œê°„ëŒ€ë³„ í”„ë¡œíŒŒì¼**: {len(profiles['hourly'])}ê°œ
- **ìš”ì¼ë³„ í”„ë¡œíŒŒì¼**: {len(profiles['daily'])}ê°œ

## ğŸš¨ ë°œê²¬ëœ ì´ìƒ í˜„ìƒ

"""
        
        if not anomalies:
            report += "âœ… ì‹œê°„ ê¸°ë°˜ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n"
        else:
            # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
            high_severity = [a for a in anomalies if a['severity'] == 'high']
            medium_severity = [a for a in anomalies if a['severity'] == 'medium']
            
            if high_severity:
                report += f"### ğŸ”´ ì‹¬ê°í•œ ì´ìƒ ({len(high_severity)}ê°œ)\n\n"
                for anomaly in high_severity:
                    report += f"- **{anomaly['type']}** ({anomaly['hour']}ì‹œ): {anomaly['description']}\n"
                report += "\n"
            
            if medium_severity:
                report += f"### ğŸŸ¡ ì£¼ì˜ í•„ìš” ({len(medium_severity)}ê°œ)\n\n"
                for anomaly in medium_severity:
                    report += f"- **{anomaly['type']}** ({anomaly['hour']}ì‹œ): {anomaly['description']}\n"
                report += "\n"
        
        # ì‹œê°„ëŒ€ë³„ ìš”ì•½
        report += "## ğŸ“ˆ ì‹œê°„ëŒ€ë³„ í™œë™ íŒ¨í„´\n\n"
        if 'hourly' in profiles:
            for hour in sorted(profiles['hourly'].keys()):
                profile = profiles['hourly'][hour]
                report += f"- **{hour:02d}ì‹œ**: í‰ê·  {profile['volume_mean']:.1f}ê°œ ë¡œê·¸, ì£¼ìš” í…œí”Œë¦¿ {profile['unique_templates']}ê°œ\n"
        
        # íŒŒì¼ë¡œ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        return report

def analyze_temporal_patterns(data_dir: str, output_dir: str = None):
    """ì‹œê°„ íŒ¨í„´ ê¸°ë°˜ ì´ìƒ íƒì§€ë¥¼ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    
    data_path = Path(data_dir)
    if output_dir is None:
        output_dir = data_path / "temporal_analysis"
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ• ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ì‹œì‘...")
    
    # ë°ì´í„° ë¡œë“œ
    parsed_df = pd.read_parquet(data_path / "parsed.parquet")
    print(f"ğŸ“Š ë¡œë“œëœ ë¡œê·¸: {len(parsed_df)}ê°œ")
    
    # íƒì§€ê¸° ì´ˆê¸°í™”
    detector = TemporalAnomalyDetector()
    
    # í”„ë¡œíŒŒì¼ êµ¬ì¶•
    print("ğŸ“ˆ ì‹œê°„ëŒ€ë³„ í”„ë¡œíŒŒì¼ êµ¬ì¶• ì¤‘...")
    profiles = detector.build_temporal_profiles(parsed_df)
    
    # í”„ë¡œíŒŒì¼ ì €ì¥
    with open(output_path / "temporal_profiles.json", 'w') as f:
        # datetime ê°ì²´ëŠ” JSON serializableí•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ë¬¸ìì—´ë¡œ ë³€í™˜
        profiles_copy = profiles.copy()
        profiles_copy['baseline_period']['start'] = str(profiles_copy['baseline_period']['start'])
        profiles_copy['baseline_period']['end'] = str(profiles_copy['baseline_period']['end'])
        json.dump(profiles_copy, f, indent=2, ensure_ascii=False)
    
    # ì´ìƒ íƒì§€
    print("ğŸ” ì‹œê°„ ê¸°ë°˜ ì´ìƒ íƒì§€ ì¤‘...")
    anomalies = detector.detect_temporal_anomalies(parsed_df, profiles)
    
    # ì´ìƒ íƒì§€ ê²°ê³¼ ì €ì¥ (numpy typesì„ Python native typesìœ¼ë¡œ ë³€í™˜)
    def convert_numpy_types(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        return obj
    
    anomalies_serializable = convert_numpy_types(anomalies)
    with open(output_path / "temporal_anomalies.json", 'w') as f:
        json.dump(anomalies_serializable, f, indent=2, ensure_ascii=False)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    print("ğŸ“„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    report = detector.generate_temporal_report(
        profiles, anomalies, 
        output_path / "temporal_report.md"
    )
    
    print(f"âœ… ì™„ë£Œ! ê²°ê³¼ëŠ” {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸš¨ ë°œê²¬ëœ ì´ìƒ: {len(anomalies)}ê°œ")
    
    # ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥
    if anomalies:
        high_count = len([a for a in anomalies if a['severity'] == 'high'])
        medium_count = len([a for a in anomalies if a['severity'] == 'medium'])
        print(f"   - ì‹¬ê°: {high_count}ê°œ, ì£¼ì˜: {medium_count}ê°œ")
    
    return anomalies, profiles

def main():
    parser = argparse.ArgumentParser(description="ì‹œê°„ íŒ¨í„´ ê¸°ë°˜ ë¡œê·¸ ì´ìƒ íƒì§€")
    parser.add_argument("--data-dir", required=True, 
                       help="ë¶„ì„í•  ë°ì´í„° ë””ë ‰í† ë¦¬ (parsed.parquet í¬í•¨)")
    parser.add_argument("--output-dir", 
                       help="ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬ (ê¸°ë³¸: data-dir/temporal_analysis)")
    
    args = parser.parse_args()
    analyze_temporal_patterns(args.data_dir, args.output_dir)

if __name__ == "__main__":
    main()
