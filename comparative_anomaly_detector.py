#!/usr/bin/env python3
"""
íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€ ë„êµ¬
- ì—¬ëŸ¬ ë¡œê·¸ íŒŒì¼ ê°„ íŒ¨í„´ ë¹„êµ
- Baseline íŒŒì¼ë“¤ê³¼ Target íŒŒì¼ ë¹„êµ
- ì„œë²„/ì„œë¹„ìŠ¤ë³„ ì°¨ì´ì  ë¶„ì„
"""
import pandas as pd
import numpy as np
import json
from pathlib import Path
from typing import Dict, List, Tuple, Optional
import argparse
from collections import Counter

class ComparativeAnomalyDetector:
    def __init__(self):
        self.baseline_profiles = {}
        self.comparison_threshold = 0.3  # 30% ì´ìƒ ì°¨ì´ëŠ” ì´ìƒìœ¼ë¡œ ê°„ì£¼
    
    def extract_file_profile(self, parsed_df: pd.DataFrame, file_name: str) -> Dict:
        """ë‹¨ì¼ íŒŒì¼ì˜ í”„ë¡œíŒŒì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        
        if len(parsed_df) == 0:
            return {
                'file_name': file_name,
                'total_logs': 0,
                'unique_templates': 0,
                'template_distribution': {},
                'error': 'Empty dataset'
            }
        
        # ê¸°ë³¸ í†µê³„
        template_counts = parsed_df['template_id'].value_counts()
        template_dist = (template_counts / len(parsed_df)).to_dict()
        
        # í˜¸ìŠ¤íŠ¸ë³„ ë¶„í¬
        host_dist = parsed_df['host'].value_counts(normalize=True).to_dict() if 'host' in parsed_df.columns else {}
        
        # í”„ë¡œì„¸ìŠ¤ë³„ ë¶„í¬
        process_dist = parsed_df['process'].value_counts(normalize=True).to_dict() if 'process' in parsed_df.columns else {}
        
        # ì‹œê°„ ë²”ìœ„
        time_range = {
            'start': str(parsed_df['timestamp'].min()) if 'timestamp' in parsed_df.columns else None,
            'end': str(parsed_df['timestamp'].max()) if 'timestamp' in parsed_df.columns else None,
            'duration_hours': None
        }
        
        if time_range['start'] and time_range['end']:
            start_dt = pd.to_datetime(time_range['start'])
            end_dt = pd.to_datetime(time_range['end'])
            time_range['duration_hours'] = (end_dt - start_dt).total_seconds() / 3600
        
        # ì—ëŸ¬/ê²½ê³  ë¡œê·¸ ë¹„ìœ¨ (í‚¤ì›Œë“œ ê¸°ë°˜ ì¶”ì •)
        error_keywords = ['error', 'ERROR', 'fail', 'FAIL', 'exception', 'Exception', 'panic', 'fatal']
        warning_keywords = ['warn', 'WARN', 'warning', 'WARNING']
        
        error_logs = parsed_df[parsed_df['raw'].str.contains('|'.join(error_keywords), case=False, na=False)]
        warning_logs = parsed_df[parsed_df['raw'].str.contains('|'.join(warning_keywords), case=False, na=False)]
        
        profile = {
            'file_name': file_name,
            'total_logs': len(parsed_df),
            'unique_templates': len(template_counts),
            'template_distribution': template_dist,
            'top_templates': list(template_counts.head(10).index),
            'host_distribution': host_dist,
            'process_distribution': process_dist,
            'time_range': time_range,
            'error_rate': len(error_logs) / len(parsed_df),
            'warning_rate': len(warning_logs) / len(parsed_df),
            'logs_per_hour': len(parsed_df) / max(time_range['duration_hours'], 1) if time_range['duration_hours'] else None,
            'template_entropy': self._calculate_entropy(template_dist),
            'rare_templates_count': len([t for t, count in template_counts.items() if count == 1])
        }
        
        return profile
    
    def _calculate_entropy(self, distribution: Dict) -> float:
        """ë¶„í¬ì˜ ì—”íŠ¸ë¡œí”¼ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
        if not distribution:
            return 0.0
        
        probs = list(distribution.values())
        probs = [p for p in probs if p > 0]  # 0 í™•ë¥  ì œê±°
        
        if not probs:
            return 0.0
        
        return -sum(p * np.log2(p) for p in probs)
    
    def compare_distributions(self, target_dist: Dict, baseline_dist: Dict) -> Dict:
        """ë‘ ë¶„í¬ ê°„ì˜ ì°¨ì´ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        
        # KL Divergence ê³„ì‚° (ë¶„í¬ ê°„ ì°¨ì´ ì¸¡ì •)
        def kl_divergence(p, q):
            # ì‘ì€ ê°’ìœ¼ë¡œ smoothing
            epsilon = 1e-10
            all_keys = set(p.keys()) | set(q.keys())
            
            p_smooth = {k: p.get(k, 0) + epsilon for k in all_keys}
            q_smooth = {k: q.get(k, 0) + epsilon for k in all_keys}
            
            # ì •ê·œí™”
            p_sum = sum(p_smooth.values())
            q_sum = sum(q_smooth.values())
            p_norm = {k: v/p_sum for k, v in p_smooth.items()}
            q_norm = {k: v/q_sum for k, v in q_smooth.items()}
            
            return sum(p_norm[k] * np.log(p_norm[k] / q_norm[k]) for k in all_keys)
        
        kl_div = kl_divergence(target_dist, baseline_dist)
        
        # ê³µí†µ í…œí”Œë¦¿ê³¼ ìœ ë‹ˆí¬ í…œí”Œë¦¿ ë¶„ì„
        target_templates = set(target_dist.keys())
        baseline_templates = set(baseline_dist.keys())
        
        common_templates = target_templates & baseline_templates
        target_only = target_templates - baseline_templates
        baseline_only = baseline_templates - target_templates
        
        # ìƒìœ„ í…œí”Œë¦¿ ì°¨ì´ ë¶„ì„
        def get_top_templates(dist, n=5):
            return sorted(dist.items(), key=lambda x: x[1], reverse=True)[:n]
        
        target_top = get_top_templates(target_dist)
        baseline_top = get_top_templates(baseline_dist)
        
        return {
            'kl_divergence': kl_div,
            'common_templates': len(common_templates),
            'target_only_templates': len(target_only),
            'baseline_only_templates': len(baseline_only),
            'jaccard_similarity': len(common_templates) / len(target_templates | baseline_templates) if target_templates | baseline_templates else 0,
            'target_top_templates': target_top,
            'baseline_top_templates': baseline_top,
            'unique_in_target': list(target_only)[:10],  # ìµœëŒ€ 10ê°œ
            'missing_from_target': list(baseline_only)[:10]  # ìµœëŒ€ 10ê°œ
        }
    
    def detect_comparative_anomalies(self, target_profile: Dict, 
                                   baseline_profiles: List[Dict]) -> List[Dict]:
        """Target íŒŒì¼ê³¼ Baseline íŒŒì¼ë“¤ì„ ë¹„êµí•˜ì—¬ ì´ìƒì„ íƒì§€í•©ë‹ˆë‹¤."""
        
        anomalies = []
        
        # Baseline í‰ê·  ê³„ì‚°
        if not baseline_profiles:
            return [{
                'type': 'no_baseline',
                'severity': 'high',
                'description': 'Baseline íŒŒì¼ì´ ì—†ì–´ ë¹„êµí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤'
            }]
        
        # ìˆ˜ì¹˜ ì§€í‘œë“¤ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ ê³„ì‚°
        metrics = ['total_logs', 'unique_templates', 'error_rate', 'warning_rate', 
                  'template_entropy', 'rare_templates_count']
        
        baseline_stats = {}
        for metric in metrics:
            values = [p.get(metric, 0) for p in baseline_profiles if p.get(metric) is not None]
            if values:
                baseline_stats[metric] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values)
                }
        
        # ê° ì§€í‘œë³„ ì´ìƒ ê²€ì‚¬
        for metric, stats in baseline_stats.items():
            target_value = target_profile.get(metric, 0)
            if target_value is None:
                continue
                
            # Z-score ê³„ì‚°
            z_score = abs(target_value - stats['mean']) / max(stats['std'], 1e-6)
            
            if z_score > 2.0:  # 2Ïƒ ì´ìƒ ì°¨ì´
                deviation_pct = abs(target_value - stats['mean']) / max(stats['mean'], 1e-6)
                anomalies.append({
                    'type': f'metric_anomaly_{metric}',
                    'severity': 'high' if z_score > 3.0 else 'medium',
                    'description': f'{metric}ì´ baseline ëŒ€ë¹„ {deviation_pct:.1%} ì°¨ì´ (Z-score: {z_score:.2f})',
                    'target_value': target_value,
                    'baseline_mean': stats['mean'],
                    'baseline_std': stats['std'],
                    'z_score': z_score
                })
        
        # í…œí”Œë¦¿ ë¶„í¬ ë¹„êµ
        if baseline_profiles and 'template_distribution' in target_profile:
            # ëª¨ë“  baselineì˜ í…œí”Œë¦¿ ë¶„í¬ë¥¼ í‰ê· í™”
            baseline_combined = {}
            for profile in baseline_profiles:
                for template, prob in profile.get('template_distribution', {}).items():
                    baseline_combined[template] = baseline_combined.get(template, 0) + prob
            
            # í‰ê· í™”
            baseline_count = len(baseline_profiles)
            baseline_avg_dist = {k: v/baseline_count for k, v in baseline_combined.items()}
            
            # ë¶„í¬ ë¹„êµ
            comparison = self.compare_distributions(
                target_profile['template_distribution'], 
                baseline_avg_dist
            )
            
            # KL Divergenceê°€ ë†’ìœ¼ë©´ ì´ìƒ
            if comparison['kl_divergence'] > 1.0:  # ì„ê³„ê°’
                anomalies.append({
                    'type': 'template_distribution_anomaly',
                    'severity': 'high' if comparison['kl_divergence'] > 2.0 else 'medium',
                    'description': f'í…œí”Œë¦¿ ë¶„í¬ê°€ baselineê³¼ í¬ê²Œ ë‹¤ë¦„ (KL: {comparison["kl_divergence"]:.3f})',
                    'kl_divergence': comparison['kl_divergence'],
                    'jaccard_similarity': comparison['jaccard_similarity'],
                    'unique_templates': comparison['target_only_templates'],
                    'missing_templates': comparison['baseline_only_templates']
                })
            
            # ê³ ìœ  í…œí”Œë¦¿ì´ ë§ìœ¼ë©´ ì´ìƒ
            if comparison['target_only_templates'] > len(baseline_avg_dist) * 0.2:
                anomalies.append({
                    'type': 'excessive_unique_templates',
                    'severity': 'medium',
                    'description': f'Targetì—ë§Œ ìˆëŠ” í…œí”Œë¦¿ì´ {comparison["target_only_templates"]}ê°œë¡œ ê³¼ë‹¤',
                    'unique_templates_count': comparison['target_only_templates'],
                    'unique_templates': comparison['unique_in_target']
                })
        
        return anomalies
    
    def generate_comparative_report(self, target_profile: Dict, baseline_profiles: List[Dict], 
                                  anomalies: List[Dict], output_path: str) -> str:
        """ë¹„êµ ë¶„ì„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        
        report = f"""# íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€ ë¦¬í¬íŠ¸

## ğŸ“Š ë¶„ì„ ëŒ€ìƒ
- **Target íŒŒì¼**: {target_profile['file_name']}
- **Baseline íŒŒì¼**: {len(baseline_profiles)}ê°œ
- **Target ë¡œê·¸ ìˆ˜**: {target_profile['total_logs']:,}ê°œ
- **Target í…œí”Œë¦¿ ìˆ˜**: {target_profile['unique_templates']}ê°œ

## ğŸ“ˆ Baseline í†µê³„
"""
        
        if baseline_profiles:
            total_logs = [p['total_logs'] for p in baseline_profiles]
            unique_templates = [p['unique_templates'] for p in baseline_profiles]
            error_rates = [p.get('error_rate', 0) for p in baseline_profiles]
            
            report += f"""
- **í‰ê·  ë¡œê·¸ ìˆ˜**: {np.mean(total_logs):,.0f}ê°œ (Â±{np.std(total_logs):.0f})
- **í‰ê·  í…œí”Œë¦¿ ìˆ˜**: {np.mean(unique_templates):.0f}ê°œ (Â±{np.std(unique_templates):.0f})
- **í‰ê·  ì—ëŸ¬ìœ¨**: {np.mean(error_rates):.2%} (Â±{np.std(error_rates):.2%})

### Baseline íŒŒì¼ ëª©ë¡
"""
            for i, profile in enumerate(baseline_profiles, 1):
                report += f"{i}. {profile['file_name']} - {profile['total_logs']:,}ê°œ ë¡œê·¸, {profile['unique_templates']}ê°œ í…œí”Œë¦¿\n"
        
        # ì´ìƒ íƒì§€ ê²°ê³¼
        report += "\n## ğŸš¨ ë°œê²¬ëœ ì´ìƒ í˜„ìƒ\n\n"
        
        if not anomalies:
            report += "âœ… ë¹„êµ ë¶„ì„ì—ì„œ ì´ìƒ í˜„ìƒì´ ë°œê²¬ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.\n\n"
        else:
            # ì‹¬ê°ë„ë³„ ë¶„ë¥˜
            high_severity = [a for a in anomalies if a['severity'] == 'high']
            medium_severity = [a for a in anomalies if a['severity'] == 'medium']
            
            if high_severity:
                report += f"### ğŸ”´ ì‹¬ê°í•œ ì´ìƒ ({len(high_severity)}ê°œ)\n\n"
                for anomaly in high_severity:
                    report += f"- **{anomaly['type']}**: {anomaly['description']}\n"
                report += "\n"
            
            if medium_severity:
                report += f"### ğŸŸ¡ ì£¼ì˜ í•„ìš” ({len(medium_severity)}ê°œ)\n\n"
                for anomaly in medium_severity:
                    report += f"- **{anomaly['type']}**: {anomaly['description']}\n"
                report += "\n"
        
        # ìƒì„¸ ë¹„êµ í‘œ
        report += "## ğŸ“‹ ìƒì„¸ ë¹„êµí‘œ\n\n"
        report += "| ì§€í‘œ | Target | Baseline í‰ê·  | ì°¨ì´ |\n"
        report += "|------|---------|---------------|------|\n"
        
        if baseline_profiles:
            metrics = [
                ('ë¡œê·¸ ìˆ˜', 'total_logs'),
                ('í…œí”Œë¦¿ ìˆ˜', 'unique_templates'), 
                ('ì—ëŸ¬ìœ¨', 'error_rate'),
                ('ê²½ê³ ìœ¨', 'warning_rate'),
                ('ì—”íŠ¸ë¡œí”¼', 'template_entropy')
            ]
            
            for metric_name, metric_key in metrics:
                target_val = target_profile.get(metric_key, 0)
                baseline_vals = [p.get(metric_key, 0) for p in baseline_profiles if p.get(metric_key) is not None]
                
                if baseline_vals:
                    baseline_mean = np.mean(baseline_vals)
                    diff_pct = (target_val - baseline_mean) / max(baseline_mean, 1e-6) * 100
                    
                    if metric_key in ['error_rate', 'warning_rate']:
                        report += f"| {metric_name} | {target_val:.2%} | {baseline_mean:.2%} | {diff_pct:+.1f}% |\n"
                    elif metric_key == 'template_entropy':
                        report += f"| {metric_name} | {target_val:.2f} | {baseline_mean:.2f} | {diff_pct:+.1f}% |\n"
                    else:
                        report += f"| {metric_name} | {target_val:,} | {baseline_mean:,.0f} | {diff_pct:+.1f}% |\n"
        
        # íŒŒì¼ë¡œ ì €ì¥
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(report)
        
        return report

def compare_log_files(target_file: str, baseline_files: List[str], output_dir: str = None):
    """ì—¬ëŸ¬ ë¡œê·¸ íŒŒì¼ì„ ë¹„êµí•˜ì—¬ ì´ìƒì„ íƒì§€í•©ë‹ˆë‹¤."""
    
    target_path = Path(target_file)
    if output_dir is None:
        output_dir = target_path.parent / "comparative_analysis"
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    
    print("ğŸ” íŒŒì¼ë³„ ë¹„êµ ì´ìƒ íƒì§€ ì‹œì‘...")
    
    detector = ComparativeAnomalyDetector()
    
    # Target íŒŒì¼ ë¶„ì„
    print(f"ğŸ“Š Target íŒŒì¼ ë¶„ì„: {target_path.name}")
    target_df = pd.read_parquet(target_file)
    target_profile = detector.extract_file_profile(target_df, target_path.name)
    
    # Baseline íŒŒì¼ë“¤ ë¶„ì„
    baseline_profiles = []
    print(f"ğŸ“Š Baseline íŒŒì¼ {len(baseline_files)}ê°œ ë¶„ì„...")
    
    for baseline_file in baseline_files:
        baseline_path = Path(baseline_file)
        try:
            baseline_df = pd.read_parquet(baseline_file)
            profile = detector.extract_file_profile(baseline_df, baseline_path.name)
            baseline_profiles.append(profile)
            print(f"   âœ… {baseline_path.name}: {profile['total_logs']}ê°œ ë¡œê·¸")
        except Exception as e:
            print(f"   âŒ {baseline_path.name}: ë¡œë“œ ì‹¤íŒ¨ ({e})")
    
    # í”„ë¡œíŒŒì¼ ì €ì¥
    # í”„ë¡œíŒŒì¼ ì €ì¥ (numpy types ë³€í™˜)
    def convert_numpy_types(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, dict):
            return {k: convert_numpy_types(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [convert_numpy_types(item) for item in obj]
        return obj
    
    all_profiles = {
        'target': convert_numpy_types(target_profile),
        'baselines': convert_numpy_types(baseline_profiles)
    }
    with open(output_path / "file_profiles.json", 'w') as f:
        json.dump(all_profiles, f, indent=2, ensure_ascii=False)
    
    # ì´ìƒ íƒì§€
    print("ğŸ” ë¹„êµ ë¶„ì„ ì¤‘...")
    anomalies = detector.detect_comparative_anomalies(target_profile, baseline_profiles)
    
    anomalies_serializable = convert_numpy_types(anomalies)
    with open(output_path / "comparative_anomalies.json", 'w') as f:
        json.dump(anomalies_serializable, f, indent=2, ensure_ascii=False)
    
    # ë¦¬í¬íŠ¸ ìƒì„±
    print("ğŸ“„ ë¦¬í¬íŠ¸ ìƒì„± ì¤‘...")
    report = detector.generate_comparative_report(
        target_profile, baseline_profiles, anomalies,
        output_path / "comparative_report.md"
    )
    
    print(f"âœ… ì™„ë£Œ! ê²°ê³¼ëŠ” {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"ğŸš¨ ë°œê²¬ëœ ì´ìƒ: {len(anomalies)}ê°œ")
    
    # ê°„ë‹¨í•œ ìš”ì•½ ì¶œë ¥
    if anomalies:
        high_count = len([a for a in anomalies if a['severity'] == 'high'])
        medium_count = len([a for a in anomalies if a['severity'] == 'medium'])
        print(f"   - ì‹¬ê°: {high_count}ê°œ, ì£¼ì˜: {medium_count}ê°œ")
    
    return anomalies, target_profile, baseline_profiles

def main():
    parser = argparse.ArgumentParser(description="íŒŒì¼ë³„ ë¹„êµ ë¡œê·¸ ì´ìƒ íƒì§€")
    parser.add_argument("--target", required=True, 
                       help="ë¶„ì„í•  target íŒŒì¼ (parsed.parquet)")
    parser.add_argument("--baselines", required=True, nargs='+',
                       help="ë¹„êµí•  baseline íŒŒì¼ë“¤ (parsed.parquet)")
    parser.add_argument("--output-dir", 
                       help="ê²°ê³¼ ì¶œë ¥ ë””ë ‰í† ë¦¬")
    
    args = parser.parse_args()
    compare_log_files(args.target, args.baselines, args.output_dir)

if __name__ == "__main__":
    main()
